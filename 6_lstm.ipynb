{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  f = zipfile.ZipFile(filename)\n",
    "  for name in f.namelist():\n",
    "    return tf.compat.as_str(f.read(name))\n",
    "  f.close()\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized probabilities.\"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.297298 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.04\n",
      "================================================================================\n",
      "kjjevpjojpyis z vt tipjifpasbsdhsaibmu oervapuevye  uivnzef a kmfdvcctcrhuhsx bd\n",
      "ehmfnfjgxgllheeeu uihtbx hfubjem mcgq litaz rl ujg  vrwdrlecgnisfaoexemyemls taa\n",
      "ggc mg yxuajmepzmw oefloqtawi  eqh as vtdazziihco  w qs eed eurdoj c wnpzje uuwd\n",
      "wofincoyzliaeqi t dnflyhntps uer munfhlplhvox efjnctsf  rudtiektbjleopq awil  bh\n",
      "sh cszztelsy   liryriqjilky gbtijpogrpxvnrh lnqcotb qatxcmxfi  rerslq rfirdaslkc\n",
      "================================================================================\n",
      "Validation set perplexity: 20.21\n",
      "Average loss at step 100: 2.592717 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.99\n",
      "Validation set perplexity: 10.36\n",
      "Average loss at step 200: 2.249137 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.46\n",
      "Validation set perplexity: 8.65\n",
      "Average loss at step 300: 2.096879 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.32\n",
      "Validation set perplexity: 8.06\n",
      "Average loss at step 400: 2.003389 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.40\n",
      "Validation set perplexity: 7.72\n",
      "Average loss at step 500: 1.942772 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.53\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 600: 1.913912 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.22\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 700: 1.865127 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.25\n",
      "Validation set perplexity: 6.58\n",
      "Average loss at step 800: 1.823330 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.13\n",
      "Validation set perplexity: 6.37\n",
      "Average loss at step 900: 1.834104 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.92\n",
      "Validation set perplexity: 6.31\n",
      "Average loss at step 1000: 1.832251 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "================================================================================\n",
      "zers suller rogan kimmen nutween the sepuerile by du aroun the jehnown ano qustu\n",
      "inis gefrence of fre coptrion of enopled eeist candron of p of is worven matly s\n",
      "cough prassional not tree utter huvater v wehle leather hea it of the forcedy mo\n",
      "we dreal licomed arogas cain sulacing the souriarpe home nicled smately philer t\n",
      "osed and encrefusly or betreos term efaile in ithude and formons no prear been m\n",
      "================================================================================\n",
      "Validation set perplexity: 6.13\n",
      "Average loss at step 1100: 1.782358 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 6.10\n",
      "Average loss at step 1200: 1.758072 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 5.81\n",
      "Average loss at step 1300: 1.739930 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 5.66\n",
      "Average loss at step 1400: 1.753448 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 5.63\n",
      "Average loss at step 1500: 1.740178 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 5.52\n",
      "Average loss at step 1600: 1.750802 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 5.64\n",
      "Average loss at step 1700: 1.719384 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 5.55\n",
      "Average loss at step 1800: 1.676926 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 1900: 1.647436 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 2000: 1.698756 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "================================================================================\n",
      "ratino so that possible west to hatoging pradually piblitary woph hower trusso d\n",
      "alstol throughip divents is skre s ristems at astinside play visk of mode was sh\n",
      "k on dnowr two mastras dyd as the bronksing as stattliins serys t is beading and\n",
      "k in than languagas but scrinced founctive moth mimating along donours lame x en\n",
      "ers later some ivisity in links worr to keads afters bly celmence which angener \n",
      "================================================================================\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 2100: 1.690635 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 2200: 1.684670 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.37\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 2300: 1.643361 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 2400: 1.661480 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 2500: 1.681128 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 2600: 1.655979 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 2700: 1.660274 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 2800: 1.652521 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 2900: 1.651395 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 3000: 1.649544 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "================================================================================\n",
      "x for the publical refour siglity daried gatake secordabilis thot singer ditrocu\n",
      "k playlines be kears of eaplay alline regurdlii open dieutle who charra in twall\n",
      " has fabletly papalations with water nuteate hand aspitian abarcaphh simplyesing\n",
      "todon abortomed goris entervates with and he ald of rimzer v considered d c two \n",
      "ervations in one nine six is kaues court while perowle hards state rungiage regu\n",
      "================================================================================\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 3100: 1.629464 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 3200: 1.647278 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 3300: 1.638186 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 3400: 1.667780 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 3500: 1.658233 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 3600: 1.669597 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 3700: 1.645660 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 3800: 1.648036 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 3900: 1.639593 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 4000: 1.652414 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n",
      "================================================================================\n",
      "qued of reterrence ourchd or i tounche tein and ide cabilar yeads is their salge\n",
      "y of minjal case of maachi in operationally and this and mik bould of have due m\n",
      "mest often immenianly v begin considerdinule andulmoribre  ont medions one six s\n",
      "neanedy evencalies acreapes creedorienn its liberiage cancrided letore circensin\n",
      "oby peop peopitadian prevent one five zero four astle between several besters de\n",
      "================================================================================\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 4100: 1.635739 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 4200: 1.637875 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 4300: 1.616519 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 4400: 1.610650 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 4500: 1.614807 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 4600: 1.617424 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 4700: 1.631412 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 4800: 1.634817 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 4900: 1.633611 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 5000: 1.608261 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "================================================================================\n",
      "ustant of the countren veryisnatip and micall ulfon rabyning by eight easmine pa\n",
      "nish s originam electrie of proses the is fidere of been a singded expleme not a\n",
      "is cerder feptur relative mads one six zero rebllinity the centious esplorsia it\n",
      "ond obsere qual simple windd dreghing as for mones are is nibbainily dyawn two z\n",
      "twad a s can sifiec issey ealoric maching botwor first gneetrandge is sice accip\n",
      "================================================================================\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 5100: 1.609218 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 5200: 1.595049 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 5300: 1.582642 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 5400: 1.583138 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 5500: 1.568712 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 5600: 1.576138 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 5700: 1.566029 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5800: 1.580752 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 5900: 1.577966 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 6000: 1.549374 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "================================================================================\n",
      "gen from two zero nine three three two eight timears or fuids garpry or about or\n",
      "lated yook storenci rulebs dracter malarit s fair fory had norn can hearlic full\n",
      "est of other seven maid in the quost all ornesed the state procest befrerse also\n",
      "ronation sing starting da qualitotics it raids locity such also of the brother w\n",
      "ball haid to a seater work randaped ruth of marstranks two spy decianting have e\n",
      "================================================================================\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6100: 1.566916 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6200: 1.535510 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6300: 1.545679 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6400: 1.539473 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6500: 1.559212 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6600: 1.600706 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6700: 1.581371 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6800: 1.608934 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6900: 1.585954 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 7000: 1.579665 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.17\n",
      "================================================================================\n",
      "kenty also dickel cult ianiso lang weawss and read indestantine by purie crassic\n",
      "lous wanny logaent these is many play age arimia casibtbent roph in the dequltor\n",
      "quine ekhpillial dur coaled to jupient juge modely bradares is an some contempor\n",
      "tion theory to kesson sultally marcive member an prime up years hun didia factis\n",
      "flen emerca resumbent and will her naby sle appriectably bri from releakone sing\n",
      "================================================================================\n",
      "Validation set perplexity: 4.27\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  #concat weights and biases(old version of TF, value is the second param)\n",
    "  ins = tf.concat(1, [ix, fx, cx, ox])\n",
    "  outs = tf.concat(1, [im, fm, cm, om])\n",
    "  bis = tf.concat(1, [ib, fb, cb, ob])\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    result = tf.matmul(i, ins) + tf.matmul(o, outs) + bis\n",
    "    #old version of split must be used\n",
    "    input_gate, forget_gate, update, output_gate = tf.split(1, 4, result)\n",
    "    state = tf.sigmoid(forget_gate) * state + tf.sigmoid(input_gate) * tf.tanh(update)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    5.0, global_step, 5000, 0.1, staircase=False)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.301555 learning rate: 5.000000\n",
      "Minibatch perplexity: 27.15\n",
      "================================================================================\n",
      "ui fys bbeoc e   hbbbs anrae iuzchza  h gcr oodtip      t vnnnychhti bfoounngapy\n",
      "quosjfbt  no t aueienztie hyqbsnrosgaiokosrwh  p f ikorzhdgtfaor ohahavno ns s j\n",
      "hcgo orx rm nmststnqepoj ojhf dft b ettlaxdfmn aelxvpis fudahyikrodrac  t reyoge\n",
      "cfxinhfe y zingtbui g lfzu  yzgknzegpn fhirz n   tnlnboinsees  hi sii mxrrrnis u\n",
      "spzjjgirl   gdahbtsu pzaaiui i y  teeyir w yew eg nxe nnicxgkiilg n ehuoiarc hyw\n",
      "================================================================================\n",
      "Validation set perplexity: 19.58\n",
      "Average loss at step 100: 2.707112 learning rate: 4.774963\n",
      "Minibatch perplexity: 11.24\n",
      "Validation set perplexity: 10.78\n",
      "Average loss at step 200: 2.315445 learning rate: 4.560054\n",
      "Minibatch perplexity: 9.55\n",
      "Validation set perplexity: 9.51\n",
      "Average loss at step 300: 2.164779 learning rate: 4.354818\n",
      "Minibatch perplexity: 8.15\n",
      "Validation set perplexity: 8.77\n",
      "Average loss at step 400: 2.084091 learning rate: 4.158819\n",
      "Minibatch perplexity: 7.87\n",
      "Validation set perplexity: 7.84\n",
      "Average loss at step 500: 2.027840 learning rate: 3.971641\n",
      "Minibatch perplexity: 6.72\n",
      "Validation set perplexity: 7.67\n",
      "Average loss at step 600: 1.970450 learning rate: 3.792888\n",
      "Minibatch perplexity: 7.39\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 700: 1.926929 learning rate: 3.622180\n",
      "Minibatch perplexity: 7.98\n",
      "Validation set perplexity: 7.10\n",
      "Average loss at step 800: 1.887896 learning rate: 3.459155\n",
      "Minibatch perplexity: 6.25\n",
      "Validation set perplexity: 6.89\n",
      "Average loss at step 900: 1.834399 learning rate: 3.303467\n",
      "Minibatch perplexity: 6.67\n",
      "Validation set perplexity: 6.50\n",
      "Average loss at step 1000: 1.838374 learning rate: 3.154787\n",
      "Minibatch perplexity: 5.56\n",
      "================================================================================\n",
      "zed gace be was preaccessure on whi his incrudes of ktoctitians miltn pary furty\n",
      "pent five evepica the gerabers as the consticpty caingeen why eactuon was douge \n",
      "drien burous antroual for to s ulli all strectical ave montaz the giewelo in the\n",
      "ven srot ktrue fear and prilitess a in the saving atth coil the cumpuopet with s\n",
      "xic in celst wnr fort s efst lempess one nine the relpures of oned a it callory \n",
      "================================================================================\n",
      "Validation set perplexity: 6.42\n",
      "Average loss at step 1100: 1.803791 learning rate: 3.012798\n",
      "Minibatch perplexity: 6.43\n",
      "Validation set perplexity: 6.17\n",
      "Average loss at step 1200: 1.804301 learning rate: 2.877200\n",
      "Minibatch perplexity: 6.33\n",
      "Validation set perplexity: 6.03\n",
      "Average loss at step 1300: 1.810167 learning rate: 2.747705\n",
      "Minibatch perplexity: 6.90\n",
      "Validation set perplexity: 5.74\n",
      "Average loss at step 1400: 1.775697 learning rate: 2.624037\n",
      "Minibatch perplexity: 6.64\n",
      "Validation set perplexity: 5.73\n",
      "Average loss at step 1500: 1.758337 learning rate: 2.505936\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 5.73\n",
      "Average loss at step 1600: 1.748026 learning rate: 2.393151\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 5.70\n",
      "Average loss at step 1700: 1.733128 learning rate: 2.285441\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 5.48\n",
      "Average loss at step 1800: 1.741833 learning rate: 2.182579\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 5.58\n",
      "Average loss at step 1900: 1.730885 learning rate: 2.084347\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 5.47\n",
      "Average loss at step 2000: 1.710234 learning rate: 1.990536\n",
      "Minibatch perplexity: 6.11\n",
      "================================================================================\n",
      "gy to of colled volinifie shohtries of he usion in the nine kuaces causate geven\n",
      "wto a dennqrurrent from allotiges expeciech five infleul languriesed usver of to\n",
      "ranus and trablet and retisted he one one eight rerend to be must seven one form\n",
      "zerwurno algoal agigablelif garue in the six or ilders and the masternature for \n",
      "ution of deainto freaducion sys neines bosemales a mocation some agarman seght t\n",
      "================================================================================\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 2100: 1.703489 learning rate: 1.900947\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 2200: 1.715680 learning rate: 1.815390\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 2300: 1.703416 learning rate: 1.733684\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 2400: 1.679676 learning rate: 1.655656\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 2500: 1.687192 learning rate: 1.581139\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 2600: 1.665523 learning rate: 1.509976\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 2700: 1.662380 learning rate: 1.442016\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 2800: 1.685130 learning rate: 1.377114\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 2900: 1.675435 learning rate: 1.315134\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 3000: 1.678049 learning rate: 1.255943\n",
      "Minibatch perplexity: 4.99\n",
      "================================================================================\n",
      "per lyvisa charie also mulcels at mociefe to appreas senerdpligioliston often he\n",
      "s e combate this a p weranened undersocution take goekhy the lates pershearner f\n",
      "f so player of the was the oche of sealfeds by a tola wellers one eight nine to \n",
      "joya sincondible bost of indiview unsiders one whate dempant of gecuraly less or\n",
      "hooway deen city har thus pirch selected the each three through the system fars \n",
      "================================================================================\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 3100: 1.661555 learning rate: 1.199416\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 3200: 1.676227 learning rate: 1.145434\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 3300: 1.672115 learning rate: 1.093881\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 3400: 1.659007 learning rate: 1.044648\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 3500: 1.663675 learning rate: 0.997631\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 3600: 1.659393 learning rate: 0.952730\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 3700: 1.643109 learning rate: 0.909850\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 3800: 1.650802 learning rate: 0.868900\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 3900: 1.634588 learning rate: 0.829794\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 4000: 1.667313 learning rate: 0.792447\n",
      "Minibatch perplexity: 4.61\n",
      "================================================================================\n",
      "ge pount togaes many its two citter grarter such were is a catted but a stage sn\n",
      "hstire or maclisfic in the and phile themallet for up syytorsly alin over que to\n",
      "sizes the scien be a elfating and the s example gastion through present actwoppu\n",
      "ge for those offece foreneria s despetieves full seor the gretate meation licap \n",
      "s stage uncold tear pake chamber stumpian from browsancaman for tree jarkia sity\n",
      "================================================================================\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 4100: 1.640088 learning rate: 0.756781\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 4200: 1.641509 learning rate: 0.722720\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 4300: 1.641765 learning rate: 0.690192\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 4400: 1.639337 learning rate: 0.659128\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 4500: 1.634088 learning rate: 0.629463\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 4600: 1.633028 learning rate: 0.601132\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 4700: 1.613276 learning rate: 0.574077\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 4800: 1.610014 learning rate: 0.548239\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 4900: 1.602426 learning rate: 0.523564\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 5000: 1.624560 learning rate: 0.500000\n",
      "Minibatch perplexity: 5.07\n",
      "================================================================================\n",
      "porito reighreand alterning lixic for nased serious it wabize and two five perfo\n",
      "us in a m comle is writates comegies and and the loys isdan evanuied of rarcan f\n",
      "ix the rest of to the sindiline threesim on the alcosome and aim one zero histip\n",
      "es meaneth of liggn the ont by a reside peosion even units of sanialy nisted lyr\n",
      "quivity what is at a finip urevoress flible for instrumentian spitily history ch\n",
      "================================================================================\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 5100: 1.636219 learning rate: 0.477496\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 5200: 1.632255 learning rate: 0.456005\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 5300: 1.589695 learning rate: 0.435482\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 5400: 1.597954 learning rate: 0.415882\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 5500: 1.598923 learning rate: 0.397164\n",
      "Minibatch perplexity: 6.24\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 5600: 1.609148 learning rate: 0.379289\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 5700: 1.611120 learning rate: 0.362218\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 5800: 1.610423 learning rate: 0.345916\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 5900: 1.646455 learning rate: 0.330347\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 6000: 1.634944 learning rate: 0.315479\n",
      "Minibatch perplexity: 5.84\n",
      "================================================================================\n",
      "bin and it creative dean volen but us in air browhand was percograckemina matwes\n",
      "jest of the presented list leaver studarges upix of the rankical is in men finat\n",
      "x founder of digityde one seven nine eight summilo this by levioial even regubar\n",
      "cs two eeverse althormeder had ductrompinismmt was featly flox neims and other w\n",
      "yrical servers had hrien for c struction mean of bewt it wede be common some bro\n",
      "================================================================================\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 6100: 1.614163 learning rate: 0.301280\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 6200: 1.630117 learning rate: 0.287720\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 6300: 1.606132 learning rate: 0.274770\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 6400: 1.592815 learning rate: 0.262404\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 6500: 1.596881 learning rate: 0.250594\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 6600: 1.587414 learning rate: 0.239315\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 6700: 1.618014 learning rate: 0.228544\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 6800: 1.607548 learning rate: 0.218258\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 6900: 1.629139 learning rate: 0.208435\n",
      "Minibatch perplexity: 4.21\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 7000: 1.576141 learning rate: 0.199054\n",
      "Minibatch perplexity: 4.85\n",
      "================================================================================\n",
      "locations lass sheists which one eight two zero zero frestilla with aur a from a\n",
      "ure as saided tralitional central sux a ese foot thei dessura wostaties and the \n",
      "amaced selication frankralems unto hegring time accesselon jened a graditers min\n",
      " by twr mended lottless forathouse forcyw willkarl that by the chavary of kan it\n",
      "j one eight novely hempaiom the set ricariganist of the utgernow are pleinizos r\n",
      "================================================================================\n",
      "Validation set perplexity: 4.42\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ons anarchists advocate social relations based upon voluntary as\n",
      "when military governments failed to revive the economy and suppr\n",
      "['(o,n)(n,s)(s, )( ,a)(a,n)(n,a)(a,r)(r,c)(c,h)(h,i)(i,s)', '(w,h)(h,e)(e,n)(n, )( ,m)(m,i)(i,l)(l,i)(i,t)(t,a)(a,r)', '(l,l)(l,e)(e,r)(r,i)(i,a)(a, )( ,a)(a,r)(r,c)(c,h)(h,e)', '( ,a)(a,b)(b,b)(b,e)(e,y)(y,s)(s, )( ,a)(a,n)(n,d)(d, )', '(m,a)(a,r)(r,r)(r,i)(i,e)(e,d)(d, )( ,u)(u,r)(r,r)(r,a)', '(h,e)(e,l)(l, )( ,a)(a,n)(n,d)(d, )( ,r)(r,i)(i,c)(c,h)', '(y, )( ,a)(a,n)(n,d)(d, )( ,l)(l,i)(i,t)(t,u)(u,r)(r,g)', '(a,y)(y, )( ,o)(o,p)(p,e)(e,n)(n,e)(e,d)(d, )( ,f)(f,o)', '(t,i)(i,o)(o,n)(n, )( ,f)(f,r)(r,o)(o,m)(m, )( ,t)(t,h)', '(m,i)(i,g)(g,r)(r,a)(a,t)(t,i)(i,o)(o,n)(n, )( ,t)(t,o)', '(n,e)(e,w)(w, )( ,y)(y,o)(o,r)(r,k)(k, )( ,o)(o,t)(t,h)', '(h,e)(e, )( ,b)(b,o)(o,e)(e,i)(i,n)(n,g)(g, )( ,s)(s,e)', '(e, )( ,l)(l,i)(i,s)(s,t)(t,e)(e,d)(d, )( ,w)(w,i)(i,t)', '(e,b)(b,e)(e,r)(r, )( ,h)(h,a)(a,s)(s, )( ,p)(p,r)(r,o)', '(o, )( ,b)(b,e)(e, )( ,m)(m,a)(a,d)(d,e)(e, )( ,t)(t,o)', '(y,e)(e,r)(r, )( ,w)(w,h)(h,o)(o, )( ,r)(r,e)(e,c)(c,e)', '(o,r)(r,e)(e, )( ,s)(s,i)(i,g)(g,n)(n,i)(i,f)(f,i)(i,c)', '(a, )( ,f)(f,i)(i,e)(e,r)(r,c)(c,e)(e, )( ,c)(c,r)(r,i)', '( ,t)(t,w)(w,o)(o, )( ,s)(s,i)(i,x)(x, )( ,e)(e,i)(i,g)', '(a,r)(r,i)(i,s)(s,t)(t,o)(o,t)(t,l)(l,e)(e, )( ,s)(s, )', '(i,t)(t,y)(y, )( ,c)(c,a)(a,n)(n, )( ,b)(b,e)(e, )( ,l)', '( ,a)(a,n)(n,d)(d, )( ,i)(i,n)(n,t)(t,r)(r,a)(a,c)(c,e)', '(t,i)(i,o)(o,n)(n, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e, )', '(d,y)(y, )( ,t)(t,o)(o, )( ,p)(p,a)(a,s)(s,s)(s, )( ,h)', '(f, )( ,c)(c,e)(e,r)(r,t)(t,a)(a,i)(i,n)(n, )( ,d)(d,r)', '(a,t)(t, )( ,i)(i,t)(t, )( ,w)(w,i)(i,l)(l,l)(l, )( ,t)', '(e, )( ,c)(c,o)(o,n)(n,v)(v,i)(i,n)(n,c)(c,e)(e, )( ,t)', '(e,n)(n,t)(t, )( ,t)(t,o)(o,l)(l,d)(d, )( ,h)(h,i)(i,m)', '(a,m)(m,p)(p,a)(a,i)(i,g)(g,n)(n, )( ,a)(a,n)(n,d)(d, )', '(r,v)(v,e)(e,r)(r, )( ,s)(s,i)(i,d)(d,e)(e, )( ,s)(s,t)', '(i,o)(o,u)(u,s)(s, )( ,t)(t,e)(e,x)(x,t)(t,s)(s, )( ,s)', '(o, )( ,c)(c,a)(a,p)(p,i)(i,t)(t,a)(a,l)(l,i)(i,z)(z,e)', '(a, )( ,d)(d,u)(u,p)(p,l)(l,i)(i,c)(c,a)(a,t)(t,e)(e, )', '(g,h)(h, )( ,a)(a,n)(n,n)(n, )( ,e)(e,s)(s, )( ,d)(d, )', '(i,n)(n,e)(e, )( ,j)(j,a)(a,n)(n,u)(u,a)(a,r)(r,y)(y, )', '(r,o)(o,s)(s,s)(s, )( ,z)(z,e)(e,r)(r,o)(o, )( ,t)(t,h)', '(c,a)(a,l)(l, )( ,t)(t,h)(h,e)(e,o)(o,r)(r,i)(i,e)(e,s)', '(a,s)(s,t)(t, )( ,i)(i,n)(n,s)(s,t)(t,a)(a,n)(n,c)(c,e)', '( ,d)(d,i)(i,m)(m,e)(e,n)(n,s)(s,i)(i,o)(o,n)(n,a)(a,l)', '(m,o)(o,s)(s,t)(t, )( ,h)(h,o)(o,l)(l,y)(y, )( ,m)(m,o)', '(t, )( ,s)(s, )( ,s)(s,u)(u,p)(p,p)(p,o)(o,r)(r,t)(t, )', '(u, )( ,i)(i,s)(s, )( ,s)(s,t)(t,i)(i,l)(l,l)(l, )( ,d)', '(e, )( ,o)(o,s)(s,c)(c,i)(i,l)(l,l)(l,a)(a,t)(t,i)(i,n)', '(o, )( ,e)(e,i)(i,g)(g,h)(h,t)(t, )( ,s)(s,u)(u,b)(b,t)', '(o,f)(f, )( ,i)(i,t)(t,a)(a,l)(l,y)(y, )( ,l)(l,a)(a,n)', '(s, )( ,t)(t,h)(h,e)(e, )( ,t)(t,o)(o,w)(w,e)(e,r)(r, )', '(k,l)(l,a)(a,h)(h,o)(o,m)(m,a)(a, )( ,p)(p,r)(r,e)(e,s)', '(e,r)(r,p)(p,r)(r,i)(i,s)(s,e)(e, )( ,l)(l,i)(i,n)(n,u)', '(w,s)(s, )( ,b)(b,e)(e,c)(c,o)(o,m)(m,e)(e,s)(s, )( ,t)', '(e,t)(t, )( ,i)(i,n)(n, )( ,a)(a, )( ,n)(n,a)(a,z)(z,i)', '(t,h)(h,e)(e, )( ,f)(f,a)(a,b)(b,i)(i,a)(a,n)(n, )( ,s)', '(e,t)(t,c)(c,h)(h,y)(y, )( ,t)(t,o)(o, )( ,r)(r,e)(e,l)', '( ,s)(s,h)(h,a)(a,r)(r,m)(m,a)(a,n)(n, )( ,n)(n,e)(e,t)', '(i,s)(s,e)(e,d)(d, )( ,e)(e,m)(m,p)(p,e)(e,r)(r,o)(o,r)', '(t,i)(i,n)(n,g)(g, )( ,i)(i,n)(n, )( ,p)(p,o)(o,l)(l,i)', '(d, )( ,n)(n,e)(e,o)(o, )( ,l)(l,a)(a,t)(t,i)(i,n)(n, )', '(t,h)(h, )( ,r)(r,i)(i,s)(s,k)(k,y)(y, )( ,r)(r,i)(i,s)', '(e,n)(n,c)(c,y)(y,c)(c,l)(l,o)(o,p)(p,e)(e,d)(d,i)(i,c)', '(f,e)(e,n)(n,s)(s,e)(e, )( ,t)(t,h)(h,e)(e, )( ,a)(a,i)', '(d,u)(u,a)(a,t)(t,i)(i,n)(n,g)(g, )( ,f)(f,r)(r,o)(o,m)', '(t,r)(r,e)(e,e)(e,t)(t, )( ,g)(g,r)(r,i)(i,d)(d, )( ,c)', '(a,t)(t,i)(i,o)(o,n)(n,s)(s, )( ,m)(m,o)(o,r)(r,e)(e, )', '(a,p)(p,p)(p,e)(e,a)(a,l)(l, )( ,o)(o,f)(f, )( ,d)(d,e)', '(s,i)(i, )( ,h)(h,a)(a,v)(v,e)(e, )( ,m)(m,a)(a,d)(d,e)']\n",
      "['(i,s)(s,t)(t,s)(s, )( ,a)(a,d)(d,v)(v,o)(o,c)(c,a)(a,t)', '(a,r)(r,y)(y, )( ,g)(g,o)(o,v)(v,e)(e,r)(r,n)(n,m)(m,e)', '(h,e)(e,s)(s, )( ,n)(n,a)(a,t)(t,i)(i,o)(o,n)(n,a)(a,l)', '(d, )( ,m)(m,o)(o,n)(n,a)(a,s)(s,t)(t,e)(e,r)(r,i)(i,e)', '(r,a)(a,c)(c,a)(a, )( ,p)(p,r)(r,i)(i,n)(n,c)(c,e)(e,s)', '(c,h)(h,a)(a,r)(r,d)(d, )( ,b)(b,a)(a,e)(e,r)(r, )( ,h)', '(r,g)(g,i)(i,c)(c,a)(a,l)(l, )( ,l)(l,a)(a,n)(n,g)(g,u)', '(f,o)(o,r)(r, )( ,p)(p,a)(a,s)(s,s)(s,e)(e,n)(n,g)(g,e)', '(t,h)(h,e)(e, )( ,n)(n,a)(a,t)(t,i)(i,o)(o,n)(n,a)(a,l)', '(t,o)(o,o)(o,k)(k, )( ,p)(p,l)(l,a)(a,c)(c,e)(e, )( ,d)', '(t,h)(h,e)(e,r)(r, )( ,w)(w,e)(e,l)(l,l)(l, )( ,k)(k,n)', '(s,e)(e,v)(v,e)(e,n)(n, )( ,s)(s,i)(i,x)(x, )( ,s)(s,e)', '(i,t)(t,h)(h, )( ,a)(a, )( ,g)(g,l)(l,o)(o,s)(s,s)(s, )', '(r,o)(o,b)(b,a)(a,b)(b,l)(l,y)(y, )( ,b)(b,e)(e,e)(e,n)', '(t,o)(o, )( ,r)(r,e)(e,c)(c,o)(o,g)(g,n)(n,i)(i,z)(z,e)', '(c,e)(e,i)(i,v)(v,e)(e,d)(d, )( ,t)(t,h)(h,e)(e, )( ,f)', '(i,c)(c,a)(a,n)(n,t)(t, )( ,t)(t,h)(h,a)(a,n)(n, )( ,i)', '(r,i)(i,t)(t,i)(i,c)(c, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)', '(i,g)(g,h)(h,t)(t, )( ,i)(i,n)(n, )( ,s)(s,i)(i,g)(g,n)', '(s, )( ,u)(u,n)(n,c)(c,a)(a,u)(u,s)(s,e)(e,d)(d, )( ,c)', '( ,l)(l,o)(o,s)(s,t)(t, )( ,a)(a,s)(s, )( ,i)(i,n)(n, )', '(c,e)(e,l)(l,l)(l,u)(u,l)(l,a)(a,r)(r, )( ,i)(i,c)(c,e)', '(e, )( ,s)(s,i)(i,z)(z,e)(e, )( ,o)(o,f)(f, )( ,t)(t,h)', '( ,h)(h,i)(i,m)(m, )( ,a)(a, )( ,s)(s,t)(t,i)(i,c)(c,k)', '(d,r)(r,u)(u,g)(g,s)(s, )( ,c)(c,o)(o,n)(n,f)(f,u)(u,s)', '( ,t)(t,a)(a,k)(k,e)(e, )( ,t)(t,o)(o, )( ,c)(c,o)(o,m)', '( ,t)(t,h)(h,e)(e, )( ,p)(p,r)(r,i)(i,e)(e,s)(s,t)(t, )', '(i,m)(m, )( ,t)(t,o)(o, )( ,n)(n,a)(a,m)(m,e)(e, )( ,i)', '(d, )( ,b)(b,a)(a,r)(r,r)(r,e)(e,d)(d, )( ,a)(a,t)(t,t)', '(s,t)(t,a)(a,n)(n,d)(d,a)(a,r)(r,d)(d, )( ,f)(f,o)(o,r)', '( ,s)(s,u)(u,c)(c,h)(h, )( ,a)(a,s)(s, )( ,e)(e,s)(s,o)', '(z,e)(e, )( ,o)(o,n)(n, )( ,t)(t,h)(h,e)(e, )( ,g)(g,r)', '(e, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e, )( ,o)(o,r)(r,i)', '(d, )( ,h)(h,i)(i,v)(v,e)(e,r)(r, )( ,o)(o,n)(n,e)(e, )', '(y, )( ,e)(e,i)(i,g)(g,h)(h,t)(t, )( ,m)(m,a)(a,r)(r,c)', '(t,h)(h,e)(e, )( ,l)(l,e)(e,a)(a,d)(d, )( ,c)(c,h)(h,a)', '(e,s)(s, )( ,c)(c,l)(l,a)(a,s)(s,s)(s,i)(i,c)(c,a)(a,l)', '(c,e)(e, )( ,t)(t,h)(h,e)(e, )( ,n)(n,o)(o,n)(n, )( ,g)', '(a,l)(l, )( ,a)(a,n)(n,a)(a,l)(l,y)(y,s)(s,i)(i,s)(s, )', '(m,o)(o,r)(r,m)(m,o)(o,n)(n,s)(s, )( ,b)(b,e)(e,l)(l,i)', '(t, )( ,o)(o,r)(r, )( ,a)(a,t)(t, )( ,l)(l,e)(e,a)(a,s)', '( ,d)(d,i)(i,s)(s,a)(a,g)(g,r)(r,e)(e,e)(e,d)(d, )( ,u)', '(i,n)(n,g)(g, )( ,s)(s,y)(y,s)(s,t)(t,e)(e,m)(m, )( ,e)', '(b,t)(t,y)(y,p)(p,e)(e,s)(s, )( ,b)(b,a)(a,s)(s,e)(e,d)', '(a,n)(n,g)(g,u)(u,a)(a,g)(g,e)(e,s)(s, )( ,t)(t,h)(h,e)', '(r, )( ,c)(c,o)(o,m)(m,m)(m,i)(i,s)(s,s)(s,i)(i,o)(o,n)', '(e,s)(s,s)(s, )( ,o)(o,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)', '(n,u)(u,x)(x, )( ,s)(s,u)(u,s)(s,e)(e, )( ,l)(l,i)(i,n)', '( ,t)(t,h)(h,e)(e, )( ,f)(f,i)(i,r)(r,s)(s,t)(t, )( ,d)', '(z,i)(i, )( ,c)(c,o)(o,n)(n,c)(c,e)(e,n)(n,t)(t,r)(r,a)', '( ,s)(s,o)(o,c)(c,i)(i,e)(e,t)(t,y)(y, )( ,n)(n,e)(e,h)', '(e,l)(l,a)(a,t)(t,i)(i,v)(v,e)(e,l)(l,y)(y, )( ,s)(s,t)', '(e,t)(t,w)(w,o)(o,r)(r,k)(k,s)(s, )( ,s)(s,h)(h,a)(a,r)', '(o,r)(r, )( ,h)(h,i)(i,r)(r,o)(o,h)(h,i)(i,t)(t,o)(o, )', '(l,i)(i,t)(t,i)(i,c)(c,a)(a,l)(l, )( ,i)(i,n)(n,i)(i,t)', '(n, )( ,m)(m,o)(o,s)(s,t)(t, )( ,o)(o,f)(f, )( ,t)(t,h)', '(i,s)(s,k)(k,e)(e,r)(r,d)(d,o)(o,o)(o, )( ,r)(r,i)(i,c)', '(i,c)(c, )( ,o)(o,v)(v,e)(e,r)(r,v)(v,i)(i,e)(e,w)(w, )', '(a,i)(i,r)(r, )( ,c)(c,o)(o,m)(m,p)(p,o)(o,n)(n,e)(e,n)', '(o,m)(m, )( ,a)(a,c)(c,n)(n,m)(m, )( ,a)(a,c)(c,c)(c,r)', '( ,c)(c,e)(e,n)(n,t)(t,e)(e,r)(r,l)(l,i)(i,n)(n,e)(e, )', '(e, )( ,t)(t,h)(h,a)(a,n)(n, )( ,a)(a,n)(n,y)(y, )( ,o)', '(d,e)(e,v)(v,o)(o,t)(t,i)(i,o)(o,n)(n,a)(a,l)(l, )( ,b)', '(d,e)(e, )( ,s)(s,u)(u,c)(c,h)(h, )( ,d)(d,e)(e,v)(v,i)']\n",
      "['( ,a)(a,n)']\n",
      "['(a,n)(n,a)']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "#like unrolled matrix of this size\n",
    "bigram_size = vocabulary_size * vocabulary_size\n",
    "\n",
    "class BigramsBatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, bigram_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "        first = self._text[self._cursor[b]]\n",
    "        if self._cursor[b] + 1 == self._text_size :\n",
    "            second = ' '\n",
    "        else :\n",
    "            second = self._text[self._cursor[b] + 1]\n",
    "        #index of unrolled 2-d matrix to 1-d matrix\n",
    "        batch[b, char2id(first) * vocabulary_size + char2id(second)] = 1.0\n",
    "        self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def bigramsCharacters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return ['(' + id2char(c//vocabulary_size) + ',' + id2char(c % vocabulary_size) + ')' \n",
    "            for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def bigramsBatches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, bigramsCharacters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BigramsBatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BigramsBatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(train_text[:64])\n",
    "print(train_text[len(train_text)//64:len(train_text)//64 + 64])\n",
    "print(bigramsBatches2string(train_batches.next()))\n",
    "\n",
    "print(bigramsBatches2string(train_batches.next()))\n",
    "print(bigramsBatches2string(valid_batches.next()))\n",
    "print(bigramsBatches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Embedding\n",
    "  embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    #(a-task)\n",
    "    embed = tf.nn.embedding_lookup(embeddings, tf.argmax(i, dimension=1))\n",
    "    output, state = lstm_cell(embed, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  #(a-task)\n",
    "  sample_input_embed = tf.nn.embedding_lookup(embeddings, tf.argmax(sample_input, dimension=1))\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  #(a-task)\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input_embed, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.312852 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.46\n",
      "================================================================================\n",
      "pl zyanqs cyekfexet b f kni ewcybxmyevtctcttzo qs n hemk tourgwe gicta s duc woc\n",
      "j  f vef a  lr  pwmtae eylyg cheys kb  lzysynnylkt ji pgqe p xoy ex ts uovvnhehe\n",
      "fkh jigheib aecb gwo  rrjrgzju yzclskelkcfn g seoe ntegch zesa rfnfxrd vheodeta \n",
      "qqei nekwvtrof jbplhejetyoeoe b zing jirxnfae v efa vznva raewsewuktvisaea qf tv\n",
      "nthel vgebrx cgs cuwhepa nrp flvqs piihedffeqgta dtra hcya dmuftesb tf m  the l \n",
      "================================================================================\n",
      "Validation set perplexity: 22.04\n",
      "Average loss at step 100: 2.266215 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.31\n",
      "Validation set perplexity: 8.52\n",
      "Average loss at step 200: 2.012950 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.31\n",
      "Validation set perplexity: 7.51\n",
      "Average loss at step 300: 1.933502 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 400: 1.901306 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.99\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 500: 1.871590 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 6.56\n",
      "Average loss at step 600: 1.796855 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 6.35\n",
      "Average loss at step 700: 1.784078 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.32\n",
      "Validation set perplexity: 6.15\n",
      "Average loss at step 800: 1.790503 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.60\n",
      "Validation set perplexity: 5.99\n",
      "Average loss at step 900: 1.775851 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 6.01\n",
      "Average loss at step 1000: 1.785675 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.26\n",
      "================================================================================\n",
      "was some in the strimitg newsend mork most one and for one one fied agunally wah\n",
      "her of nebat the sonuariana anotitutial king three astionary tomal word no fm wa\n",
      "ding and st fom very mosered giw b jained schizg the b l spilbizing donomsidom s\n",
      "qudiske or dloys g tick four sixt in the isidial pde in in s versed and transthe\n",
      "ves his or sing and brive toican blohd ittubly ham ud refreck any stiducificanal\n",
      "================================================================================\n",
      "Validation set perplexity: 5.77\n",
      "Average loss at step 1100: 1.743869 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 5.89\n",
      "Average loss at step 1200: 1.715256 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.13\n",
      "Validation set perplexity: 5.73\n",
      "Average loss at step 1300: 1.712783 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 5.68\n",
      "Average loss at step 1400: 1.720819 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 5.55\n",
      "Average loss at step 1500: 1.713327 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 5.53\n",
      "Average loss at step 1600: 1.700174 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 5.58\n",
      "Average loss at step 1700: 1.683957 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 1800: 1.658562 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 5.36\n",
      "Average loss at step 1900: 1.670561 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 2000: 1.658785 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "================================================================================\n",
      "daithmics atziand as a dolloy teist fffferent of examssatic traditance nobne def\n",
      "ent sciation for seasember do manrriby neckeant zero seven four a pusical versed\n",
      "quesizining a brearly condicted the form player the chartly for gear level loaff\n",
      "varee ho reason to appulant such these varcause argrant is disessions as fon e t\n",
      " cash ant is the suce the see semblius rus octer frence afft mother thistanism d\n",
      "================================================================================\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 2100: 1.663891 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 2200: 1.684671 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 2300: 1.685880 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.31\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 2400: 1.667544 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 5.35\n",
      "Average loss at step 2500: 1.672916 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 5.21\n",
      "Average loss at step 2600: 1.658124 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 2700: 1.667773 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 2800: 1.674908 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 2900: 1.663766 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.12\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 3000: 1.677799 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "================================================================================\n",
      "wever resign of bookny as eurwaction serna the triscropm have infradies and dese\n",
      "imate of one eight six exteist it guelez morphary ited is have to by two two zer\n",
      "elm creased of proor the sidentain tetent from charatifics was if two km usually\n",
      "ism or cuta panching to textrans a katret in of brita use webrizussha also commu\n",
      "al glame obsido systems with are of prosperta a essican and frenchton fresde eng\n",
      "================================================================================\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 3100: 1.648317 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 3200: 1.630454 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 3300: 1.643917 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 3400: 1.632725 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 3500: 1.673572 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.19\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 3600: 1.651493 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 3700: 1.652191 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 3800: 1.657735 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 3900: 1.654251 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.29\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 4000: 1.642796 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "================================================================================\n",
      "jorice milaf comepalteconal opennast courns uight producially and follows amarpy\n",
      "table i is to reveln bree a partemary were counta industruds xdleme fression wit\n",
      "ments provent were other inn secure gaures five there druct six shobertation inv\n",
      "e marrying orginatorial than of wavifaction rayeb tradimers welberse mult green \n",
      "ed c one four zero zero c more does agaled which an acso jyston parto sime for t\n",
      "================================================================================\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 4100: 1.625495 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 4200: 1.617797 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 4300: 1.626980 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 4400: 1.621415 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 4500: 1.650803 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 4600: 1.629461 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 4700: 1.634156 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 4800: 1.622481 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 4900: 1.635606 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 5000: 1.627312 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "================================================================================\n",
      "phy gis treathred hadition and general in setessey georgatest in anence includin\n",
      "encible inclaidemmensmes reecess p torow londo life on that accoppress a greorin\n",
      "matic gan isiteid mater devowar of four two one seven zero zero wome eight mpil \n",
      "ds of many steelep growing of the one seven wom verces the soun and and after th\n",
      "y another norm the publesses to o bil progran two zero ral appate which invernvi\n",
      "================================================================================\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 5100: 1.579319 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 5200: 1.576559 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 5300: 1.578773 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 5400: 1.575969 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 5500: 1.569163 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 5600: 1.541424 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.23\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 5700: 1.554482 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 5800: 1.581956 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 5900: 1.561595 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 6000: 1.563297 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "================================================================================\n",
      "des and hiz which mat and guaginator have site thous of he performs albemers of \n",
      "ke for joerces rest first curces by orler shusts five zero zero myth bas of the \n",
      "ngs as this invold music bizar the vihicaman governments rost relationse for kne\n",
      "ug agos widese n new reasustrated due stell where making decland watour to aling\n",
      "ck for ited first cratex or khandard and hav descending one nine nine seven seve\n",
      "================================================================================\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6100: 1.555773 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.35\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 6200: 1.566601 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 6300: 1.559258 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 6400: 1.551005 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.12\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 6500: 1.533189 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 6600: 1.576692 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 6700: 1.545557 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 6800: 1.550753 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 6900: 1.549212 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 7000: 1.563008 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "================================================================================\n",
      " it ro b the sugh by the trade may one six zero zero zero zoned the source by ge\n",
      "by afboritationists but inbr and comps for title epurorat completed  pakethingum\n",
      "ylemic capitation the leader them bern points ised in fale bass mattle or be nor\n",
      "yon curced by he argort the carf feurant extences conceptinal aroups in douginme\n",
      "ques with n one nine eight three zero nine three arongana system general from el\n",
      "================================================================================\n",
      "Validation set perplexity: 4.42\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
