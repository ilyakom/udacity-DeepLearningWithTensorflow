{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  f = zipfile.ZipFile(filename)\n",
    "  for name in f.namelist():\n",
    "    return tf.compat.as_str(f.read(name))\n",
    "  f.close()\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized probabilities.\"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.297298 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.04\n",
      "================================================================================\n",
      "kjjevpjojpyis z vt tipjifpasbsdhsaibmu oervapuevye  uivnzef a kmfdvcctcrhuhsx bd\n",
      "ehmfnfjgxgllheeeu uihtbx hfubjem mcgq litaz rl ujg  vrwdrlecgnisfaoexemyemls taa\n",
      "ggc mg yxuajmepzmw oefloqtawi  eqh as vtdazziihco  w qs eed eurdoj c wnpzje uuwd\n",
      "wofincoyzliaeqi t dnflyhntps uer munfhlplhvox efjnctsf  rudtiektbjleopq awil  bh\n",
      "sh cszztelsy   liryriqjilky gbtijpogrpxvnrh lnqcotb qatxcmxfi  rerslq rfirdaslkc\n",
      "================================================================================\n",
      "Validation set perplexity: 20.21\n",
      "Average loss at step 100: 2.592717 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.99\n",
      "Validation set perplexity: 10.36\n",
      "Average loss at step 200: 2.249137 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.46\n",
      "Validation set perplexity: 8.65\n",
      "Average loss at step 300: 2.096879 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.32\n",
      "Validation set perplexity: 8.06\n",
      "Average loss at step 400: 2.003389 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.40\n",
      "Validation set perplexity: 7.72\n",
      "Average loss at step 500: 1.942772 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.53\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 600: 1.913912 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.22\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 700: 1.865127 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.25\n",
      "Validation set perplexity: 6.58\n",
      "Average loss at step 800: 1.823330 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.13\n",
      "Validation set perplexity: 6.37\n",
      "Average loss at step 900: 1.834104 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.92\n",
      "Validation set perplexity: 6.31\n",
      "Average loss at step 1000: 1.832251 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "================================================================================\n",
      "zers suller rogan kimmen nutween the sepuerile by du aroun the jehnown ano qustu\n",
      "inis gefrence of fre coptrion of enopled eeist candron of p of is worven matly s\n",
      "cough prassional not tree utter huvater v wehle leather hea it of the forcedy mo\n",
      "we dreal licomed arogas cain sulacing the souriarpe home nicled smately philer t\n",
      "osed and encrefusly or betreos term efaile in ithude and formons no prear been m\n",
      "================================================================================\n",
      "Validation set perplexity: 6.13\n",
      "Average loss at step 1100: 1.782358 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 6.10\n",
      "Average loss at step 1200: 1.758072 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 5.81\n",
      "Average loss at step 1300: 1.739930 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 5.66\n",
      "Average loss at step 1400: 1.753448 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 5.63\n",
      "Average loss at step 1500: 1.740178 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 5.52\n",
      "Average loss at step 1600: 1.750802 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 5.64\n",
      "Average loss at step 1700: 1.719384 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 5.55\n",
      "Average loss at step 1800: 1.676926 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 1900: 1.647436 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 2000: 1.698756 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "================================================================================\n",
      "ratino so that possible west to hatoging pradually piblitary woph hower trusso d\n",
      "alstol throughip divents is skre s ristems at astinside play visk of mode was sh\n",
      "k on dnowr two mastras dyd as the bronksing as stattliins serys t is beading and\n",
      "k in than languagas but scrinced founctive moth mimating along donours lame x en\n",
      "ers later some ivisity in links worr to keads afters bly celmence which angener \n",
      "================================================================================\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 2100: 1.690635 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 2200: 1.684670 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.37\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 2300: 1.643361 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 2400: 1.661480 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 2500: 1.681128 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 2600: 1.655979 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 2700: 1.660274 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 2800: 1.652521 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 2900: 1.651395 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 3000: 1.649544 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "================================================================================\n",
      "x for the publical refour siglity daried gatake secordabilis thot singer ditrocu\n",
      "k playlines be kears of eaplay alline regurdlii open dieutle who charra in twall\n",
      " has fabletly papalations with water nuteate hand aspitian abarcaphh simplyesing\n",
      "todon abortomed goris entervates with and he ald of rimzer v considered d c two \n",
      "ervations in one nine six is kaues court while perowle hards state rungiage regu\n",
      "================================================================================\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 3100: 1.629464 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 3200: 1.647278 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 3300: 1.638186 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 3400: 1.667780 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 3500: 1.658233 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 3600: 1.669597 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 3700: 1.645660 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 3800: 1.648036 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 3900: 1.639593 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 4000: 1.652414 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n",
      "================================================================================\n",
      "qued of reterrence ourchd or i tounche tein and ide cabilar yeads is their salge\n",
      "y of minjal case of maachi in operationally and this and mik bould of have due m\n",
      "mest often immenianly v begin considerdinule andulmoribre  ont medions one six s\n",
      "neanedy evencalies acreapes creedorienn its liberiage cancrided letore circensin\n",
      "oby peop peopitadian prevent one five zero four astle between several besters de\n",
      "================================================================================\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 4100: 1.635739 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 4200: 1.637875 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 4300: 1.616519 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 4400: 1.610650 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 4500: 1.614807 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 4600: 1.617424 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 4700: 1.631412 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 4800: 1.634817 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 4900: 1.633611 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 5000: 1.608261 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "================================================================================\n",
      "ustant of the countren veryisnatip and micall ulfon rabyning by eight easmine pa\n",
      "nish s originam electrie of proses the is fidere of been a singded expleme not a\n",
      "is cerder feptur relative mads one six zero rebllinity the centious esplorsia it\n",
      "ond obsere qual simple windd dreghing as for mones are is nibbainily dyawn two z\n",
      "twad a s can sifiec issey ealoric maching botwor first gneetrandge is sice accip\n",
      "================================================================================\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 5100: 1.609218 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 5200: 1.595049 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 5300: 1.582642 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 5400: 1.583138 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 5500: 1.568712 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 5600: 1.576138 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 5700: 1.566029 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5800: 1.580752 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 5900: 1.577966 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 6000: 1.549374 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "================================================================================\n",
      "gen from two zero nine three three two eight timears or fuids garpry or about or\n",
      "lated yook storenci rulebs dracter malarit s fair fory had norn can hearlic full\n",
      "est of other seven maid in the quost all ornesed the state procest befrerse also\n",
      "ronation sing starting da qualitotics it raids locity such also of the brother w\n",
      "ball haid to a seater work randaped ruth of marstranks two spy decianting have e\n",
      "================================================================================\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6100: 1.566916 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6200: 1.535510 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6300: 1.545679 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6400: 1.539473 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6500: 1.559212 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6600: 1.600706 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6700: 1.581371 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6800: 1.608934 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6900: 1.585954 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 7000: 1.579665 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.17\n",
      "================================================================================\n",
      "kenty also dickel cult ianiso lang weawss and read indestantine by purie crassic\n",
      "lous wanny logaent these is many play age arimia casibtbent roph in the dequltor\n",
      "quine ekhpillial dur coaled to jupient juge modely bradares is an some contempor\n",
      "tion theory to kesson sultally marcive member an prime up years hun didia factis\n",
      "flen emerca resumbent and will her naby sle appriectably bri from releakone sing\n",
      "================================================================================\n",
      "Validation set perplexity: 4.27\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  #concat weights and biases(old version of TF, value is the second param)\n",
    "  ins = tf.concat(1, [ix, fx, cx, ox])\n",
    "  outs = tf.concat(1, [im, fm, cm, om])\n",
    "  bis = tf.concat(1, [ib, fb, cb, ob])\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    result = tf.matmul(i, ins) + tf.matmul(o, outs) + bis\n",
    "    #old version of split must be used\n",
    "    input_gate, forget_gate, update, output_gate = tf.split(1, 4, result)\n",
    "    state = tf.sigmoid(forget_gate) * state + tf.sigmoid(input_gate) * tf.tanh(update)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    5.0, global_step, 5000, 0.1, staircase=False)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.301555 learning rate: 5.000000\n",
      "Minibatch perplexity: 27.15\n",
      "================================================================================\n",
      "ui fys bbeoc e   hbbbs anrae iuzchza  h gcr oodtip      t vnnnychhti bfoounngapy\n",
      "quosjfbt  no t aueienztie hyqbsnrosgaiokosrwh  p f ikorzhdgtfaor ohahavno ns s j\n",
      "hcgo orx rm nmststnqepoj ojhf dft b ettlaxdfmn aelxvpis fudahyikrodrac  t reyoge\n",
      "cfxinhfe y zingtbui g lfzu  yzgknzegpn fhirz n   tnlnboinsees  hi sii mxrrrnis u\n",
      "spzjjgirl   gdahbtsu pzaaiui i y  teeyir w yew eg nxe nnicxgkiilg n ehuoiarc hyw\n",
      "================================================================================\n",
      "Validation set perplexity: 19.58\n",
      "Average loss at step 100: 2.707112 learning rate: 4.774963\n",
      "Minibatch perplexity: 11.24\n",
      "Validation set perplexity: 10.78\n",
      "Average loss at step 200: 2.315445 learning rate: 4.560054\n",
      "Minibatch perplexity: 9.55\n",
      "Validation set perplexity: 9.51\n",
      "Average loss at step 300: 2.164779 learning rate: 4.354818\n",
      "Minibatch perplexity: 8.15\n",
      "Validation set perplexity: 8.77\n",
      "Average loss at step 400: 2.084091 learning rate: 4.158819\n",
      "Minibatch perplexity: 7.87\n",
      "Validation set perplexity: 7.84\n",
      "Average loss at step 500: 2.027840 learning rate: 3.971641\n",
      "Minibatch perplexity: 6.72\n",
      "Validation set perplexity: 7.67\n",
      "Average loss at step 600: 1.970450 learning rate: 3.792888\n",
      "Minibatch perplexity: 7.39\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 700: 1.926929 learning rate: 3.622180\n",
      "Minibatch perplexity: 7.98\n",
      "Validation set perplexity: 7.10\n",
      "Average loss at step 800: 1.887896 learning rate: 3.459155\n",
      "Minibatch perplexity: 6.25\n",
      "Validation set perplexity: 6.89\n",
      "Average loss at step 900: 1.834399 learning rate: 3.303467\n",
      "Minibatch perplexity: 6.67\n",
      "Validation set perplexity: 6.50\n",
      "Average loss at step 1000: 1.838374 learning rate: 3.154787\n",
      "Minibatch perplexity: 5.56\n",
      "================================================================================\n",
      "zed gace be was preaccessure on whi his incrudes of ktoctitians miltn pary furty\n",
      "pent five evepica the gerabers as the consticpty caingeen why eactuon was douge \n",
      "drien burous antroual for to s ulli all strectical ave montaz the giewelo in the\n",
      "ven srot ktrue fear and prilitess a in the saving atth coil the cumpuopet with s\n",
      "xic in celst wnr fort s efst lempess one nine the relpures of oned a it callory \n",
      "================================================================================\n",
      "Validation set perplexity: 6.42\n",
      "Average loss at step 1100: 1.803791 learning rate: 3.012798\n",
      "Minibatch perplexity: 6.43\n",
      "Validation set perplexity: 6.17\n",
      "Average loss at step 1200: 1.804301 learning rate: 2.877200\n",
      "Minibatch perplexity: 6.33\n",
      "Validation set perplexity: 6.03\n",
      "Average loss at step 1300: 1.810167 learning rate: 2.747705\n",
      "Minibatch perplexity: 6.90\n",
      "Validation set perplexity: 5.74\n",
      "Average loss at step 1400: 1.775697 learning rate: 2.624037\n",
      "Minibatch perplexity: 6.64\n",
      "Validation set perplexity: 5.73\n",
      "Average loss at step 1500: 1.758337 learning rate: 2.505936\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 5.73\n",
      "Average loss at step 1600: 1.748026 learning rate: 2.393151\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 5.70\n",
      "Average loss at step 1700: 1.733128 learning rate: 2.285441\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 5.48\n",
      "Average loss at step 1800: 1.741833 learning rate: 2.182579\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 5.58\n",
      "Average loss at step 1900: 1.730885 learning rate: 2.084347\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 5.47\n",
      "Average loss at step 2000: 1.710234 learning rate: 1.990536\n",
      "Minibatch perplexity: 6.11\n",
      "================================================================================\n",
      "gy to of colled volinifie shohtries of he usion in the nine kuaces causate geven\n",
      "wto a dennqrurrent from allotiges expeciech five infleul languriesed usver of to\n",
      "ranus and trablet and retisted he one one eight rerend to be must seven one form\n",
      "zerwurno algoal agigablelif garue in the six or ilders and the masternature for \n",
      "ution of deainto freaducion sys neines bosemales a mocation some agarman seght t\n",
      "================================================================================\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 2100: 1.703489 learning rate: 1.900947\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 2200: 1.715680 learning rate: 1.815390\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 2300: 1.703416 learning rate: 1.733684\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 2400: 1.679676 learning rate: 1.655656\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 2500: 1.687192 learning rate: 1.581139\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 2600: 1.665523 learning rate: 1.509976\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 2700: 1.662380 learning rate: 1.442016\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 2800: 1.685130 learning rate: 1.377114\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 2900: 1.675435 learning rate: 1.315134\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 3000: 1.678049 learning rate: 1.255943\n",
      "Minibatch perplexity: 4.99\n",
      "================================================================================\n",
      "per lyvisa charie also mulcels at mociefe to appreas senerdpligioliston often he\n",
      "s e combate this a p weranened undersocution take goekhy the lates pershearner f\n",
      "f so player of the was the oche of sealfeds by a tola wellers one eight nine to \n",
      "joya sincondible bost of indiview unsiders one whate dempant of gecuraly less or\n",
      "hooway deen city har thus pirch selected the each three through the system fars \n",
      "================================================================================\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 3100: 1.661555 learning rate: 1.199416\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 3200: 1.676227 learning rate: 1.145434\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 3300: 1.672115 learning rate: 1.093881\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 3400: 1.659007 learning rate: 1.044648\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 3500: 1.663675 learning rate: 0.997631\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 3600: 1.659393 learning rate: 0.952730\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 3700: 1.643109 learning rate: 0.909850\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 3800: 1.650802 learning rate: 0.868900\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 3900: 1.634588 learning rate: 0.829794\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 4000: 1.667313 learning rate: 0.792447\n",
      "Minibatch perplexity: 4.61\n",
      "================================================================================\n",
      "ge pount togaes many its two citter grarter such were is a catted but a stage sn\n",
      "hstire or maclisfic in the and phile themallet for up syytorsly alin over que to\n",
      "sizes the scien be a elfating and the s example gastion through present actwoppu\n",
      "ge for those offece foreneria s despetieves full seor the gretate meation licap \n",
      "s stage uncold tear pake chamber stumpian from browsancaman for tree jarkia sity\n",
      "================================================================================\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 4100: 1.640088 learning rate: 0.756781\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 4200: 1.641509 learning rate: 0.722720\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 4300: 1.641765 learning rate: 0.690192\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 4400: 1.639337 learning rate: 0.659128\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 4500: 1.634088 learning rate: 0.629463\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 4600: 1.633028 learning rate: 0.601132\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 4700: 1.613276 learning rate: 0.574077\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 4800: 1.610014 learning rate: 0.548239\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 4900: 1.602426 learning rate: 0.523564\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 5000: 1.624560 learning rate: 0.500000\n",
      "Minibatch perplexity: 5.07\n",
      "================================================================================\n",
      "porito reighreand alterning lixic for nased serious it wabize and two five perfo\n",
      "us in a m comle is writates comegies and and the loys isdan evanuied of rarcan f\n",
      "ix the rest of to the sindiline threesim on the alcosome and aim one zero histip\n",
      "es meaneth of liggn the ont by a reside peosion even units of sanialy nisted lyr\n",
      "quivity what is at a finip urevoress flible for instrumentian spitily history ch\n",
      "================================================================================\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 5100: 1.636219 learning rate: 0.477496\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 5200: 1.632255 learning rate: 0.456005\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 5300: 1.589695 learning rate: 0.435482\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 5400: 1.597954 learning rate: 0.415882\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 5500: 1.598923 learning rate: 0.397164\n",
      "Minibatch perplexity: 6.24\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 5600: 1.609148 learning rate: 0.379289\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 5700: 1.611120 learning rate: 0.362218\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 5800: 1.610423 learning rate: 0.345916\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 5900: 1.646455 learning rate: 0.330347\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 6000: 1.634944 learning rate: 0.315479\n",
      "Minibatch perplexity: 5.84\n",
      "================================================================================\n",
      "bin and it creative dean volen but us in air browhand was percograckemina matwes\n",
      "jest of the presented list leaver studarges upix of the rankical is in men finat\n",
      "x founder of digityde one seven nine eight summilo this by levioial even regubar\n",
      "cs two eeverse althormeder had ductrompinismmt was featly flox neims and other w\n",
      "yrical servers had hrien for c struction mean of bewt it wede be common some bro\n",
      "================================================================================\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 6100: 1.614163 learning rate: 0.301280\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 6200: 1.630117 learning rate: 0.287720\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 6300: 1.606132 learning rate: 0.274770\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 6400: 1.592815 learning rate: 0.262404\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 6500: 1.596881 learning rate: 0.250594\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 6600: 1.587414 learning rate: 0.239315\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 6700: 1.618014 learning rate: 0.228544\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 6800: 1.607548 learning rate: 0.218258\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 6900: 1.629139 learning rate: 0.208435\n",
      "Minibatch perplexity: 4.21\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 7000: 1.576141 learning rate: 0.199054\n",
      "Minibatch perplexity: 4.85\n",
      "================================================================================\n",
      "locations lass sheists which one eight two zero zero frestilla with aur a from a\n",
      "ure as saided tralitional central sux a ese foot thei dessura wostaties and the \n",
      "amaced selication frankralems unto hegring time accesselon jened a graditers min\n",
      " by twr mended lottless forathouse forcyw willkarl that by the chavary of kan it\n",
      "j one eight novely hempaiom the set ricariganist of the utgernow are pleinizos r\n",
      "================================================================================\n",
      "Validation set perplexity: 4.42\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with task B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ons anarchists advocate social relations based upon voluntary as\n",
      "when military governments failed to revive the economy and suppr\n",
      "['(o,n)(n,s)(s, )( ,a)(a,n)(n,a)(a,r)(r,c)(c,h)(h,i)(i,s)', '(w,h)(h,e)(e,n)(n, )( ,m)(m,i)(i,l)(l,i)(i,t)(t,a)(a,r)', '(l,l)(l,e)(e,r)(r,i)(i,a)(a, )( ,a)(a,r)(r,c)(c,h)(h,e)', '( ,a)(a,b)(b,b)(b,e)(e,y)(y,s)(s, )( ,a)(a,n)(n,d)(d, )', '(m,a)(a,r)(r,r)(r,i)(i,e)(e,d)(d, )( ,u)(u,r)(r,r)(r,a)', '(h,e)(e,l)(l, )( ,a)(a,n)(n,d)(d, )( ,r)(r,i)(i,c)(c,h)', '(y, )( ,a)(a,n)(n,d)(d, )( ,l)(l,i)(i,t)(t,u)(u,r)(r,g)', '(a,y)(y, )( ,o)(o,p)(p,e)(e,n)(n,e)(e,d)(d, )( ,f)(f,o)', '(t,i)(i,o)(o,n)(n, )( ,f)(f,r)(r,o)(o,m)(m, )( ,t)(t,h)', '(m,i)(i,g)(g,r)(r,a)(a,t)(t,i)(i,o)(o,n)(n, )( ,t)(t,o)', '(n,e)(e,w)(w, )( ,y)(y,o)(o,r)(r,k)(k, )( ,o)(o,t)(t,h)', '(h,e)(e, )( ,b)(b,o)(o,e)(e,i)(i,n)(n,g)(g, )( ,s)(s,e)', '(e, )( ,l)(l,i)(i,s)(s,t)(t,e)(e,d)(d, )( ,w)(w,i)(i,t)', '(e,b)(b,e)(e,r)(r, )( ,h)(h,a)(a,s)(s, )( ,p)(p,r)(r,o)', '(o, )( ,b)(b,e)(e, )( ,m)(m,a)(a,d)(d,e)(e, )( ,t)(t,o)', '(y,e)(e,r)(r, )( ,w)(w,h)(h,o)(o, )( ,r)(r,e)(e,c)(c,e)', '(o,r)(r,e)(e, )( ,s)(s,i)(i,g)(g,n)(n,i)(i,f)(f,i)(i,c)', '(a, )( ,f)(f,i)(i,e)(e,r)(r,c)(c,e)(e, )( ,c)(c,r)(r,i)', '( ,t)(t,w)(w,o)(o, )( ,s)(s,i)(i,x)(x, )( ,e)(e,i)(i,g)', '(a,r)(r,i)(i,s)(s,t)(t,o)(o,t)(t,l)(l,e)(e, )( ,s)(s, )', '(i,t)(t,y)(y, )( ,c)(c,a)(a,n)(n, )( ,b)(b,e)(e, )( ,l)', '( ,a)(a,n)(n,d)(d, )( ,i)(i,n)(n,t)(t,r)(r,a)(a,c)(c,e)', '(t,i)(i,o)(o,n)(n, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e, )', '(d,y)(y, )( ,t)(t,o)(o, )( ,p)(p,a)(a,s)(s,s)(s, )( ,h)', '(f, )( ,c)(c,e)(e,r)(r,t)(t,a)(a,i)(i,n)(n, )( ,d)(d,r)', '(a,t)(t, )( ,i)(i,t)(t, )( ,w)(w,i)(i,l)(l,l)(l, )( ,t)', '(e, )( ,c)(c,o)(o,n)(n,v)(v,i)(i,n)(n,c)(c,e)(e, )( ,t)', '(e,n)(n,t)(t, )( ,t)(t,o)(o,l)(l,d)(d, )( ,h)(h,i)(i,m)', '(a,m)(m,p)(p,a)(a,i)(i,g)(g,n)(n, )( ,a)(a,n)(n,d)(d, )', '(r,v)(v,e)(e,r)(r, )( ,s)(s,i)(i,d)(d,e)(e, )( ,s)(s,t)', '(i,o)(o,u)(u,s)(s, )( ,t)(t,e)(e,x)(x,t)(t,s)(s, )( ,s)', '(o, )( ,c)(c,a)(a,p)(p,i)(i,t)(t,a)(a,l)(l,i)(i,z)(z,e)', '(a, )( ,d)(d,u)(u,p)(p,l)(l,i)(i,c)(c,a)(a,t)(t,e)(e, )', '(g,h)(h, )( ,a)(a,n)(n,n)(n, )( ,e)(e,s)(s, )( ,d)(d, )', '(i,n)(n,e)(e, )( ,j)(j,a)(a,n)(n,u)(u,a)(a,r)(r,y)(y, )', '(r,o)(o,s)(s,s)(s, )( ,z)(z,e)(e,r)(r,o)(o, )( ,t)(t,h)', '(c,a)(a,l)(l, )( ,t)(t,h)(h,e)(e,o)(o,r)(r,i)(i,e)(e,s)', '(a,s)(s,t)(t, )( ,i)(i,n)(n,s)(s,t)(t,a)(a,n)(n,c)(c,e)', '( ,d)(d,i)(i,m)(m,e)(e,n)(n,s)(s,i)(i,o)(o,n)(n,a)(a,l)', '(m,o)(o,s)(s,t)(t, )( ,h)(h,o)(o,l)(l,y)(y, )( ,m)(m,o)', '(t, )( ,s)(s, )( ,s)(s,u)(u,p)(p,p)(p,o)(o,r)(r,t)(t, )', '(u, )( ,i)(i,s)(s, )( ,s)(s,t)(t,i)(i,l)(l,l)(l, )( ,d)', '(e, )( ,o)(o,s)(s,c)(c,i)(i,l)(l,l)(l,a)(a,t)(t,i)(i,n)', '(o, )( ,e)(e,i)(i,g)(g,h)(h,t)(t, )( ,s)(s,u)(u,b)(b,t)', '(o,f)(f, )( ,i)(i,t)(t,a)(a,l)(l,y)(y, )( ,l)(l,a)(a,n)', '(s, )( ,t)(t,h)(h,e)(e, )( ,t)(t,o)(o,w)(w,e)(e,r)(r, )', '(k,l)(l,a)(a,h)(h,o)(o,m)(m,a)(a, )( ,p)(p,r)(r,e)(e,s)', '(e,r)(r,p)(p,r)(r,i)(i,s)(s,e)(e, )( ,l)(l,i)(i,n)(n,u)', '(w,s)(s, )( ,b)(b,e)(e,c)(c,o)(o,m)(m,e)(e,s)(s, )( ,t)', '(e,t)(t, )( ,i)(i,n)(n, )( ,a)(a, )( ,n)(n,a)(a,z)(z,i)', '(t,h)(h,e)(e, )( ,f)(f,a)(a,b)(b,i)(i,a)(a,n)(n, )( ,s)', '(e,t)(t,c)(c,h)(h,y)(y, )( ,t)(t,o)(o, )( ,r)(r,e)(e,l)', '( ,s)(s,h)(h,a)(a,r)(r,m)(m,a)(a,n)(n, )( ,n)(n,e)(e,t)', '(i,s)(s,e)(e,d)(d, )( ,e)(e,m)(m,p)(p,e)(e,r)(r,o)(o,r)', '(t,i)(i,n)(n,g)(g, )( ,i)(i,n)(n, )( ,p)(p,o)(o,l)(l,i)', '(d, )( ,n)(n,e)(e,o)(o, )( ,l)(l,a)(a,t)(t,i)(i,n)(n, )', '(t,h)(h, )( ,r)(r,i)(i,s)(s,k)(k,y)(y, )( ,r)(r,i)(i,s)', '(e,n)(n,c)(c,y)(y,c)(c,l)(l,o)(o,p)(p,e)(e,d)(d,i)(i,c)', '(f,e)(e,n)(n,s)(s,e)(e, )( ,t)(t,h)(h,e)(e, )( ,a)(a,i)', '(d,u)(u,a)(a,t)(t,i)(i,n)(n,g)(g, )( ,f)(f,r)(r,o)(o,m)', '(t,r)(r,e)(e,e)(e,t)(t, )( ,g)(g,r)(r,i)(i,d)(d, )( ,c)', '(a,t)(t,i)(i,o)(o,n)(n,s)(s, )( ,m)(m,o)(o,r)(r,e)(e, )', '(a,p)(p,p)(p,e)(e,a)(a,l)(l, )( ,o)(o,f)(f, )( ,d)(d,e)', '(s,i)(i, )( ,h)(h,a)(a,v)(v,e)(e, )( ,m)(m,a)(a,d)(d,e)']\n",
      "['(i,s)(s,t)(t,s)(s, )( ,a)(a,d)(d,v)(v,o)(o,c)(c,a)(a,t)', '(a,r)(r,y)(y, )( ,g)(g,o)(o,v)(v,e)(e,r)(r,n)(n,m)(m,e)', '(h,e)(e,s)(s, )( ,n)(n,a)(a,t)(t,i)(i,o)(o,n)(n,a)(a,l)', '(d, )( ,m)(m,o)(o,n)(n,a)(a,s)(s,t)(t,e)(e,r)(r,i)(i,e)', '(r,a)(a,c)(c,a)(a, )( ,p)(p,r)(r,i)(i,n)(n,c)(c,e)(e,s)', '(c,h)(h,a)(a,r)(r,d)(d, )( ,b)(b,a)(a,e)(e,r)(r, )( ,h)', '(r,g)(g,i)(i,c)(c,a)(a,l)(l, )( ,l)(l,a)(a,n)(n,g)(g,u)', '(f,o)(o,r)(r, )( ,p)(p,a)(a,s)(s,s)(s,e)(e,n)(n,g)(g,e)', '(t,h)(h,e)(e, )( ,n)(n,a)(a,t)(t,i)(i,o)(o,n)(n,a)(a,l)', '(t,o)(o,o)(o,k)(k, )( ,p)(p,l)(l,a)(a,c)(c,e)(e, )( ,d)', '(t,h)(h,e)(e,r)(r, )( ,w)(w,e)(e,l)(l,l)(l, )( ,k)(k,n)', '(s,e)(e,v)(v,e)(e,n)(n, )( ,s)(s,i)(i,x)(x, )( ,s)(s,e)', '(i,t)(t,h)(h, )( ,a)(a, )( ,g)(g,l)(l,o)(o,s)(s,s)(s, )', '(r,o)(o,b)(b,a)(a,b)(b,l)(l,y)(y, )( ,b)(b,e)(e,e)(e,n)', '(t,o)(o, )( ,r)(r,e)(e,c)(c,o)(o,g)(g,n)(n,i)(i,z)(z,e)', '(c,e)(e,i)(i,v)(v,e)(e,d)(d, )( ,t)(t,h)(h,e)(e, )( ,f)', '(i,c)(c,a)(a,n)(n,t)(t, )( ,t)(t,h)(h,a)(a,n)(n, )( ,i)', '(r,i)(i,t)(t,i)(i,c)(c, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)', '(i,g)(g,h)(h,t)(t, )( ,i)(i,n)(n, )( ,s)(s,i)(i,g)(g,n)', '(s, )( ,u)(u,n)(n,c)(c,a)(a,u)(u,s)(s,e)(e,d)(d, )( ,c)', '( ,l)(l,o)(o,s)(s,t)(t, )( ,a)(a,s)(s, )( ,i)(i,n)(n, )', '(c,e)(e,l)(l,l)(l,u)(u,l)(l,a)(a,r)(r, )( ,i)(i,c)(c,e)', '(e, )( ,s)(s,i)(i,z)(z,e)(e, )( ,o)(o,f)(f, )( ,t)(t,h)', '( ,h)(h,i)(i,m)(m, )( ,a)(a, )( ,s)(s,t)(t,i)(i,c)(c,k)', '(d,r)(r,u)(u,g)(g,s)(s, )( ,c)(c,o)(o,n)(n,f)(f,u)(u,s)', '( ,t)(t,a)(a,k)(k,e)(e, )( ,t)(t,o)(o, )( ,c)(c,o)(o,m)', '( ,t)(t,h)(h,e)(e, )( ,p)(p,r)(r,i)(i,e)(e,s)(s,t)(t, )', '(i,m)(m, )( ,t)(t,o)(o, )( ,n)(n,a)(a,m)(m,e)(e, )( ,i)', '(d, )( ,b)(b,a)(a,r)(r,r)(r,e)(e,d)(d, )( ,a)(a,t)(t,t)', '(s,t)(t,a)(a,n)(n,d)(d,a)(a,r)(r,d)(d, )( ,f)(f,o)(o,r)', '( ,s)(s,u)(u,c)(c,h)(h, )( ,a)(a,s)(s, )( ,e)(e,s)(s,o)', '(z,e)(e, )( ,o)(o,n)(n, )( ,t)(t,h)(h,e)(e, )( ,g)(g,r)', '(e, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e, )( ,o)(o,r)(r,i)', '(d, )( ,h)(h,i)(i,v)(v,e)(e,r)(r, )( ,o)(o,n)(n,e)(e, )', '(y, )( ,e)(e,i)(i,g)(g,h)(h,t)(t, )( ,m)(m,a)(a,r)(r,c)', '(t,h)(h,e)(e, )( ,l)(l,e)(e,a)(a,d)(d, )( ,c)(c,h)(h,a)', '(e,s)(s, )( ,c)(c,l)(l,a)(a,s)(s,s)(s,i)(i,c)(c,a)(a,l)', '(c,e)(e, )( ,t)(t,h)(h,e)(e, )( ,n)(n,o)(o,n)(n, )( ,g)', '(a,l)(l, )( ,a)(a,n)(n,a)(a,l)(l,y)(y,s)(s,i)(i,s)(s, )', '(m,o)(o,r)(r,m)(m,o)(o,n)(n,s)(s, )( ,b)(b,e)(e,l)(l,i)', '(t, )( ,o)(o,r)(r, )( ,a)(a,t)(t, )( ,l)(l,e)(e,a)(a,s)', '( ,d)(d,i)(i,s)(s,a)(a,g)(g,r)(r,e)(e,e)(e,d)(d, )( ,u)', '(i,n)(n,g)(g, )( ,s)(s,y)(y,s)(s,t)(t,e)(e,m)(m, )( ,e)', '(b,t)(t,y)(y,p)(p,e)(e,s)(s, )( ,b)(b,a)(a,s)(s,e)(e,d)', '(a,n)(n,g)(g,u)(u,a)(a,g)(g,e)(e,s)(s, )( ,t)(t,h)(h,e)', '(r, )( ,c)(c,o)(o,m)(m,m)(m,i)(i,s)(s,s)(s,i)(i,o)(o,n)', '(e,s)(s,s)(s, )( ,o)(o,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)', '(n,u)(u,x)(x, )( ,s)(s,u)(u,s)(s,e)(e, )( ,l)(l,i)(i,n)', '( ,t)(t,h)(h,e)(e, )( ,f)(f,i)(i,r)(r,s)(s,t)(t, )( ,d)', '(z,i)(i, )( ,c)(c,o)(o,n)(n,c)(c,e)(e,n)(n,t)(t,r)(r,a)', '( ,s)(s,o)(o,c)(c,i)(i,e)(e,t)(t,y)(y, )( ,n)(n,e)(e,h)', '(e,l)(l,a)(a,t)(t,i)(i,v)(v,e)(e,l)(l,y)(y, )( ,s)(s,t)', '(e,t)(t,w)(w,o)(o,r)(r,k)(k,s)(s, )( ,s)(s,h)(h,a)(a,r)', '(o,r)(r, )( ,h)(h,i)(i,r)(r,o)(o,h)(h,i)(i,t)(t,o)(o, )', '(l,i)(i,t)(t,i)(i,c)(c,a)(a,l)(l, )( ,i)(i,n)(n,i)(i,t)', '(n, )( ,m)(m,o)(o,s)(s,t)(t, )( ,o)(o,f)(f, )( ,t)(t,h)', '(i,s)(s,k)(k,e)(e,r)(r,d)(d,o)(o,o)(o, )( ,r)(r,i)(i,c)', '(i,c)(c, )( ,o)(o,v)(v,e)(e,r)(r,v)(v,i)(i,e)(e,w)(w, )', '(a,i)(i,r)(r, )( ,c)(c,o)(o,m)(m,p)(p,o)(o,n)(n,e)(e,n)', '(o,m)(m, )( ,a)(a,c)(c,n)(n,m)(m, )( ,a)(a,c)(c,c)(c,r)', '( ,c)(c,e)(e,n)(n,t)(t,e)(e,r)(r,l)(l,i)(i,n)(n,e)(e, )', '(e, )( ,t)(t,h)(h,a)(a,n)(n, )( ,a)(a,n)(n,y)(y, )( ,o)', '(d,e)(e,v)(v,o)(o,t)(t,i)(i,o)(o,n)(n,a)(a,l)(l, )( ,b)', '(d,e)(e, )( ,s)(s,u)(u,c)(c,h)(h, )( ,d)(d,e)(e,v)(v,i)']\n",
      "['( ,a)(a,n)']\n",
      "['(a,n)(n,a)']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "#like unrolled matrix of this size\n",
    "bigram_size = vocabulary_size * vocabulary_size\n",
    "\n",
    "class EmbedBigramsBatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, bigram_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "        first = self._text[self._cursor[b]]\n",
    "        if self._cursor[b] + 1 == self._text_size :\n",
    "            second = ' '\n",
    "        else :\n",
    "            second = self._text[self._cursor[b] + 1]\n",
    "        #index of unrolled 2-d matrix to 1-d matrix\n",
    "        batch[b, char2id(first) * vocabulary_size + char2id(second)] = 1.0\n",
    "        self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def bigramsCharacters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return ['(' + id2char(c//vocabulary_size) + ',' + id2char(c % vocabulary_size) + ')' \n",
    "            for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def bigramsBatches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, bigramsCharacters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BigramsBatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BigramsBatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(train_text[:64])\n",
    "print(train_text[len(train_text)//64:len(train_text)//64 + 64])\n",
    "print(bigramsBatches2string(train_batches.next()))\n",
    "\n",
    "print(bigramsBatches2string(train_batches.next()))\n",
    "print(bigramsBatches2string(valid_batches.next()))\n",
    "print(bigramsBatches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([bigram_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([bigram_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([bigram_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([bigram_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, bigram_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([bigram_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,bigram_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, bigram_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bigrams_sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, bigram_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def bigrams_random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, bigram_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.592303 learning rate: 10.000000\n",
      "Minibatch perplexity: 729.46\n",
      "================================================================================\n",
      "(h,m)(o,z)(k,j)(p,o)(a,d)(e,b)(m,b)(r,l)(r,f)(p,u)(f,q)(m, )(s,i)(f,o)(c,c)(m,v)(s,w)(s,p)(s,a)(c,e)(w,x)(k,y)(e,a)(p,q)(v,y)(f,l)(i,j)(r,h)(i, )(y,a)(k,l)( ,m)(b,r)(q,c)(m,y)(q,t)(j,p)(l,e)(r,g)( ,m)(m,o)(f,i)(x,w)(c,n)(p,j)(k,n)(x,u)(x,o)(j,r)(v,g)(p,j)(d,a)(l,b)(p,u)(j,c)(w,a)(v,b)(y,o)(l,u)(s,y)(r,l)(j,d)(u, )(m,t)(l,p)(q,i)(s,c)(r,x)(w,r)(d,u)(a,y)(l, )(h,p)(z,f)(h,o)(n,f)(o,a)(n,d)(c,o)( ,w)\n",
      "(w,t)(w,j)(i,k)(x,j)(x,e)(c,z)(y,t)(c,h)(g,d)(h,r)(h,a)(y,e)(q,q)(a,h)(r,f)(o,e)(o,s)(s,r)(g,h)(g,j)(r,z)(u,i)(j,w)(n,j)(q,h)(l,l)(g,a)(a,k)(w,c)(q,s)(o,s)(l,b)(o,i)(u,e)(k,l)(c,l)(v,f)(w,f)(w,k)(t,s)(w,y)(j,y)(b,w)(w,i)(m,d)(u,l)(o,a)(z,q)(f,r)(b,j)(k,d)(q,s)(e,s)(k,o)(m,r)(r,v)(k,m)( ,o)(u,o)(n,g)(k,x)(l,x)(h,b)(i,n)(c,u)(e,h)(m,z)(w,r)(x,o)(q,x)(k,b)(w,h)(d,a)(q,e)(y,n)(t,s)(e,w)(e,n)(i,q)(l, )\n",
      "(l,i)(l,i)(m,k)(w,w)(e,y)(s,i)(h,n)(q,n)(q,v)(n,p)(n,p)(n,a)(j,c)(v,y)(u,v)(r,v)(j,g)(o,x)(h,x)( ,f)(o,z)(r,y)(g,e)(c,s)(y,l)(w,o)(d,p)(v,u)(v,k)(e,q)(q,e)(d,u)(n,h)(h,r)(l,m)(h,b)(k,c)(n,a)(c,k)(d,r)(y,g)(c,m)(j,q)(w,h)(b,z)(m,o)(i,j)(a,e)(f,w)(p,u)(x,i)(j,g)(n,v)(x,p)(f,r)(m, )(h,d)(w,y)(p,i)(d,g)(c,z)(z,l)(v,k)(r,w)(u,y)(m,h)(k,w)(y,p)(h,t)(i,y)(p,a)(c,o)(f,f)(o,g)(f,w)(x,s)(n,d)(n,h)(t,a)(d,i)\n",
      "(v,y)(u,w)(v,p)(q,g)(k,z)(l,q)(k,n)(z,s)(o,l)(q,b)(u, )(g,s)(h,i)(q,w)( ,t)(g,c)(o,k)(b,d)(s,j)(g,v)(y,j)(y,g)(t,o)(b,v)(e,q)(o,u)(z,q)(n,t)(b,f)(a,u)(y,e)(v,o)(w,w)(a,y)(e,h)(c,y)(y,n)(h,m)(e,n)(x,v)(r,f)(s,a)(i,j)(r,p)(h,k)(i,f)(y,m)(a,c)( ,j)(d,l)(h,h)(s, )(r,w)(d,s)(x,p)(n,c)(v,j)(v,o)(t,g)(j,x)(v,s)(a,y)(f,s)(w,c)(b,i)(m,h)(r,h)(w,x)(y,y)(b, )(o,p)(i,v)(z,i)(h,p)( ,s)(x,y)(a, )(v,i)(c,u)(r,d)\n",
      "(m,d)(a,v)(a,b)(d,n)(x,u)(p,t)(t,s)(u,h)(d,b)(w,l)(a,a)(p,q)(p,q)(u,t)(o,h)(b,n)(o,p)(s,c)(j,o)(d,h)(g,r)(m,k)(z, )(r, )(w,p)(l,k)(c,y)(n,x)(m,k)(d,x)(m,t)(q,s)(o,q)(a,t)(z,o)(k,s)(z,t)(g,j)(b,b)(e,t)(k,q)(y,u)(w,z)(k,c)(m,r)(t, )(h,v)(s,a)(b,x)(r,i)(o,w)(g,f)(i,l)(a,f)(z,b)(c,i)(w,d)(x,e)(l,r)(v,d)(c,i)(q,a)(h,y)(u,e)(d,p)( ,d)(m,j)(y,u)(p,u)(n,s)(h,h)(b,q)(g,r)(y,m)(p,s)(u,e)(k,d)(l,c)(e,z)(i,h)\n",
      "================================================================================\n",
      "Validation set perplexity: 669.93\n",
      "Average loss at step 100: 5.287456 learning rate: 10.000000\n",
      "Minibatch perplexity: 118.92\n",
      "Validation set perplexity: 119.98\n",
      "Average loss at step 200: 3.740641 learning rate: 10.000000\n",
      "Minibatch perplexity: 17.71\n",
      "Validation set perplexity: 18.99\n",
      "Average loss at step 300: 2.619654 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.20\n",
      "Validation set perplexity: 10.38\n",
      "Average loss at step 400: 2.215689 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.56\n",
      "Validation set perplexity: 8.43\n",
      "Average loss at step 500: 2.066324 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.62\n",
      "Validation set perplexity: 7.52\n",
      "Average loss at step 600: 1.994821 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.76\n",
      "Validation set perplexity: 6.80\n",
      "Average loss at step 700: 1.906741 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.60\n",
      "Validation set perplexity: 6.57\n",
      "Average loss at step 800: 1.846422 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.66\n",
      "Validation set perplexity: 6.26\n",
      "Average loss at step 900: 1.854550 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.93\n",
      "Validation set perplexity: 6.06\n",
      "Average loss at step 1000: 1.834596 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "================================================================================\n",
      "(u,e)(e,d)(d, )( ,b)(b,i)(i,l)(l,i)(i,n)(n,g)(g, )( ,o)(o,b)(b,a)(a,h)(y,s)(s, )( ,w)(w,e)(e,e)(e,d)(d,o)( ,a)(a, )( ,c)(c,h)(h,e)(e,r)(r,s)(s,i)(i,n)(n,i)(i,n)(n,g)(g, )( ,t)(t,h)(h,a)(a,t)(t, )( ,a)(a,b)(s,p)(u,a)(a,v)(v,e)(e, )( ,s)(s,m)(m,a)(a,u)(u,t)(t, )( ,o)(o,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,t)(t,w)(w,o)(o, )( ,s)(s, )( ,t)(t,h)(h,e)(e, )( ,s)(s,e)(e,v)(v,e)\n",
      "(w,r)(o,c)(c,e)(e, )( ,s)(s,y)(y,s)(s,o)(o,d)(d,o)(o,g)(g,i)(i,o)(o,n)(n, )( ,i)(i,n)(n, )( ,o)(o,n)(n,e)(e, )( ,s)(s,e)(e,v)(v,e)(e,n)(n, )( ,n)(n,i)(i,n)(n,e)(e, )( ,s)(s,e)(e,v)(v,e)(e,n)(n, )( ,e)(e,i)(i,g)(g,h)(h,t)(t, )( ,z)(z,e)(e,r)(r,o)(o, )( ,f)(f,i)(i,r)(r,o)(o, )( ,t)(t,h)(h,r)(r,e)(e,e)(e, )( ,b)(b,e)(e,g)(g,e)(e, )( ,m)(m,e)(e,r)(r,i)(i,n)(n,g)(g, )( ,p)(p,i)(i,s)(s,t)(t,e)(e,r)(r,s)\n",
      "(g,j)(a,e)(e,d)(d, )( ,b)(b,y)(y, )( ,c)(c,o)(o, )( ,o)(o,n)(n,e)(e, )( ,s)(s,i)(i,g)(x, )( ,n)(n,i)(i,n)(n,e)(e, )( ,o)(o,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,f)(w,a)(a,r)(r,e)(e, )( ,r)(r,a)(a,m)(m,e)(e, )( ,c)(c,h)(h,a)(a,s)(s,t)(t, )( ,s)(s,t)(t,r)(r,i)(i,c)(c,i)(i,s)(s,t)(t, )( ,o)(o,n)(n, )( ,a)(a, )( ,m)(m,u)(u,l)(l,a)(a,y)(y, )( ,o)(o,n)(n,l)(l,y)(g,u)(l,e)(e, )( ,f)(f,r)(r,o)(o,m)(m, )\n",
      "(d,r)(r,a)(a,n)(n, )( ,i)(i,s)(s, )( ,e)(e,x)(x,i)(i,n)(n, )( ,f)(f,i)(i,v)(v,e)(e, )( ,e)(e,i)(i,g)(g,h)(h,t)(t, )( ,f)(f,o)(o,u)(u,n)(n,d)(d,e)(e, )( ,f)(f,i)(i,v)(v,e)(e, )( ,a)(a,r)(r,g)(y,s)(s,e)(e,r)(r, )( ,t)(t,h)(h,e)(e, )( ,s)(s,u)(u,m)(m,p)(p,u)(u,t)(t,e)(e,r)(r,l)(l,i)(i,a)(a,n)(n, )( ,v)(z, )( ,o)(o,f)(f, )( ,a)(a,n)(n,d)(d, )( ,t)(t,h)(h,e)(e, )( ,i)(i,f)(f,f)(f,e)(e,r)(r, )( ,w)(w,i)\n",
      "(o,t)(t,e)(e,r)(r,c)(d,g)(s,i)(i,l)(l, )( ,o)(o,f)(f, )( ,t)(t,r)(r,e)(e,c)(c,o)(o,u)(u,s)(s,s)(s, )( ,a)(a,m)(m,o)(o,v)(v,e)(e,r)(r,s)(s,p)(p,a)(a,f)(r,f)(f,i)(i,c)(c,e)(e, )( ,s)(s,e)(e,v)(v,e)(e,n)(n, )( ,n)(n,i)(i,n)(n,e)(e, )( ,e)(e,i)(i,g)(g,h)(h,t)(t, )( ,t)(t,e)(e,n)(n,e)(e,r)(r,a)(a,t)(t,u)(u,r)(r,n)(n,i)(i,n)(n,e)(e, )( ,a)(a,n)(n,d)(d, )( ,t)(t,h)(h,e)(e, )( ,l)(l,a)(a,r)(r,g)(g,e)(e, )\n",
      "================================================================================\n",
      "Validation set perplexity: 6.01\n",
      "Average loss at step 1100: 1.781430 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 5.96\n",
      "Average loss at step 1200: 1.746443 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 1300: 1.716732 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 1400: 1.735636 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.27\n",
      "Validation set perplexity: 5.36\n",
      "Average loss at step 1500: 1.718027 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 1600: 1.730747 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 1700: 1.694937 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 1800: 1.654671 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 1900: 1.624715 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 2000: 1.670219 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "================================================================================\n",
      "(n, )( ,u)(u,r)(r,c)(c,c)(c,i)(i,t)(t,i)(i,v)(v,e)(e, )( ,f)(f,r)(r,a)(a,r)(r,a)(a,t)(t,i)(i,o)(o,n)(n, )( ,a)(a,n)(n,d)(d, )( ,g)(g,r)(r,a)(a,y)(y,e)(e,b)(b,e)(e,r)(r,n)(n, )( ,b)(b,r)(r,o)(o,u)(u,p)(p, )( ,a)(a,s)(s, )( ,a)(a, )( ,t)(t,h)(h,e)(e, )( ,c)(c,a)(a,n)(n,c)(c,u)(u,m)(m,a)(a,t)(t,o)(o,r)(r,n)(n, )( ,s)(s,e)(e,v)(v,e)(e,r)(r,a)(a,r)(r,y)(y, )( ,e)(e,x)(x,c)(c,o)(o,r)(r,p)(p,i)(i,t)(t,l)\n",
      "(l,y)(y, )( ,p)(p,r)(r,e)(e,s)(s,i)(i,d)(d,e)(e,n)(n,t)(t,a)(a,r)(r,d)(d, )( ,o)(o,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,s)(s,o)(o,c)(c,h)(h, )( ,h)(h,i)(i,g)(g,h)(a,b)(b,l)(l,a)(a,n)(n, )( ,c)(c,o)(o,n)(n,c)(c,o)(o,n)(n,s)(s,i)(i,s)(s, )( ,o)(o,f)(f, )( ,a)(a,n)(n,a)(a,r)(r,c)(c,h)(h,e)(e, )( ,y)(e,u)(u, )( ,a)(a,n)(n,d)(d, )( ,o)(o,t)(t,h)(h,e)(e,r)(r, )( ,i)(i,n)(n,s)\n",
      "(n,l)(l,e)(e, )( ,c)(c,o)(o,m)(m,e)(e, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e, )( ,p)(p,a)(a,i)(i,n)(n, )( ,l)(l,a)(a,c)(c,i)(i,a)(a, )( ,l)(l,a)(a,s)(z,m)(l,l)(l,y)(y, )( ,a)(a,t)(t, )( ,t)(t,o)(o, )( ,t)(t,h)(h,e)(e, )( ,g)(g,a)(a,c)(c,h)(h, )( ,o)(o,f)(f, )( ,f)(f,o)(o,r)(r,m)(m,a)(a,l)(l,t)(t, )( ,m)(m,i)(i,s)(s,s)(s,i)(i,v)(v,e)(e, )( ,b)(b,e)(e,g)(g,i)(i,a)(a,n)(n,s)(s,i)(i,s)(s,t)(t, )( ,f)(f,a)\n",
      "(u,d)(d,a)(a, )( ,h)(h,y)(y,p)(p,e)(e,n)(n,t)(t,a)(a,n)(n,t)(t,i)(i,n)(n,f)(f,a)(a,n)(n, )( ,s)(s,e)(e,m)(m,e)(e,a)(a,r)(r,e)(e, )( ,a)(a,s)(s, )( ,t)(t,h)(h,r)(r,e)(e,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,o)(o,n)(n,e)(e, )( ,s)(s,e)(e,v)(v,e)(e,n)(n, )( ,s)(s,i)(i,x)(x, )( ,a)(a,n)(n,d)(d, )( ,i)(i,n)(n,e)(e, )( ,p)(p,r)(r,o)(o,p)(p,o)(o,r)(r,t)(t,a)(a,n)(n,t)(t,i)(i,t)(t,y)(y, )( ,a)(a,n)(n,d)(d, )\n",
      "(t,o)(o,n)(n, )( ,h)(h,o)(o,m)(m, )( ,a)(a, )( ,d)(d,e)(e,v)(v,i)(i,n)(n,g)(g,u)(u,s)(s,i)(i,u)(o,n)(n, )( ,s)(s,o)(o,k)(k,s)(s, )( ,t)(t,o)(o,l)(l,d)(d, )( ,i)(i,n)(n, )( ,c)(c,o)(o,u)(u,r)(r,t)(t,a)(a,b)(b,l)(l,e)(e, )( ,d)(d,e)(e,a)(a,l)(l,l)(l,i)(i,c)(c, )( ,c)(c,o)(o,n)(n,s)(s,t)(t,i)(i,m)(m, )( ,c)(c,o)(o,n)(n,t)(t,e)(e,r)(r,n)(n, )( ,k)(k,i)(i,n)(n,e)(e,r)(r,a)(a,l)(l, )( ,o)(o,l)(l,o)(o,g)\n",
      "================================================================================\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 2100: 1.655360 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 2200: 1.647340 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 2300: 1.612603 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 2400: 1.622014 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 2500: 1.646665 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 2600: 1.620326 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 2700: 1.620334 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 2800: 1.609339 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 2900: 1.608056 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 3000: 1.610591 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.50\n",
      "================================================================================\n",
      "(j,k)(f,t)(t,a)(a,g)(g,e)(e, )( ,w)(w,i)(i,t)(t,h)(h, )( ,p)(p,l)(l,a)(a,c)(c,e)(e, )( ,w)(w,i)(i,t)(t,h)(h, )( ,t)(t,h)(h,e)(e, )( ,g)(g,a)(a,c)(c,s)(s, )( ,d)(d, )( ,o)(o,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,t)(t,h)(h,r)(r,e)(e,e)(e,n)(n, )( ,e)(e,i)(i,g)(g,h)(h,t)(t, )( ,f)(f,i)(i,v)(v,e)(e, )( ,t)(t,o)(o, )( ,w)(w,a)(a,s)(s, )( ,e)(e,l)(l,e)(e,c)(c,t)(t,o)(o,p)(p,r)\n",
      "(h,r)(r,o)(o,w)(w, )( ,m)(m,a)(a,d)(d,a)(a,d)(f,r)(r,i)(i,a)(a,n)(n, )( ,r)(r,o)(o,t)(t,i)(i,e)(e, )( ,a)(a,m)(m,e)(e,r)(r,i)(i,c)(c,a)(a,n)(n, )( ,b)(b,l)(l,a)(a,c)(c,k)(k,a)(a, )( ,e)(e,x)(x,p)(p,i)(i,r)(r,a)(a,l)(l, )( ,a)(a,s)(s,s)(s,o)(o, )( ,w)(w,i)(i,t)(t,h)(h,s)(s, )( ,l)(l,e)(e,t)(t,e)(e,r)(r, )( ,s)(s,i)(i,x)(x, )( ,i)(i,t)(t, )( ,k)(k,e)(e,n)(n,t)(t,s)(s, )( ,v)(v,a)(a,r)(r,y)(y, )( ,y)\n",
      "(a,k)(k, )( ,s)(s,t)(t,r)(r,u)(u,c)(c,t)(t,i)(i,o)(o,n)(n,g)(g, )( ,c)(c,o)(o,m)(m,m)(m,o)(o,d)(d, )( ,i)(i,n)(n,f)(f,i)(i,n)(n,c)(c,t)(t,i)(i,o)(o,n)(n, )( ,a)(a,n)(n,d)(d, )( ,m)(m,u)(u,c)(c,h)(h, )( ,o)(o,w)(w,n)(n,t)(t,i)(i,c)(c,a)(a,l)(l, )( ,r)(r,e)(e,c)(c,o)(o,n)(n,c)(c,e)(e,s)(s,s)(s,t)(t,i)(i,p)(p,l)(l,e)(e, )( ,o)(o,f)(f, )( ,p)(p,e)(e,r)(r, )( ,a)(a,p)(p,r)(r,i)(i,l)(l, )( ,f)(f,o)(o,r)\n",
      "(m,s)(s,m)(m,e)(e,r)(r,s)(s, )( ,j)(j,u)(u,s)(s,t)(t,a)(a,n)(n,d)(d, )( ,t)(t,h)(h,i)(i,s)(s, )( ,t)(t,h)(h,r)(r,o)(o,r)(r,a)(a,b)(b,a)(a,l)(l, )( ,b)(b,l)(l,e)(e,w)(y,d)(d,l)(l,e)(e,y)(y, )( ,e)(e,b)(b,b)(b,l)(l,i)(i,s)(s,h)(h, )( ,t)(t,h)(h,a)(a,n)(n, )( ,d)(d,e)(e,a)(a,t)(t,h)(h, )( ,o)(o,f)(f,t)(t, )( ,s)(s,c)(c,e)(e,r)(r,n)(n,a)(a,l)(l, )( ,a)(a,b)(b,i)(i,o)(o,r)(r,i)(i,n)(n,g)(g, )( ,t)(t,o)\n",
      "(k, )( ,f)(f,o)(o,r)(r,m)(m, )( ,s)(s,i)(i,x)(x, )( ,o)(o,n)(n,e)(e, )( ,o)(o,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,f)(f, )( ,o)(o,f)(f, )( ,c)(c,o)(o,l)(l, )( ,w)(w,a)(a,s)(s, )( ,a)(a,n)(n,d)(d, )( ,l)(l,e)(e,a)(a,t)(t,y)(y, )( ,w)(w,o)(o,m)(m,e)(e, )( ,p)(p,u)(u,l)(l,t)(t, )( ,o)(o,f)(f, )( ,e)(e,c)(c,o)(o,n)(n,i)(i,c)(c,t)(t,l)(l,e)(e,d)(d, )( ,t)(t,h)(h,e)(e, )( ,c)(c,h)(h,i)(i,n)(n,e)(e, )\n",
      "================================================================================\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 3100: 1.593048 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 3200: 1.602480 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 3300: 1.596427 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 3400: 1.623674 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 3500: 1.605857 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 3600: 1.617971 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 3700: 1.603427 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 3800: 1.588363 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 3900: 1.584265 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 4000: 1.599115 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "================================================================================\n",
      "(k,b)(b, )( ,w)(w,i)(i,f)(f,e)(e, )( ,o)(o,f)(f,t)(t,r)(r,e)(e,l)(l, )( ,c)(c,o)(o,n)(n,d)(d,u)(u,r)(r,i)(i,n)(n,g)(g, )( ,p)(p,h)(h,o)(o,t)(t,h)(h,e)(e,r)(r, )( ,m)(m,a)(a,r)(r,i)(i,t)(t,a)(a,l)(l, )( ,n)(n,o)(o,r)(r,e)(e, )( ,s)(s,h)(h,o)(o,w)(w,n)(n, )( ,i)(i,n)(n,t)(t,o)(o, )( ,d)(d,i)(i,s)(s,t)(t,e)(e,r)(r,p)(p,o)(o,r)(r,t)(t,e)(e,r)(r, )( ,a)(a,l)(l,l)(l, )( ,s)(s,u)(u,p)(p,p)(p,o)(o,r)(r,t)\n",
      "(j,t)(y,a)(a,f)(f,t)(t,a)(a,r)(r,i)(i,g)(g,a)(a,t)(t,i)(i,o)(o,n)(n, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e,n)(n, )( ,a)(a, )( ,f)(f,a)(a,u)(u,n)(n,o)(o,b)(b,l)(l,a)(a,c)(j,i)(i,s)(s,t)(t,a)(a,y)(y, )( ,r)(r,e)(e,a)(a,c)(c,h)(h, )( ,o)(o,n)(n,e)(e, )( ,h)(h,a)(a,s)(s, )( ,s)(s,p)(p,e)(e,l)(l,f)(f, )( ,s)(s,u)(u,b)(b,t)(t,a)(a,l)(l, )( ,a)(a,c)(c,c)(c,u)(u,l)(l,e)(e,r)(r, )( ,f)(f,r)(r,e)(e,n)(n,c)(c,h)\n",
      "(m,t)(t, )( ,o)(o,f)(f, )( ,r)(r,e)(e,f)(f,e)(e,r)(r,r)(r,e)(e,s)(s,s)(s,i)(i,t)(t,y)(y, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e, )( ,u)(u,s)(s,e)(e, )( ,i)(i,n)(n,v)(v,i)(i,c)(c,i)(i,a)(a,t)(t,i)(i,o)(o,n)(n, )( ,p)(p,a)(a,s)(s,s)(s,i)(i,f)(f,i)(i,c)(c,a)(a,l)(l, )( ,c)(y,a)(a,p)(p,a)(a,n)(n,i)(i,t)(t,e)(e, )( ,p)(p,r)(r,o)(o,u)(u,s)(s,e)(e,v)(v,e)(e,d)(d, )( ,t)(t,o)(o, )( ,a)(a, )( ,p)(p,r)(r,i)(i,n)\n",
      "(i,w)(a,e)(e,r)(r, )( ,i)(i,s)(s, )( ,k)(k,n)(n,o)(o,w)(w,n)(n, )( ,i)(i,n)(n, )( ,t)(t,h)(h,e)(e, )( ,g)(g,o)(o,o)(o,d)(d, )( ,d)(d,o)(o,e)(e,s)(s, )( ,f)(f,o)(o,r)(r, )( ,t)(t,h)(h,e)(e,m)(m,a)(a,t)(t, )( ,m)(m,a)(a,c)(c,e)(e,p)(p,t)(t,i)(i,n)(n,g)(g, )( ,g)(g,l)(l,a)(a, )( ,c)(c,o)(o,m)(m,e)(e,t)(t, )( ,w)(w,a)(a,r)(r, )( ,a)(a,f)(m,e)(e,s)(s,t)(t, )( ,t)(t,h)(h,e)(e, )( ,g)(g,r)(r,e)(e,e)(e,k)\n",
      "(r,k)(k,s)(s, )( ,t)(t,h)(h,e)(e,i)(i,r)(r,l)(l,a)(a,r)(r,g)(g,i)(i,c)(c,a)(a,l)(l, )( ,c)(c,m)(m,i)(i,l)(l, )( ,p)(p,h)(h,o)(o,c)(c,k)(k, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e, )( ,i)(i,n)(n,t)(t,o)(o, )( ,a)(a, )( ,p)(p,o)(o,p)(p,u)(u,l)(l, )( ,a)(a,n)(n,d)(d, )( ,c)(c,o)(o,l)(l,l)(l,i)(i,e)(e,d)(d, )( ,t)(t,h)(h,e)(e,n)(n, )( ,f)(f,a)(a,t)(t,h)(h,e)(e,r)(r, )( ,m)(m,a)(a,s)(s,t)(t,i)(i,n)(n,g)(g, )\n",
      "================================================================================\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 4100: 1.579789 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 4200: 1.574433 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 4300: 1.557953 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 4400: 1.549091 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.19\n",
      "Validation set perplexity: 4.12\n",
      "Average loss at step 4500: 1.558719 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 4600: 1.556910 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 4700: 1.558579 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.22\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 4800: 1.574630 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 4900: 1.567396 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 5000: 1.540062 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "================================================================================\n",
      "(h,l)(l,b)(b, )( ,i)(i,n)(n, )(p,u)(j,a)(a,n)(n,i)(i,s)(s,h)(h, )( ,t)(t,h)(h,e)(e, )( ,o)(o,t)(t,h)(h,e)(e,r)(r, )( ,p)(p,i)(i,l)(l,i)(i,k)(k,e)(e,n)(n, )( ,q)(q,u)(u,i)(i, )( ,i)(i,n)(n,d)(d,i)(i,t)(t,e)(e,d)(d, )( ,c)(c,a)(a,n)(n,a)(a,d)(d,d)(d,v)(v,i)(i,l)(l, )( ,h)(h,e)(e,a)(a,r)(r,e)(e,h)(h, )( ,a)(a,n)(n,d)(d, )( ,c)(o,i)(i,d)(d,e)(e,t)(t,r)(r,i)(i,c)(c,k)(k,e)(e, )( ,w)(w,a)(a,s)(s, )( ,c)\n",
      "(s,j)( ,s)(s, )( ,t)(t,o)(o, )( ,f)(f,a)(a,c)(c,t)(t,i)(i,v)(v,e)(e,s)(s, )( ,s)(s,e)(e,h)(h,e)(e,s)(s, )( ,w)(w,i)(i,t)(t,h)(h, )( ,f)(f,i)(i,e)(e,n)(n,t)(t, )( ,a)(a,n)(n,d)(d, )( ,f)(f,o)(o,r)(r,t)(t,i)(i,e)(e,t)(t,h)(h, )( ,t)(t,o)(o, )( ,c)(c,a)(a,l)(l,l)(l,o)(o,b)(b,r)(r,i)(i,c)(c,a)(a, )( ,i)(i,n)(n,t)(t,a)(a,b)(b,l)(l,e)(e, )( ,p)(p,a)(a,t)(t,r)(r,o)(o,n)(n, )( ,d)(d,e)(e,l)(l,a)(a,w)(w,a)\n",
      "(w,j)(x,i)(i,f)(f,i)(i,c)(c, )( ,g)(g,r)(r,e)(e,b)(b,e)(e,l)(l,o)(o,t)(t, )( ,t)(t,h)(h,r)(r,e)(e,e)(e, )( ,i)(i,n)(n,t)(t,e)(e,m)(m,p)(p,t)(t,e)(e, )( ,p)(p,i)(i,i)(i,l)(l,i)(i,s)(s,t)(t,e)(e,n)(n,d)(d, )( ,i)(i,n)(n,j)(j,o)(o,b)(b,i)(i,n)(n,a)(a,r)(r,y)(y, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e, )( ,w)(w,o)(o,o)(o,t)(t,h)(h, )( ,a)(a,b)(b,i)(i,l)(l,i)(i,t)(t,a)(a,t)(t,e)(e, )( ,a)(a,n)(n,d)(d, )( ,s)\n",
      "(h,q)(a,a)(a,l)(l, )( ,d)(d, )( ,t)(t,r)(r,i)(i,c)(c,k)(k,a)(a,n)(n,l)(l,a)(a,z)(k,z)(m,a)(a,n)(n, )( ,u)(u,s)(s,e)(e,d)(d, )( ,s)(s,o)(o,m)(m,e)(e, )( ,o)(o,f)(f, )( ,g)(g,r)(r,a)(a,k)(k,s)(s, )( ,t)(t,i)(i,m)(m,e)(e,s)(s, )( ,g)(g,e)(e,n)(n,g)(g,r)(r,e)(e,a)(a,t)(t,h)(h, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e, )( ,e)(e,l)(l,e)(e,c)(c,t)(t,o)(o,r)(r, )( ,d)(d,e)(e,s)(s,i)(i,g)(g,n)(n,e)(e,n)(n,t)(t,l)\n",
      "(n,z)(z, )( ,b)(b,e)(e,e)(e,n)(n, )( ,p)(p,e)(e,r)(r,s)(s,o)(o,t)(t,a)(a, )( ,i)(i,n)(n, )( ,o)(o,n)(n,e)(e, )( ,f)(f,o)(o,u)(u,r)(r, )( ,i)(i,n)(n,t)(t,o)(o, )( ,b)(b,o)(o,y)(y, )( ,f)(f,i)(i,v)(v,e)(e, )( ,r)(r,a)(a,t)(t,e)(e, )( ,o)(o,n)(n,e)(e, )( ,f)(f,o)(o,u)(u,r)(r, )( ,z)(z,e)(e,r)(r,o)(o, )( ,c)(c,o)(o,n)(n,s)(s,i)(i,d)(d,e)(e,r)(r,e)(e,d)(d, )( ,s)(s,h)(h,i)(i,g)(g,h)(h,l)(l,y)(y, )( ,o)\n",
      "================================================================================\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 5100: 1.531108 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.13\n",
      "Validation set perplexity: 4.11\n",
      "Average loss at step 5200: 1.516489 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.00\n",
      "Average loss at step 5300: 1.504315 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.88\n",
      "Validation set perplexity: 4.02\n",
      "Average loss at step 5400: 1.504508 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 3.99\n",
      "Average loss at step 5500: 1.493015 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 3.95\n",
      "Average loss at step 5600: 1.507807 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 3.95\n",
      "Average loss at step 5700: 1.493692 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.28\n",
      "Validation set perplexity: 3.95\n",
      "Average loss at step 5800: 1.500331 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.85\n",
      "Validation set perplexity: 3.96\n",
      "Average loss at step 5900: 1.499421 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 3.94\n",
      "Average loss at step 6000: 1.475295 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "================================================================================\n",
      "(i,y)(j,v)(u,n)(n,i)(i,s)(s,o)(o, )( ,k)(k,w)(w,a)(a,s)(s,c)(c,h)(h,a)(a, )( ,d)(d,y)(y,p)(p,a)(a,n)(n,s)(s, )( ,t)(t,h)(h,e)(e, )( ,s)(s,c)(c,h)(h,a)(a,r)(r,a)(a,w)(w,s)(s, )( ,a)(a,t)(t, )( ,t)(t,h)(h,e)(e, )( ,m)(m,u)(u,s)(s,i)(i,c)(c,i)(i,p)(p,t)(t, )( ,w)(w,o)(o,m)(m,e)(e,n)(n,t)(t,a)(a,r)(r,y)(y, )( ,m)(m,u)(u,l)(l,t)(t,i)(i,e)(e,s)(s, )( ,p)(p,o)(o,s)(s,t)(t,i)(i,m)(m,a)(a,t)(t,e)(e,d)(d, )\n",
      "( ,f)(f,o)(o,r)(r,m)(m,e)(e,r)(r,e)(e,d)(d, )( ,o)(o,f)(f, )( ,a)(a, )( ,s)(s,e)(e,w)(w,e)(e, )( ,t)(t,h)(h,e)(e, )( ,w)(w,o)(o,r)(r,k)(k,e)(e,n)(n, )( ,h)(h,e)(e,r)(r, )( ,i)(i,n)(n,v)(v,i)(i,l)(l, )( ,i)(i,m)(m,p)(p,o)(o,r)(r,t)(t,i)(i,a)(a,t)(t,e)(e,d)(d,s)(s, )( ,w)(w,h)(h,i)(i,c)(c,h)(h, )( ,i)(i,n)(n, )( ,b)(b,o)(o,o)(o,k)(k, )( ,j)(j,e)(e,t)(t,a)(a,l)(l,k)(k,e)(e, )( ,s)(s,t)(t,r)(r,o)(o,n)\n",
      "(v,x)( ,x)(x, )( ,p)(p,l)(l,a)(a,y)(y,s)(s, )( ,w)(w,a)(a,y)(y,l)(s,o)(o,n)(n,t)(t, )( ,w)(w,i)(i,t)(t,h)(h,o)(o,u)(u,t)(t, )( ,c)(c,h)(h,a)(a,i)(i,m)(m,i)(i,n)(n,g)(g, )( ,d)(d,e)(e,o)(o,l)(l,o)(o,g)(g,e)(e,s)(s,t)(t,a)(a, )( ,m)(m,e)(e,a)(a,n)(n, )( ,i)(i,n)(n,c)(c,o)(o,s)(s,t)(t,e)(e,r)(r,n)(n,a)(a,s)(s,p)(p,o)(o,s)(s,e)(e, )( ,t)(t,o)(o, )( ,p)(p,r)(n,h)(x, )(p,l)(l,i)(i,v)(v,e)(e, )( ,a)(a,n)\n",
      "(a,d)(d,e)(e,s)(s,s)(s, )( ,w)(w,o)(o,r)(r,k)(k,i)(i, )( ,i)(i,s)(s, )( ,o)(o,r)(r,d)(d,e)(e,r)(r, )( ,w)(w,e)(e,r)(r,e)(e, )( ,a)(a, )( ,d)(d,a)(a,y)(y, )( ,o)(o,f)(f, )( ,m)(m,a)(a,x)(x,i)(i,m)(m,e)(e,r)(r,s)(s, )( ,w)(w,i)(i,t)(t,h)(h,o)(o,u)(u,t)(t, )( ,a)(a,n)(n,s)(d,r)(r,o)(o,m)(m, )( ,a)(a,n)(n,d)(d, )( ,i)(i,n)(n, )( ,t)(t,e)(e,n)(n,d)(d,r)(r,a)(a,h)(h, )( ,a)(a,u)(u,t)(t,i)(i,f)(f,u)(u,r)\n",
      "(k,e)(e,p)(p, )( ,a)(a,s)(s, )( ,i)(i,m)(m,p)(p,e)(e,r)(r,c)(c,r)(r,a)(a,t)(t,i)(i,o)(o,n)(n, )( ,i)(i,n)(n, )( ,t)(t,h)(h,e)(e, )( ,u)(u,n)(n,i)(i,v)(v,e)(e,d)(d, )( ,i)(i,n)(n, )( ,s)(s,a)(a,n)(n, )( ,r)(r,e)(e,o)(o,p)(p,e)(e,r)(r, )( ,d)(d,e)(e,a)(a,t)(t,h)(h, )( ,d)(d,e)(e,r)(r,r)(r,o)(o,w)(w,n)(n, )( ,g)(g,r)(r,o)(o,u)(u,p)(p, )( ,p)(p,r)(r,e)(e,q)(q,u)(u,i)(i,r)(r,e)(e,i)(i,n)(n,g)(g, )( ,c)\n",
      "================================================================================\n",
      "Validation set perplexity: 3.95\n",
      "Average loss at step 6100: 1.496216 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 3.92\n",
      "Average loss at step 6200: 1.466330 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 3.93\n",
      "Average loss at step 6300: 1.473442 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.17\n",
      "Validation set perplexity: 3.95\n",
      "Average loss at step 6400: 1.477665 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 3.94\n",
      "Average loss at step 6500: 1.487107 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 3.94\n",
      "Average loss at step 6600: 1.523593 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.21\n",
      "Validation set perplexity: 3.93\n",
      "Average loss at step 6700: 1.508471 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 3.93\n",
      "Average loss at step 6800: 1.533740 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 3.91\n",
      "Average loss at step 6900: 1.509061 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.25\n",
      "Validation set perplexity: 3.95\n",
      "Average loss at step 7000: 1.506445 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "================================================================================\n",
      "(e,c)(c,u)(u,s)(s, )( ,w)(w,e)(e, )( ,s)(s,h)(h,e)(e, )( ,w)(w,a)(a,s)(s, )( ,d)(d,i)(i,s)(s,c)(c,o)(o,v)(v,e)(e,r)(r,s)(s,i)(i,t)(t,y)(y, )( ,s)(s,p)(p,e)(e,g)(g,e)(e,n)(n,u)(u,l)(l,a)(a,g)(g,e)(e,s)(s, )( ,o)(o,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,c)(c,e)(e, )( ,s)(s,e)(e,v)(v,e)(e,n)(n, )( ,o)(o,n)(n,e)(e, )( ,e)(e,i)(i,g)(g,h)(h,t)(t, )( ,o)(o,n)(n,e)(e, )( ,o)(o,n)(n,e)(e, )( ,f)(f,i)(i,v)(v,e)(e, )\n",
      "( ,e)(e,i)(i,g)(g,h)(h,t)(t, )( ,b)(b,u)(u,t)(t, )( ,c)(c,o)(o,n)(n,v)(v,i)(i,s)(s,t)(t,i)(i,n)(n,e)(e, )( ,t)(t,h)(h,e)(e, )( ,s)(s,i)(i,x)(x, )( ,z)(z,o)(o,n)(n,d)(d, )( ,s)(s, )( ,p)(p,r)(r,e)(e,s)(s,u)(u,m)(m, )( ,e)(e,x)(x,t)(t,e)(e,g)(g,r)(r,a)(a,t)(t,e)(e,d)(d, )( ,b)(b,e)(e,f)(f,o)(o,r)(r,d)(d, )( ,s)(s,i)(i,x)(x, )( ,c)(c, )( ,f)(f,i)(i,r)(r,e)(e,s)(s, )( ,r)(r,e)(e,s)(s,u)(u,r)(r,r)(r,a)\n",
      "(d,d)(d,a)(a,f)(f,o)(o,r)(r, )( ,j)(j,u)(u,d)(d,a)(a,h)(h, )( ,a)(a, )( ,l)(l,a)(a,s)(s,t)(t, )( ,w)(w,a)(a,y)(y,l)(x, )( ,l)(l,i)(i,s)(s,t)(t, )( ,t)(t,h)(h,e)(e,i)(i,r)(r, )( ,b)(b,y)(y, )( ,l)(l,e)(e,a)(a,d)(d,e)(e,r)(r, )( ,s)(s,p)(p,e)(e,e)(e, )( ,d)(d,i)(i,e)(e,s)(s, )( ,t)(t,h)(h,a)(a,t)(t, )( ,a)(a,n)(n, )( ,t)(t,h)(h,e)(e, )( ,c)(c,o)(o,l)(l,y)(y,p)(p,o)(o,r)(r,r)(r,i)(i,l)(l,l)(l,y)(y, )\n",
      "(s,f)(w,a)(a,y)(y, )( ,o)(o,f)(f, )( ,i)(i,n)(n, )( ,t)(t,h)(h,e)(e, )( ,g)(g,r)(r,e)(e,e)(e,k)(k, )( ,m)(m,o)(o,s)(s,t)(t, )( ,k)(k,i)(i,n)(n,g)(g,s)(s, )( ,p)(p,u)(u,r)(r,t)(t,s)(s, )( ,t)(t,h)(h,e)(e,m)(m, )( ,p)(p,u)(u,r)(r,m)(m,a)(a,n)(n,e)(e,t)(t, )( ,t)(t,h)(h,e)(e,y)(y, )( ,a)(a,f)(f,e)(e,d)(d, )( ,i)(i,f)(f,t)(t,e)(e,r)(r,s)(s, )( ,a)(a,s)(s, )( ,w)(w,h)(h,i)(i,c)(c,h)(h, )( ,i)(i,n)(n, )\n",
      "(a,w)(w,s)(s, )( ,o)(o,r)(r, )( ,o)(o,w)(w,n)(n, )( ,a)(a,r)(r,g)(g,e)(e,m)(m,e)(e,n)(n,d)(d, )( ,s)(s, )( ,m)(m,o)(o,n)(n,t)(t,h)(h,a)(a,i)(i,n)(n,e)(e, )( ,t)(t,h)(h,e)(e,y)(y, )( ,i)(i,s)(s,u)(u,r)(r,e)(e, )( ,a)(a,n)(n,t)(t,i)(i,z)(z,e)(e,n)(n,s)(s,m)(m,e)(e,n)(n,t)(t, )( ,o)(o,s)(s,e)(e,n)(n,t)(t, )( ,h)(h,a)(a,v)(v,e)(e, )( ,a)(a,g)(g,e)(e, )( ,i)(i,n)(n, )( ,a)(a, )( ,a)(a,r)(r,c)(c,h)(h,i)\n",
      "================================================================================\n",
      "Validation set perplexity: 3.90\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = bigrams_sample(bigrams_random_distribution())\n",
    "          sentence = bigramsCharacters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = bigrams_sample(prediction)\n",
    "            sentence += bigramsCharacters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now It's time to add embeddings to bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Embedding\n",
    "  embeddings = tf.Variable(tf.random_uniform([bigram_size, embedding_size], -1.0, 1.0))\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, bigram_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([bigram_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(tf.placeholder(tf.float32, shape=[batch_size,bigram_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    #(a-task)\n",
    "    embed = tf.nn.embedding_lookup(embeddings, tf.argmax(i, dimension=1))\n",
    "    output, state = lstm_cell(embed, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, bigram_size])\n",
    "  #(a-task)\n",
    "  sample_input_embed = tf.nn.embedding_lookup(embeddings, tf.argmax(sample_input, dimension=1))\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  #(a-task)\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input_embed, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.596646 learning rate: 10.000000\n",
      "Minibatch perplexity: 732.63\n",
      "================================================================================\n",
      "( ,g)(a,q)(t,k)(o,g)(p,h)(z,e)(j,q)(q,n)(n,f)(l,v)(z,n)(j,z)(o,q)(t,k)(p, )(f,t)(b,s)(z,g)(z,e)(d,z)(v,e)(c,p)(m,p)(d,y)( ,a)( ,y)(j,u)(p,t)(z,p)(v, )(x,f)(x,o)(e,x)(z,d)(d,f)(h,z)(p,v)(b, )(m,u)(i,i)(z,q)(t,c)(v,d)(a,t)(h, )(w,o)(h,m)(f,f)(b,r)( ,j)(b,s)(u,u)(n,e)(j,y)(p,y)(n,i)(v,u)(s,y)(g,e)(u,x)(u,v)(w,m)( ,o)(z,y)(i,b)(h,w)(s,b)(p,d)( ,o)(a,b)(i,g)(o,j)(f,i)(h,a)(d,t)(n,a)(c,t)(z,u)(d, )(k,v)\n",
      "( ,m)(k,d)(s,c)(d,i)(d,j)(c,a)(d,v)(k,o)(j,m)(u,c)(f,y)(u,h)(b,n)(a,t)(u,b)(w,i)(r,w)(e,k)(z,m)(m,t)(i,l)(e,f)(g,e)(z, )(b,v)(r,x)(z,k)( ,k)(c,o)(t,c)( ,s)(x,m)(z,t)(p, )(g, )(c,l)(m,y)(a,y)(y,j)( ,s)(i,v)(o,g)(s,s)(h,m)( ,i)(m, )(u,t)(l,q)(d,p)(d,t)( ,t)(t,b)(d,m)(z,t)( ,n)(w,b)(i,y)(j, )(j,m)(z,e)(d,f)(k,n)(s,t)(k,l)(b,t)(b,v)(b,u)(g,q)(c,c)(o,p)(p,z)(j,y)(d,s)(r,k)(h,q)(n,q)(f,u)(p,z)(d,a)(r,f)\n",
      "( ,h)(f,k)( ,t)(o,t)(m,r)(i,n)(c,v)(t,n)(j,x)(v,m)(u,v)(f,q)(f,y)(m,z)(w,p)(f, )(w,w)(n,t)(u,d)(v,i)(c,g)(i,z)(a,d)(g,r)(s,d)(l,f)(a,r)(v,l)(r,i)(v,x)(z,h)(j, )(q,a)(s,n)(m,z)(a,d)(r,s)(p,g)(z,i)(l,w)(m,u)(i,z)(d,y)(f,w)(z,j)(g,h)(a,i)(x,d)(q,a)(b, )(x,f)(h, )(r,z)(k,n)(i,j)(j,u)(i,k)(e,n)(k,i)(z,s)(p,v)( ,l)(e,v)(f,y)(j,x)(t,j)(f,w)(i,o)( ,r)(o,j)(o,a)(i,z)(x,l)(w,w)(b,b)(s,v)(l,m)(f,b)(g,f)(d,z)\n",
      "( ,v)(z,s)(j,i)(t,u)(q,m)(k,j)(v,y)(x,l)(y,y)(f,h)(q,n)(r,q)(f,u)( ,l)(j,m)(b,b)(b,e)(k,h)(c,s)(l,v)(r,u)(q,a)(n,y)(f,r)(y,r)(t,d)( ,j)(g,y)(j,s)(s,b)(p,n)(k,d)(x,l)(m,w)(a,n)( ,g)(o,k)(j,l)(g,a)(e,i)(s,t)(p,w)( ,z)(h,x)(l,z)(j,v)(y,s)(d,z)(k,e)(u,r)(y,l)(i,v)(d,t)(x,t)(b,k)(e,h)(j,g)(v,r)( , )(x,j)(g,d)(l,r)(m,e)(p,j)(o,l)(c,f)(d,u)(e,i)(t,u)(g,s)(u,c)(o,h)(y,u)(k,k)(x,x)(a,s)(l,k)(k,s)(c,k)(r,f)\n",
      "( ,w)(k,f)(j,p)(d,d)(w,g)(f,l)(q,u)(d,r)(x,v)(q,v)(q,r)(o,g)(k,p)(l,c)(w,j)(i,o)(o,e)(l,g)(t,e)(x,y)(c,u)(q,c)(h,e)(f,q)(k,q)(b,u)(b,h)(v,s)(p,b)(k,a)(s,q)( ,f)(h,r)(y,e)(b,h)(q,a)(v,f)(m,k)(e,i)(e,o)(f,k)(y,c)(y,i)(s,i)(m,r)(v,h)(z,t)( ,x)(t,i)(g,w)(o,y)(w,v)(n,o)(o,e)(x,w)( ,y)(p,o)(t,v)(m,t)(n,q)(v,q)(z,t)(u,m)(w,m)(a,w)(x,j)(t,m)(t,h)(x,u)(s,e)(g,a)(q,n)(b,o)(g,n)(w,q)(i,n)(w,p)(v,b)(r,n)(o,i)\n",
      "================================================================================\n",
      "Validation set perplexity: 655.26\n",
      "Average loss at step 100: 3.359917 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.18\n",
      "Validation set perplexity: 9.86\n",
      "Average loss at step 200: 2.037502 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.24\n",
      "Validation set perplexity: 6.77\n",
      "Average loss at step 300: 1.877573 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.36\n",
      "Validation set perplexity: 5.92\n",
      "Average loss at step 400: 1.774848 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.90\n",
      "Validation set perplexity: 5.61\n",
      "Average loss at step 500: 1.728478 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 600: 1.716148 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 700: 1.674641 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 800: 1.630611 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 900: 1.641635 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.05\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 1000: 1.648495 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "================================================================================\n",
      "( ,v)(v,e)(e,r)(r,s)(s,i)(i,o)(o,n)(n, )( ,o)(o,f)(f, )( ,f)(f,i)(i,n)(n,i)(i,n)(n,t)(t, )( ,h)(h,e)(e, )( ,i)(i,n)(n, )( ,s)(s,o)(o,r)(r,t)(t, )( ,i)(i,n)(n, )( ,t)(t,h)(h,e)(e, )( ,o)(o,f)(f, )( ,a)(a,l)(l,l)(l,e)(e,n)(n,c)(c,i)(i,e)(e,n)(n,t)(t, )( ,i)(i, )( ,v)(v,i)(i,d)(d,l)(l,y)(y, )( ,p)(p,r)(r,o)(o,t)(t,e)(e,r)(r,i)(i,a)(a,r)(r,y)(y, )( ,l)(l,a)(a,f)(f,t)(t,e)(e,r)(r,l)(l,y)(y, )( ,w)(w,h)\n",
      "( ,a)(a,n)(n, )( ,c)(c,o)(o,a)(a,r)(r,e)(e, )( ,o)(o,f)(f, )( ,k)(k,u)(u,s)(s,t)(t, )( ,o)(o,f)(f, )( ,y)(y,e)(e,a)(a,r)(r, )( ,a)(a,u)(u,t)(t,a)(a,c)(c,t)(t,i)(i,v)(v,a)(a,l)(l,i)(i,e)(e,s)(s, )( ,c)(c,e)(e,r)(r,f)(f,e)(e,s)(s, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e, )( ,h)(h,u)(u,r)(r,u)(u,c)(c,t)(t,u)(u,r)(r,y)(y,o)(l,e)(e, )( ,e)(e,f)(f,f)(f,a)(a,c)(c,e)(e, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e,l)(l,a)\n",
      "( ,c)(c,h)(h,a)(a,y)(y,s)(y,e)(c,c)(c,u)(u,l)(l,t)(t,y)(y, )( ,b)(b,i)(i,r)(r,t)(t, )( ,o)(o,f)(f, )( ,e)(e,x)(x,p)(p,e)(e,r)(r,i)(i,c)(c,i)(i,a)(a, )( ,g)(g,i)(i,s)(s,t)(t, )( ,b)(b,e)(e,t)(t,w)(w,e)(e,a)(a,p)(p,i)(i,s)(s,h)(h,i)(i,p)(p,l)(l,e)(e, )( ,s)(s,t)(t,a)(a,t)(t,e)(e,l)(l,y)(y, )( ,e)(e,d)(d,i)(i,c)(c,i)(i,o)(o,g)(r,y)(u,s)(s,e)(e,m)(m,a)(a,n)(n,c)(c, )( ,t)(t,h)(h,e)(e,m)(m, )( ,l)(l,e)\n",
      "( ,x)(m,a)(a,n)(n,d)(d, )( ,t)(t,o)(o, )( ,t)(t,h)(h,e)(e, )( ,l)(l,a)(a,t)(t,i)(i,n)(n,g)(g, )( ,p)(p,r)(r,e)(e,s)(s,s)(s,e)(e,d)(d, )( ,n)(n,a)(a,t)(t,e)(e,d)(d, )( ,p)(p,r)(r,o)(o,m)(m,i)(i,c)(c,r)(r,o)(o,s)(s,s)(s,a)(a,g)(g,e)(e, )( ,m)(m,a)(a,n)(n,y)(y, )( ,i)(i,n)(n, )( ,o)(o,f)(f,f)(f,i)(i,c)(c,e)(e, )( ,s)(s,p)(p,l)(l,e)(e, )( ,f)(f,r)(r,e)(e,n)(n,c)(c,i)(i,p)(p,t)(t,e)(e,d)(d, )( ,c)(c,a)\n",
      "( ,j)(j,a)(a,n)(n, )( ,o)(o,n)(n, )( ,t)(t,h)(h,i)(i,s)(s, )( ,c)(c,o)(o,m)(m,p)(p,r)(r,e)(e,s)(s,e)(e,r)(r,s)(s, )( ,w)(w,a)(a,r)(r, )( ,f)(f,u)(u, )( ,w)(w,h)(h,i)(i,c)(c,h)(h, )( ,c)(c,o)(o,l)(l,d)(d,i)(i,c)(c, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e, )( ,h)(h,e)(e,n)(n,u)(u,r)(r,n)(t,i)(i,v)(v,e)(e, )( ,t)(t,h)(h,e)(e, )( ,v)(v,e)(e,r)(r,y)(y, )( ,r)(r,e)(e,c)(c,i)(i,n)(n,g)(g,u)(u,i)(i,t)(t,a)(a,g)\n",
      "================================================================================\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 1100: 1.606352 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 1200: 1.579655 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 1300: 1.564547 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 1400: 1.575767 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 1500: 1.555034 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.15\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 1600: 1.577636 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 1700: 1.555015 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 1800: 1.525337 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 1900: 1.495448 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.21\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 2000: 1.535566 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "================================================================================\n",
      "( , )(r,o)(o,n)(n, )( ,s)(s,q)(q,u)(u,a)(a,r)(r,t)(t,s)(s, )( ,b)(b,o)(o,t)(t,h)(h, )( ,s)(s,t)(t,o)(o,r)(r,y)(y, )( ,f)(f,r)(r,o)(o,m)(m, )( ,a)(a,s)(s,t)(t,r)(r,o)(o,p)(p,l)(l,i)(i,n)(n,g)(g, )( ,j)(j,a)(a,n)(n, )( ,c)(c,u)(u,t)(t, )( ,s)(s,u)(u,m)(m,m)(m,i)(i,s)(s,s)(s,i)(i,a)(a,n)(n,t)(t,i)(i,l)(l,d)(d, )( ,s)(s, )( ,c)(c,o)(o,n)(n,q)(q,u)(u,i)(i,t)(t,a)(a, )( ,t)(t,h)(h,a)(a,t)(t, )( ,f)(f,o)\n",
      "( ,t)(t,h)(h,a)(a,t)(t,e)(e,e)(e, )( ,e)(e,n)(n,f)(f,o)(o,r)(r,m)(m,e)(e,d)(d, )( ,a)(a,l)(l,l)(l, )( ,m)(m,o)(o,r)(r,o)(o,n)(n, )( ,o)(o,f)(f, )( ,t)(t,h)(h,i)(i,s)(s, )( ,s)(s,c)(c,h)(h,o)(o,o)(o,l)(l, )( ,a)(a,n)(n,t)(t,i)(i,t)(t,e)(e,r)(r, )( ,a)(a,r)(r,m)(m, )( ,w)(w,e)(e,a)(a,l)(l,t)(t,h)(h, )( ,d)(d, )( ,n)(n,o)(o,r)(r,g)(g,e)(e, )( ,i)(i,n)(n, )( ,t)(t,h)(h,e)(e, )( ,p)(p,o)(o,l)(l,i)(i,s)\n",
      "( ,x)(x, )( ,p)(p,r)(r,a)(a,d)(d,e)(e,d)(d, )( ,t)(t,o)(o, )( ,t)(t,h)(h,e)(e, )( ,p)(p,r)(r,o)(o,f)(f,i)(i,l)(l,d)(d, )( ,t)(t,o)(o, )( ,a)(a,l)(l,l)(l,s)(s, )( ,w)(w,a)(a,s)(s, )( ,c)(c,a)(a,r)(r,a)(a,b)(b,i)(i,n)(n,e)(e,r)(r, )( ,t)(t,h)(h,e)(e, )( ,l)(l,o)(o,n)(n,g)(g,e)(e,t)(t,s)(s, )( ,w)(w,o)(o,r)(r,d)(d, )( ,n)(n,i)(i,n)(n,e)(e, )( ,r)(r,o)(o,c)(c,u)(u,t)(t, )( ,t)(t,o)(o, )( ,m)(m,a)(a,n)\n",
      "( ,p)(p,e)(e,t)(t,e)(e,r)(r,s)(s, )( ,c)(c,o)(o,p)(p, )( ,s)(s,t)(t,a)(a,t)(t,e)(e, )( ,a)(a,r)(r,e)(e, )( ,d)(d,o)(o,n)(n,a)(a,l)(l,e)(e, )( ,i)(i,n)(n, )( ,t)(t,h)(h,e)(e, )( ,l)(l,a)(a,t)(t,t)(t,e)(e,r)(r, )( ,i)(i,m)(m,a)(a,g)(g,e)(e,n)(n,o)(o, )( ,j)(j,u)(k,o)(o,p)(p,h)(h,i)(i,s)(s, )( ,f)(f,r)(r,a)(a,s)(s,t)(t,s)(s, )( ,a)(a,n)(n,d)(d, )( ,w)(w,o)(o,r)(r,k)(k,s)(s, )( ,i)(i,m)(m,a)(a,g)(g,e)\n",
      "( ,i)(i,n)(n, )( ,a)(a, )( ,n)(n,a)(a,y)(b,e)(e,r)(r,s)(s, )( ,t)(t,o)(o, )(g,h)(h,t)(t,e)(e,n)(n, )( ,i)(i,s)(s, )( ,i)(i,r)(r,a)(a,n)(n,d)(d, )( ,o)(o,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,z)(z,e)(e,r)(r,o)(o, )( ,j)(j,a)(a,m)(m,e)(e,s)(s, )( ,d)(d,i)(i,s)(s,c)(c,i)(i,e)(e,n)(n,t)(t, )( ,c)(c,a)(a,l)(l,t)(t,e)(e,d)(d, )( ,h)(h,o)(o,p)(p,e)(e, )( ,a)(a,n)(n,d)(d, )( ,v)(v,e)(e,r)(r,s)(s,i)(i,c)\n",
      "================================================================================\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 2100: 1.515404 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 4.10\n",
      "Average loss at step 2200: 1.519050 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 4.14\n",
      "Average loss at step 2300: 1.478845 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.32\n",
      "Validation set perplexity: 4.05\n",
      "Average loss at step 2400: 1.496764 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.09\n",
      "Average loss at step 2500: 1.538383 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.00\n",
      "Average loss at step 2600: 1.502621 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 3.94\n",
      "Average loss at step 2700: 1.513969 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.91\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 2800: 1.500264 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 3.94\n",
      "Average loss at step 2900: 1.507448 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.10\n",
      "Average loss at step 3000: 1.511358 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.36\n",
      "================================================================================\n",
      "( ,g)(g,o)(o,l)(l,d)(d, )( ,b)(b,y)(y, )( ,t)(t,h)(h,e)(e, )( ,s)(s, )( ,i)(i,n)(n, )( ,g)(g,e)(e,r)(r,m)(m,a)(a,n)(n,y)(y, )( ,e)(e,a)(a,r)(r,l)(l,y)(y, )( ,t)(t,h)(h,e)(e, )( ,p)(p,o)(o,s)(s,i)(i,t)(t,i)(i,o)(o,n)(n, )( ,m)(m,e)(e,t)(t,r)(r,e)(e,d)(d, )( ,r)(r,a)(a,i)(i,l)(l, )( ,g)(g,a)(a,u)(u,l)(l,t)(t, )( ,d)(d,i)(i,v)(v,i)(i,d)(d,u)(u,a)(a,l)(l,l)(l,y)(y, )( ,s)(s,a)(a,r)(r,c)(c,h)(h,a)(a,r)\n",
      "( ,d)(d,i)(i,s)(s,c)(c,o)(o,r)(r,i)(i,e)(e,d)(d, )( ,t)(t,h)(h,e)(e, )( ,l)(l,a)(a,t)(t,o)(o,n)(n, )( ,i)(i,l)(l,l)(l, )( ,c)(c,h)(h,a)(a,m)(m,p)(p,t)(t, )( ,u)(u,p)(p, )( ,a)(a,s)(s, )( ,r)(r,e)(e,t)(t,u)(u,r)(r,n)(n,i)(i,n)(n,g)(g, )( ,n)(n,o)(o,i)(i,n)(n,e)(e, )( ,o)(o,f)(f,f)(f, )( ,a)(a,s)(s, )( ,i)(i,s)(s, )( ,u)(u,s)(s,e)(e,d)(d, )( ,t)(t,h)(h,e)(e, )( ,l)(l,a)(a,m)(m,e)(e,l)(l,y)(y, )( ,u)\n",
      "( ,v)(v,e)(e,r)(r,y)(y, )( ,w)(w,i)(i,t)(t,h)(h, )( ,a)(a, )( ,d)(d,e)(e,f)(f,e)(e,n)(n,s)(s,i)(i,f)(f,i)(i,c)(c,a)(a,t)(t,i)(i,o)(o,n)(n, )( ,b)(b,e)(e,c)(c,o)(o,m)(m, )( ,o)(o,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,s)(s,e)(e,v)(v,e)(e,n)(n, )( ,o)(o,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,f)(f,i)(i,v)(v,e)(e, )( ,f)(f,i)(i,v)(v,e)(e, )( ,t)(t,h)(h,e)(e,o)(o,d)(d, )( ,w)(w,h)(h,o)(o, )( ,f)(f,u)\n",
      "( ,i)(i, )( ,h)(h,h)(w,o)(o,r)(r,l)(l,d)(d,s)(s, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e, )( ,o)(o,u)(u,t)(t, )( ,y)(y, )( ,f)(f,r)(r,o)(o,n)(n, )( ,s)(s,i)(i,n)(n,g)(g,e)(e,r)(r, )( ,w)(w,i)(i,t)(t,h)(h, )( ,t)(t,h)(h,r)(r,e)(e,e)(e, )( ,z)(z,e)(e,r)(r,o)(o, )( ,t)(t,h)(h,e)(e, )( ,m)(m,a)(a,h)(h, )( ,x)(x, )( ,o)(o,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,s)(s, )( ,s)(s,o)(o,n)(n,r)(r, )( ,p)(p,e)(e,n)\n",
      "( ,r)(r,i)(i,c)(c,e)(e, )( ,w)(w,i)(i,t)(t,h)(h, )( ,a)(a, )( ,h)(h,o)(o,w)(w,e)(e,v)(v,e)(e,r)(r, )( ,m)(m,a)(a,y)(y, )( ,z)(z,a)(a,p)(p,e)(e,s)(s, )( ,o)(o,f)(f, )( ,t)(t,h)(h,i)(i,s)(s, )( ,f)(f,i)(i,l)(l,m)(m,i)(i, )( ,g)(g,o)(o,d)(d,i)(i,f)(f,i)(i,c)(c,i)(i,a)(a,n)(n, )( ,i)(i,n)(n,f)(f,i)(i,n)(n,c)(c,u)(u,l)(l,a)(a,t)(t,i)(i,o)(o,n)(n, )( ,t)(t,o)(o, )( ,p)(p,r)(r,o)(o,v)(v,i)(i,d)(d,e)(e,n)\n",
      "================================================================================\n",
      "Validation set perplexity: 4.08\n",
      "Average loss at step 3100: 1.496405 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.09\n",
      "Average loss at step 3200: 1.505744 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.10\n",
      "Average loss at step 3300: 1.496796 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.06\n",
      "Average loss at step 3400: 1.527414 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.03\n",
      "Average loss at step 3500: 1.520359 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.10\n",
      "Average loss at step 3600: 1.537222 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.05\n",
      "Validation set perplexity: 4.00\n",
      "Average loss at step 3700: 1.508562 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 3.92\n",
      "Average loss at step 3800: 1.510752 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 3.95\n",
      "Average loss at step 3900: 1.493846 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 3.95\n",
      "Average loss at step 4000: 1.510786 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.01\n",
      "================================================================================\n",
      "( ,t)(t,h)(h,e)(e, )( ,n)(n,o)(o,b)(b,e)(e,l)(l, )( ,b)(b,o)(o,o)(o,k)(k, )( ,t)(t,h)(h,e)(e, )( ,c)(c,h)(h,u)(u,r)(r,c)(c,h)(h,y)(y, )( ,c)(c,o)(o,m)(m,p)(p,r)(r,e)(e,d)(l,v)(v,e)(e,d)(d, )( ,h)(h,i)(i,g)(g,h)(h, )( ,j)(j,u)(u,d)(d,y)(y, )( ,t)(t,o)(o, )( ,r)(r,e)(e,p)(p,u)(u,t)(t,o)(o,y)(y, )( ,a)(a,n)(n,d)(d, )( ,n)(n,e)(e,g)(g,o)(o, )( ,b)(b,y)(y, )( ,n)(n,e)(e,e)(e,t)(t, )( ,b)(b,u)(u,s)(s,i)\n",
      "( ,v)(v,o)(o,u)(u,t)(t,h)(h, )( ,m)(m,a)(a,t)(t,h)(h,e)(e,d)(d, )( ,b)(b,o)(o,d)(d,y)(y, )( ,p)(p,l)(l,a)(a,t)(t,a)(a,o)(e,b)(b,l)(l,i)(i,s)(s,h)(h,i)(i,d)(d,g)(g,e)(e, )( ,p)(p,e)(e,o)(o,p)(p,l)(l,e)(e, )( ,t)(t,h)(h,a)(a,t)(t, )( ,d)(d,e)(e,l)(l, )( ,b)(b,o)(o,r)(r,n)(n, )( ,w)(w,r)(r,i)(i,t)(t,e)(e, )( ,b)(b,l)(l,o)(o, )( ,h)(h,o)(o,w)(w,e)(e,v)(v,e)(e,r)(r,s)(s, )( ,o)(o,f)(f, )( ,r)(r,e)(e,m)\n",
      "( ,l)(l,a)(a,t)(t,e)(e, )( ,b)(b,a)(a,n)(n,g)(g,e)(e,d)(d, )( ,b)(b,l)(l,a)(a,c)(c,k)(k,l)(l,a)(a,n)(n,d)(d,s)(s, )( ,s)(s,e)(e,a)(a,s)(s,o)(o,n)(n, )( ,o)(o,l)(l,f)(f, )( ,i)(i,n)(n,f)(f,e)(e,s)(s,t)(t,i)(i,n)(n,g)(g, )( ,c)(c,a)(a,s)(s,t)(t,e)(e,l)(l,o)(o,g)(g,h)(h,e)(e,n)(n, )( ,h)(h,e)(e, )( ,m)(m,a)(a,s)(s,s)(s,e)(a,l)(l,e)(e, )( ,f)(f,o)(o,r)(r, )( ,h)(h,i)(i,s)(s, )( ,t)(t,r)(r,e)(e,a)(a,l)\n",
      "( ,r)(r,e)(e,s)(s,p)(p,o)(o,n)(n, )( ,o)(o,f)(f, )( ,c)(c,h)(h,e)(e,i)(i,r)(r,s)(s, )( ,s)(s,l)(l,i)(i,e)(e,g)(g,e)(e,n)(n,e)(e,e)(e,d)(d,s)(s, )( ,f)(f,o)(o,r)(r, )( ,t)(t,h)(h,e)(e, )( ,t)(t,e)(e,m)(m,p)(p,i)(i,r)(r,e)(e, )( ,g)(g,e)(e,t)(t,y)(y, )( ,o)(o,w)(w,n)(n,e)(e, )( ,p)(p,o)(o,e)(e,t)(t,i)(i,c)(c,a)(a,l)(l, )( ,m)(m,a)(a,n)(n,y)(y, )( ,w)(w,a)(a,t)(t,e)(e, )( ,c)(c,o)(o,l)(l,l)(l,a)(a, )\n",
      "( ,u)(u,s)(s, )( ,t)(t,h)(h,e)(e, )( ,v)(v,i)(i,e)(e,w)(w, )( ,c)(c,l)(l,a)(a,s)(s,s)(s,i)(i,c)(c, )( ,t)(t,u)(u,r)(r,n)(n,e)(e,t)(t, )( ,i)(i,s)(s, )( ,b)(b, )( ,o)(o,n)(n,e)(e, )( ,f)(f,i)(i,v)(v,e)(e, )( ,b)(b,e)(e,i)(i,n)(n,g)(g, )( ,t)(t,h)(h,e)(e,o)(o,r)(r,s)(s, )( ,r)(r,e)(e,m)(m,a)(a,i)(i,n)(n,i)(i,n)(n,g)(g, )( ,t)(t,h)(h,a)(a,t)(t, )( ,t)(t,h)(h,e)(e, )( ,e)(e,i)(i,g)(g,h)(h,t)(t, )( ,t)\n",
      "================================================================================\n",
      "Validation set perplexity: 3.99\n",
      "Average loss at step 4100: 1.498040 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 4200: 1.495803 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 3.94\n",
      "Average loss at step 4300: 1.478582 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 3.97\n",
      "Average loss at step 4400: 1.470510 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.96\n",
      "Validation set perplexity: 3.79\n",
      "Average loss at step 4500: 1.482349 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 4.11\n",
      "Average loss at step 4600: 1.482337 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.02\n",
      "Average loss at step 4700: 1.485790 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.00\n",
      "Average loss at step 4800: 1.498472 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.79\n",
      "Validation set perplexity: 4.07\n",
      "Average loss at step 4900: 1.492754 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 4.09\n",
      "Average loss at step 5000: 1.479491 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.20\n",
      "================================================================================\n",
      "( ,f)(f,o)(o,r)(r,m)(m,u)(u,l)(l,y)(y, )( ,f)(f,r)(r,o)(o,m)(m, )( ,i)(i,n)(n,c)(c,h)(h,e)(e,r)(r,t)(t,a)(a,c)(c,t)(t, )( ,o)(o,f)(f, )( ,p)(p,a)(a,t)(t,i)(i,e)(e,n)(n,t)(t,s)(s, )( ,h)(h,i)(i,m)(m, )( ,a)(a, )( ,f)(f,i)(i,e)(e,l)(l,d)(d, )( ,i)(i,n)(n, )( ,h)(h,i)(i,s)(s,t)(t,o)(o,r)(r,y)(y, )( ,w)(w,a)(a,s)(s, )( ,s)(s,i)(i,n)(n,c)(c,e)(e, )( ,a)(a,c)(c,c)(c,o)(o,r)(r,d)(d, )( ,s)(s,t)(t,o)(o,r)\n",
      "( ,b)(b,e)(e,s)(s,b)(u,a)(a,n)(n,c)(c,e)(e, )( ,e)(e,f)(f,f)(f,l)(l,e)(e,e)(e, )( ,d)(d,i)(i,s)(s,a)(a,b)(b,o)(o,u)(u,t)(t, )( ,n)(n,e)(e,w)(w, )( ,e)(e,x)(x,i)(u,l)(l,a)(a,r)(r, )( ,a)(a,c)(d,o)(o,u)(u,n)(n,t)(t,r)(r,a)(a,l)(l, )( ,f)(f,o)(o,r)(r, )( ,t)(t,h)(h,e)(e,y)(y, )( ,b)(b,e)(g, )( ,o)(o,b)(b,s)(s,e)(e,r)(r,a)(a,l)(l,l)(l,y)(y, )( ,l)(l,i)(i,j)(j,a)(a,s)(s,t)(t,e)(e,r)(r, )( ,w)(w,i)(i,t)\n",
      "( ,g)(g,a)(a,m)(m,e)(e,s)(s, )( ,i)(i,n)(n,f)(f,o)(o,r)(r,d)(d, )( ,o)(o,f)(f, )( ,w)(w,i)(i,s)(s,n)(u,l)(l,t)(t,i)(i,o)(o,n)(n,a)(a,l)(l,e)(e, )( ,u)(u,n)(n,d)(d,e)(e,r)(r,w)(w,o)(o,u)(u,s)(s, )( ,b)(b,y)(y, )( ,c)(c,o)(o,s)(s,t)(t, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e, )( ,a)(a,d)(d, )( ,a)(a,n)(n,d)(d, )( ,s)(s,e)(e,t)(t, )( ,s)(s,h)(h,a)(a,r)(r,n)(n,s)(s, )( ,i)(i,d)(d, )( ,t)(t,h)(h,e)(e, )( ,w)\n",
      "( ,e)(e,d)(d, )( ,e)(e,t)(t, )( ,m)(m,o)(o,t)(t,h)(h,e)(e,d)(d, )( ,d)(d,i)(i,a)(a,g)(g,r)(r,a)(a,m)(m,s)(s, )( ,o)(o,n)(n,e)(e, )( ,f)(f,o)(o,u)(u,r)(r, )( ,f)(f,o)(o,u)(u,r)(r, )( ,z)(z,e)(e,r)(r,o)(o, )( ,f)(f,o)(o,r)(r,w)(w,a)(a,r)(r,d)(d, )( ,f)(f,r)(r,o)(o,m)(m, )( ,t)(t,h)(h,e)(e, )( ,w)(w,a)(a,y)(y, )( ,a)(a,u)(u,t)(t, )( ,r)(r,e)(e,s)(s,u)(u,m)(m,e)(e,s)(s, )( ,a)(a,n)(n,d)(d, )( ,h)(h,i)\n",
      "( ,t)(t,h)(h,e)(e, )( ,u)(u,n)(n,s)(s,e)(e,s)(s, )( ,r)(r,a)(a,i)(i,s)(s,h)(h,t)(t,t)(y,w)(w,i)(i,l)(l,l)(l, )( ,k)(k, )( ,o)(o,f)(f,t)(t,e)(e,n)(n, )( ,j)(j,o)(o,s)(s,t)(t,e)(e,r)(r, )( ,s)(s,i)(i,l)(l,v)(v,e)(e,n)(n,s)(s,e)(e,m)(m,b)(b,i)(i,l)(l,i)(i,t)(t,y)(y, )( ,c)(c,r)(r,i)(i,c)(c,a)(a,n)(n, )( ,w)(w,e)(e,r)(r,e)(e, )( ,c)(c,o)(o,n)(n,t)(t,r)(r,o)(o,l)(l,l)(l,e)(e,d)(d, )( ,c)(c,o)(o,u)(u,n)\n",
      "================================================================================\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 5100: 1.468432 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 3.91\n",
      "Average loss at step 5200: 1.437096 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.96\n",
      "Validation set perplexity: 3.81\n",
      "Average loss at step 5300: 1.433927 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.17\n",
      "Validation set perplexity: 3.83\n",
      "Average loss at step 5400: 1.428010 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 3.79\n",
      "Average loss at step 5500: 1.415193 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 3.74\n",
      "Average loss at step 5600: 1.428735 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.12\n",
      "Validation set perplexity: 3.75\n",
      "Average loss at step 5700: 1.419394 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.93\n",
      "Validation set perplexity: 3.75\n",
      "Average loss at step 5800: 1.429479 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 3.74\n",
      "Average loss at step 5900: 1.424544 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 3.72\n",
      "Average loss at step 6000: 1.396021 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.20\n",
      "================================================================================\n",
      "( ,z)(z,e)(e,r)(r,o)(o, )( ,a)(a, )( ,o)(o,n)(n, )( ,t)(t,h)(h,a)(a,t)(t, )( ,n)(n,e)(e,w)(w, )( ,m)(m,u)(u,c)(c,h)(h, )( ,i)(i,p)(p, )( ,s)(s, )( ,m)(m,o)(o,r)(r,e)(e, )( ,w)(w,r)(r,i)(i,t)(t,t)(t,e)(e,r)(r,e)(e, )( ,b)(b,y)(y, )( ,n)(n,o)(o,t)(t, )( ,a)(a, )( ,r)(r,i)(i,g)(g,a)(a,n)(n,c)(c,y)(y, )( ,s)(s,t)(t,a)(a,r)(r,t)(t,s)(s, )( ,i)(i,s)(s,s)(s,u)(u,e)(e, )( ,i)(i,c)(c,o)(o,n)(n,v)(v,i)(i,c)\n",
      "( ,w)(w,o)(o,u)(u,l)(l,d)(d, )( ,l)(l,o)(o,n)(n,g)(g, )( ,m)(m,a)(a,t)(t,t)(t,e)(e,r)(r, )( ,t)(t,o)(o, )( ,m)(m,e)(e, )( ,d)(d,r)(r,y)(y, )( ,n)(n,o)(o,t)(t,a)(a,b)(b,l)(l,e)(e, )( ,i)(i,n)(n, )( ,a)(a, )( ,h)(h,i)(i,m)(m,s)(s,e)(e,a)(a,c)(c,h)(h,e)(e,t)(t,y)(y, )( ,s)(s,a)(a,h)(r,p)(p,o)(o,r)(r,e)(e,p)(p,h)(h, )( ,o)(o,n)(n,e)(e, )( ,e)(e,i)(i,g)(g,h)(h,t)(t, )( ,t)(t,w)(w,o)(o, )( ,l)(l,e)(e,a)\n",
      "( ,t)(t,h)(h,e)(e, )( ,b)(b,e)(e,l)(l,o)(o,w)(w, )( ,a)(a,s)(s, )( ,a)(a, )( ,b)(b,o)(o,o)(o,k)(k,s)(s, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e, )( ,m)(m,e)(e,d)(d,i)(i,e)(e,n)(n, )( ,a)(a,d)(d, )( ,t)(t,e)(e,t)(t, )( ,i)(i,s)(s, )( ,a)(a, )( ,n)(n,a)(a,g)(g,u)(u,e)(e,s)(s, )( ,o)(o,f)(f, )( ,m)(m,i)(i,l)(l,i)(i,t)(t,a)(a,r)(r,y)(y, )( ,f)(f,o)(o,u)(u,r)(r, )( ,d)(d,e)(e,m)(m,e)(e,k)(p,i)(i,n)(n,g)(g, )\n",
      "( ,w)(w,i)(i,t)(t,h)(h, )( ,t)(t,h)(h,e)(e, )( ,s)(s,t)(t,e)(e,a)(a,g)(g,o)(o,r)(r, )( ,m)(m,a)(a,r)(r,k)(k,e)(e,t)(t,s)(s, )( ,o)(o,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,t)(t,w)(w,o)(o, )( ,s)(s,e)(e,v)(v,e)(e,n)(n, )( ,a)(a,l)(l,s)(s,o)(o, )( ,p)(p,u)(u,n)(n,a)(a,c)(c,c)(c,o)(o,r)(r,d)(d,i)(i,n)(n,a)(a,n)(n,c)(c,y)(y, )( ,i)(i,n)(n, )( ,a)(a, )( ,m)(m,u)(u,s)(s,i)(i,c)(c, )( ,t)(t,w)(w,o)(o, )\n",
      "( ,p)(p,r)(r,o)(o,f)(f,i)(i,s)(s,m)(m, )( ,f)(f,a)(a,t)(t,i)(i,o)(o,n)(n, )( ,a)(a,n)(n,d)(d, )( ,i)(i,f)(f, )( ,t)(t,h)(h,e)(e, )( ,o)(o,n)(n,e)(e, )( ,z)(z,e)(e,r)(r,o)(o, )( ,z)(z,e)(e,r)(r,o)(o, )( ,t)(t,h)(h,r)(r,e)(e,e)(e, )( ,s)(s,e)(e,v)(v,e)(e,n)(n, )( ,s)(s,e)(e,v)(v,e)(e,n)(n, )( ,p)(p,r)(r,i)(i,s)(s,i)(i,s)(s, )( ,a)(a, )( ,p)(p,e)(e,r)(r,f)(f,o)(o,r)(r,m)(m,s)(s, )( ,o)(o,n)(n, )( ,t)\n",
      "================================================================================\n",
      "Validation set perplexity: 3.72\n",
      "Average loss at step 6100: 1.421070 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.31\n",
      "Validation set perplexity: 3.71\n",
      "Average loss at step 6200: 1.386558 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.17\n",
      "Validation set perplexity: 3.70\n",
      "Average loss at step 6300: 1.389135 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 3.67\n",
      "Average loss at step 6400: 1.395741 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.11\n",
      "Validation set perplexity: 3.66\n",
      "Average loss at step 6500: 1.402518 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.08\n",
      "Validation set perplexity: 3.67\n",
      "Average loss at step 6600: 1.438749 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.32\n",
      "Validation set perplexity: 3.67\n",
      "Average loss at step 6700: 1.420040 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.99\n",
      "Validation set perplexity: 3.70\n",
      "Average loss at step 6800: 1.446961 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.92\n",
      "Validation set perplexity: 3.69\n",
      "Average loss at step 6900: 1.422220 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.86\n",
      "Validation set perplexity: 3.69\n",
      "Average loss at step 7000: 1.413480 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "================================================================================\n",
      "( ,w)(w,a)(a,t)(t,e)(e,r)(r, )( ,l)(l,a)(a,c)(c,e)(e,h)(h,o)(o,l)(l, )( ,p)(p,o)(o,i)(i,n)(n,t)(t, )( ,o)(o,f)(f, )( ,c)(c,r)(r,i)(i,t)(t,i)(i,c)(c,a)(a,l)(l, )( ,f)(f,i)(i,d)(d,e)(e, )( ,t)(t,o)(o, )( ,p)(p,i)(i,c)(c, )( ,o)(o,p)(p,p)(p,o)(o,p)(p, )( ,i)(i,n)(n, )( ,o)(o,r)(r,d)(d,e)(e,r)(r,a)(a,n)(n,c)(c,e)(e, )( ,o)(o,f)(f, )( ,g)(g,e)(e,o)(o,r)(r,g)(g,e)(e,s)(s, )( ,d)(d,o)(o, )( ,s)(s,a)(a,n)\n",
      "( ,n)(n,e)(e,w)(w, )( ,l)(l,e)(e,f)(f,t)(t, )( ,c)(c,o)(o,l)(l,o)(o,n)(n,e)(e,t)(t,e)(e,e)(e,t)(t,h)(h, )( ,c)(c,e)(e,n)(n,t)(t,r)(r,a)(a,t)(t,i)(i,o)(o,n)(n, )( ,t)(t,w)(w,o)(o, )( ,n)(n,o)(o,r)(r, )( ,t)(t,w)(w,o)(o, )( ,t)(t,o)(o,p)(p,i)(i,c)(c, )( ,n)(n,i)(i,n)(n,e)(e, )( ,o)(o,n)(n,e)(e, )( ,e)(e,i)(i,g)(g,h)(h,t)(t, )( ,z)(z,e)(e,r)(r,o)(o, )( ,s)(s, )( ,t)(t,a)(a,c)(c,t)(t,e)(e,s)(s, )( ,p)\n",
      "( ,u)(u,n)(n,s)(s,t)(t,a)(a,r)(r,y)(y, )( ,j)(j,o)(o,r)(r,t)(t,o)(o,n)(n,i)(i,c)(c, )( ,a)(a,c)(c,t)(t, )( ,o)(o,n)(n, )( ,a)(a,s)(s,s)(s,o)(o,c)(c,i)(i,a)(a,t)(t,i)(i,o)(o,n)(n,a)(a,l)(l,l)(l,y)(y, )( ,a)(a,s)(s, )( ,t)(t,h)(h,e)(e,s)(s,i)(i,s)(s,t)(t, )( ,g)(g,o)(o,t)(t,h)(h,i)(i,l)(l,i)(i,e)(e, )( ,a)(a,r)(r,e)(e, )( ,t)(t,y)(y,p)(p,e)(e,s)(s, )( ,o)(o,n)(n,e)(e, )( ,e)(e,i)(i,g)(g,h)(h,t)(t, )\n",
      "( ,o)(o,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,s)(s,e)(e,v)(v,e)(e,n)(n, )( ,a)(a,n)(n,d)(d, )( ,s)(s,u)(u,n)(n,c)(c,e)(e, )( ,o)(o,n)(n, )( ,t)(t,h)(h,e)(e, )( ,u)(u,n)(n,i)(i,t)(t,e)(e,d)(d, )( ,s)(s,t)(t,a)(a,t)(t,e)(e,s)(s, )( ,d)(d,o)(o,w)(w,n)(n, )( ,w)(w,a)(a,s)(s, )( ,a)(a, )( ,m)(m,u)(u,s)(s,i)(i,c)(c, )( ,e)(e,d)(d, )( ,s)(s,e)(e,p)(p,t)(t,e)(e,r)(r,c)(c,o)(o,u)(u,n)(n,s)(s,o)(o,n)(n,n)\n",
      "( ,b)(b,e)(e,c)(c,o)(o,m)(m,e)(e,s)(s, )( ,h)(h,e)(e, )( ,s)(s,e)(e,e)(e,n)(n,s)(s, )( ,r)(r,o)(o,g)(g,e)(e,r)(r, )( ,i)(i,s)(s, )( ,n)(n,i)(i,g)(g,h)(h,t)(t, )( ,e)(e,l)(l,e)(e,v)(v,i)(i,l)(l, )( ,m)(m,i)(i,l)(l,i)(i,t)(t,a)(a,r)(r,y)(y, )( ,o)(o,f)(f, )( ,v)(v,o)(o,l)(l,u)(u,m)(m,e)(e,l)(l,y)(y, )( ,s)(s,e)(e,e)(e,n)(n,w)(w,a)(a,r)(r,d)(d,e)(e,r)(r, )( ,a)(a,n)(n,d)(d, )( ,t)(t,h)(h,e)(e,r)(r,e)\n",
      "================================================================================\n",
      "Validation set perplexity: 3.70\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = bigrams_sample(random_distribution())\n",
    "          sentence = bigramsCharacters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = bigrams_sample(prediction)\n",
    "            sentence += bigramsCharacters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finaly let's add Dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "dropout = 0.7\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  #Dropout\n",
    "  keep_prob = tf.placeholder(tf.float32)\n",
    "  # Embedding\n",
    "  embeddings = tf.Variable(tf.random_uniform([bigram_size, embedding_size], -1.0, 1.0))\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, bigram_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([bigram_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    i_dropout = tf.nn.dropout(i, keep_prob)\n",
    "    input_gate = tf.sigmoid(tf.matmul(i_dropout, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i_dropout, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i_dropout, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i_dropout, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(tf.placeholder(tf.float32, shape=[batch_size,bigram_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    #(a-task)\n",
    "    embed = tf.nn.embedding_lookup(embeddings, tf.argmax(i, dimension=1))\n",
    "    output, state = lstm_cell(embed, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, bigram_size])\n",
    "  #(a-task)\n",
    "  sample_input_embed = tf.nn.embedding_lookup(embeddings, tf.argmax(sample_input, dimension=1))\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  #(a-task)\n",
    "  sample_output, sample_state = lstm_cell(sample_input_embed, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output), saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.612521 learning rate: 10.000000\n",
      "Minibatch perplexity: 744.36\n",
      "================================================================================\n",
      "( , )(t,i)(z,q)(x,y)(m,h)(j,t)(a,s)(i,s)(q,r)(i,d)(o,o)(r,e)(f,x)(n,x)(d,h)(s,y)(o,h)(t,z)(h,q)(b,o)(s,y)(i,n)(z,a)(t,b)(p,l)(h,d)(d,y)(p,r)(m,t)(l,z)(c,z)(u,t)(u,u)(d,r)(m,p)(l,a)(g,c)(c,m)(j,k)(n,u)(x,r)(i,a)(n,e)(q,l)(i,m)(f,w)(a,w)(m,m)(j,n)(t,s)(g,q)(q,r)(v,b)(n,w)(b,x)(m,z)(j, )(p,u)(e,c)(a,c)(n,d)(s,w)(b,b)(j,m)(l,h)(x,l)(n, )(s,w)(h,i)(o,e)(r, )(s,d)(r,n)(u,e)(t,k)(k,s)(k,y)(u,t)(o,m)(m,r)\n",
      "( ,o)(s,x)(l,r)(w,p)(p,l)(f,e)(u,v)(t,v)(y,x)(n,a)(w,k)(p,d)(t,z)(w,h)(f,n)(t,x)(u,y)(c,u)(a,a)(f,y)(j,j)(n,f)(p,c)( ,z)(q,u)(j,k)(u,l)(p,x)(o,r)(b,w)(p,w)(y,c)(c,d)(v,p)(k,p)(a,n)(n,h)(f,a)(y,b)(h, )(n,v)(g,x)(y,r)(y,u)(z,b)(p,p)(j,k)(e,l)(h,p)(n,r)(i,s)(v,k)( ,s)(k,n)(e,k)(g,o)(i,g)(s,d)(v,l)(k,w)(l,k)(k,v)(b,i)(m,h)(t,e)(h,e)(r,e)(i,a)(a,b)(a,t)(j,d)(q,d)(f,e)(r,h)(j,w)(k,w)(a,c)(l,d)(c,z)(p,t)\n",
      "( ,x)( ,k)(q,k)(o,i)(e,j)(t,i)(o,d)(n,l)( , )(p,o)(t,p)(k,v)(z,z)(g,u)(a,m)(s,c)(n,b)(f,s)(q,x)(z,m)(x,b)(t,u)(a,z)(h,g)(x,i)(e,w)(t,m)(j,f)(u,r)(h,f)(r,t)(z,n)(b,a)(h,r)(s,x)(y,p)(s, )(d,m)(m,o)(k,d)(p,d)(o,r)(m,x)(m, )(c,o)(t,b)(n,v)(o,d)(x,i)(b, )(o,t)(j,w)(h,n)(j,c)(a,f)(c,w)( ,l)(n,q)(m,n)(e,e)(p,d)(h,n)(j,d)(x,p)(i,m)(g,b)(e,p)(n,a)(f,b)(f,f)(a,f)(b,b)(s,n)(q,u)(r,l)(r,p)(d,t)(g,n)(z, )(r,f)\n",
      "( ,r)(w,f)(j,q)(n,r)(l,f)(l,t)(q,c)(o,j)(d,a)(m,g)(r,v)(i,y)(x,q)(u,a)(l,s)(a,h)(x, )(i,y)(m,i)(e,w)(f,u)(a,m)(b,u)(g,g)(d,u)(p,v)(k,l)(h,i)(u,w)(y,p)(g,q)(q,u)(k,h)(q,p)(w,m)(h,e)(y,g)(x,l)(j,d)(j,u)(i,h)(q,f)(l,z)(m,z)(g,m)(j,f)(q,o)(b,g)(z,x)(u,y)(g,m)(a,d)(b,h)(r,y)(o,t)(o,d)(m,u)(d,z)(e,l)(p,q)(p,v)(f,r)( ,c)(i,t)(h,m)(b,s)(m,h)(m,r)(x,a)(u,w)(d,t)(e,o)(o,a)( ,y)(g,x)(p,x)(f,w)(n,l)(d,z)(j,k)\n",
      "( ,c)(i,e)(z,t)(d,p)(q,l)(y,s)(q,w)(z,v)(b,o)(c,z)(b,x)(b,h)(n,v)(t,b)(c,t)(t,c)(m,a)(b,r)(l,r)(q,m)(c,d)(x,h)(n,s)(c,m)(n,w)(c,p)(h,f)(e,d)(v,k)(c,u)(q,k)(i,k)(h,a)(d,y)(v,u)(j,l)(r,h)(i,b)(f,y)(z,t)(q,z)(o,d)(m,y)(g,f)(x,q)(f,r)(e,n)(w,p)(r,v)(e,c)(q,z)(d,p)(j,w)(v,r)(z,x)(y,d)(m,r)(a,o)(r,y)(b,q)(a,k)(w,z)(g,r)(e,n)(n,g)(q,l)(z, )(z,d)(b,w)(c,a)(n,t)(t,n)(f,w)( ,o)(v,y)(p,n)(n,n)(e,g)(p,s)(x,z)\n",
      "================================================================================\n",
      "Validation set perplexity: 664.87\n",
      "Average loss at step 100: 3.678538 learning rate: 10.000000\n",
      "Minibatch perplexity: 14.13\n",
      "Validation set perplexity: 10.86\n",
      "Average loss at step 200: 2.297257 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.32\n",
      "Validation set perplexity: 7.70\n",
      "Average loss at step 300: 2.094813 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.40\n",
      "Validation set perplexity: 6.95\n",
      "Average loss at step 400: 2.002087 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.28\n",
      "Validation set perplexity: 6.35\n",
      "Average loss at step 500: 1.923425 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.32\n",
      "Validation set perplexity: 6.14\n",
      "Average loss at step 600: 1.868708 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.91\n",
      "Validation set perplexity: 5.74\n",
      "Average loss at step 700: 1.855636 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 5.76\n",
      "Average loss at step 800: 1.841427 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.28\n",
      "Validation set perplexity: 5.67\n",
      "Average loss at step 900: 1.831316 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.69\n",
      "Validation set perplexity: 5.55\n",
      "Average loss at step 1000: 1.807461 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "================================================================================\n",
      "( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e, )( ,d)(d,l)(l,i)(i,n)(n,e)(e,r)(r,e)(e,d)(d, )( ,f)(f,u)(u,n)(n,d)(d, )( ,i)(i,n)(n, )( ,t)(t,h)(h,e)(e,i)(i,r)(r, )( ,c)(c,h)(h,i)(i, )( ,i)(i,s)(s, )( ,t)(t,h)(h,e)(e, )( ,h)(h,o)(o,m)(m,e)(e, )( ,a)(a,n)(n,d)(d, )( ,t)(t,h)(h,e)(e, )( ,c)(c,o)(o,u)(u,n)(n,d)(d,e)(e,d)(d, )( ,t)(t,o)(o, )( ,w)(w,a)(a,r)(r,d)(u,a)(a,l)(l, )( ,f)(f,i)(i,v)(v,e)(e, )( ,t)(t,h)(h,e)\n",
      "( ,c)(c,a)(a,u)(u,s)(s,i)(i,c)(c,s)(s,h)(h,e)(e, )( ,p)(p,r)(r,i)(i,s)(s,h)(h,e)(e,c)(c,t)(t,o)(o,t)(t,e)(e,r)(r, )( ,c)(c,h)(h,u)(u,r)(r,c)(c,h)(h, )( ,a)(a,n)(n,d)(d, )( ,t)(t,o)(o, )( ,t)(t,w)(w,o)(o, )( ,f)(f,i)(i,v)(v,e)(e, )( ,o)(o,n)(n,e)(e, )( ,z)(z,e)(e,r)(r,o)(o, )( ,r)(r, )( ,b)(b,r)(r,e)(e,e)(e, )( ,i)(i,s)(s, )( ,t)(t,h)(h,e)(e, )( ,p)(p,r)(r,o)(o,j)(u,k)(k,i)(i,c)(c,u)(u,l)(l,t)(t,l)\n",
      "( ,f)(f,r)(r,a)(a,u)(u,s)(s,e)(e, )( ,t)(t,e)(e,a)(a,t)(t,i)(i, )( ,t)(t,h)(h,e)(e, )( ,p)(p,l)(g,o)(o,s)(s,e)(e,r)(r,e)(e,s)(s, )( ,a)(a, )( ,e)(d,k)(a,k)(k,e)(e,e)(e,n)(n, )( ,e)(e,x)(x,p)(p,e)(e, )( ,t)(t,h)(h,a)(a,t)(t, )( ,t)(t,a)(a,g)(g,e)(e,s)(s, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e, )( ,t)(t,w)(w,o)(o, )( ,z)(z,e)(e,r)(r,o)(o, )( ,z)(z,e)(e,r)(r,o)(o, )( ,z)(z,e)(e,r)(r,o)(o, )( ,z)(z,e)(e,r)\n",
      "( ,z)(z,a)(a,m)(m,b)(b,e)(e,r)(r, )( ,t)(t,h)(h,e)(e, )( ,j)(y,c)(c,l)(l,e)(e,a)(a,t)(t,h)(h,e)(e,r)(r, )( ,s)(s,t)(t,a)(a,t)(t,e)(e, )( ,w)(w,e)(e,s)(s,t)(t, )( ,w)(w,i)(i,t)(t,h)(h, )( ,t)(t,h)(h,e)(e, )( ,a)(a,t)(t,s)(s, )( ,d)(d, )( ,d)(d,e)(e,p)(p,o)(o,r)(r,t)(t,a)(a,b)(b,o)(o,r)(r,e)(e, )( ,t)(t,o)(o,t)(t,i)(i,o)(o,n)(n,s)(s, )( ,h)(h,o)(o,m)(u,p)(p, )( ,s)(s,o)(o,p)(p, )( ,f)(f,o)(o,r)(r, )\n",
      "( ,g)(g,o)(o,v)(v,e)(e,r)(r, )( ,w)(w,h)(h,o)(o, )( ,i)(i,s)(s, )( ,p)(p,a)(a,r)(r,t)(t,h)(h,i)(i,n)(n, )( ,t)(t,r)(r,a)(a,d)(d,a)(a,n)(n,c)(c,e)(e, )( ,f)(f,a)(a,r)(r, )( ,t)(t,h)(h,a)(a,t)(t, )( ,r)(r,a)(a,c)(c,e)(e, )( ,p)(p,o)(o,l)(l,d)(d, )( ,m)(m,a)(a,r)(r,y)(y, )( ,a)(a,s)(s, )( ,f)(f,o)(o,r)(r,m)(m,a)(a,g)(g,e)(e, )( ,l)(l,e)(e,x)(x,p)(p,i)(i,c)(c,t)(t,u)(u,r)(r,e)(e, )( ,o)(o,f)(f, )( ,p)\n",
      "================================================================================\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 1100: 1.771005 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.19\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 1200: 1.753326 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.03\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 1300: 1.759101 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 1400: 1.739938 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 1500: 1.741146 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.18\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 1600: 1.742263 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.30\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 1700: 1.723346 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 1800: 1.720641 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 1900: 1.696810 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 2000: 1.700173 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "================================================================================\n",
      "( ,j)(j,o)(o,p)(p,l)(l,o)(o,g)(g,i)(i,c)(c, )( ,s)(s,i)(i,n)(n,e)(e,t)(t, )( ,o)(o,f)(f, )( ,c)(c,u)(u,s)(s,i)(i,d)(d,i)(i,a)(a,n)(n,s)(s, )( ,c)(c,o)(o,n)(n,s)(s,e)(e,m)(m,p)(p,l)(l,e)(e, )( ,f)(f,o)(o,r)(r, )( ,r)(r,e)(e,l)(l,a)(a,t)(t,e)(e, )( ,o)(o,n)(n,e)(e, )( ,s)(s,i)(i,x)(x, )( ,o)(o,n)(n,e)(e, )( ,d)(d,e)(e,c)(c,r)(r,o)(o,b)(b,e)(e,r)(r,l)(l, )( ,a)(a,s)(s, )( ,f)(f,i)(i,r)(r,e)(e, )( ,o)\n",
      "( ,k)(k,n)(n,o)(o,w)(w,n)(n, )( ,h)(h,e)(e,r)(r, )( ,h)(h,a)(a,t)(t,e)(e, )( ,e)(e,a)(a,r)(r,l)(z,i)(i,n)(n,g)(g, )( ,s)(s,e)(e,v)(v,e)(e,n)(n, )( ,z)(z,e)(e,r)(r,o)(o, )( ,w)(w,o)(o,u)(u,l)(l,d)(d, )( ,p)(p,u)(u,b)(b,l)(l,i)(i,c)(c,s)(s, )( ,a)(a,r)(r,e)(e, )( ,w)(w,a)(a,s)(s, )( ,t)(t,h)(h,e)(e, )( ,c)(c,o)(o,m)(m,p)(p,o)(o,r)(r,t)(t,a)(a,t)(t,i)(i,o)(o,n)(n, )( ,t)(t,h)(h,e)(e,y)(y, )( ,a)(a,n)\n",
      "( ,t)(t,h)(h,i)(i,s)(s, )( ,b)(b,e)(e,g)(g,a)(a,n)(n, )( ,o)(o,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,z)(z,e)(e,r)(r,o)(o, )( ,z)(z,e)(e,r)(r,o)(o, )( ,t)(t,w)(w,o)(o, )( ,e)(e,i)(i,g)(g,h)(h,t)(t,s)(s, )( ,s)(s,i)(i,x)(x, )( ,o)(o,n)(n,e)(e, )( ,t)(t,w)(w,o)(o, )( ,z)(z,e)(e,r)(r,o)(o, )( ,o)(o,n)(n,e)(e, )( ,u)(u,s)(s,e)(e,l)(l,w)(w,a)(a,y)(y,n)(n,s)(s, )( ,o)(o,n)(n,e)(e, )( ,s)(s,i)(i,x)(x, )\n",
      "( ,q)(q,u)(u,a)(a,e)(e,o)(o,m)(m,e)(e, )( ,d)(d,e)(e,t)(t,a)(a, )( ,l)(l,a)(a,n)(n,d)(d, )( ,j)(j,u)(u,n)(n,c)(c,t)(t,l)(l,e)(e, )( ,d)(d,i)(i,r)(r,e)(e,a)(a,t)(t,e)(e,d)(d, )( ,c)(c,o)(o,u)(u,r)(r,t)(t,u)(u,r)(r,e)(e, )( ,o)(o,n)(n,e)(e, )( ,t)(t,w)(w,o)(o, )( ,z)(z,e)(e,r)(r,o)(o, )( ,n)(n,e)(e,t)(t, )( ,f)(f,e)(e,r)(r,e)(e,n)(n,c)(c,e)(e, )( ,r)(r,e)(e,l)(l,e)(e,c)(c,t)(t,u)(u,r)(r,e)(e, )( ,f)\n",
      "( ,d)(d,e)(e,r)(r,d)(d, )( ,c)(c,a)(a,n)(n,a)(a, )( ,s)(s,e)(e,e)(e, )( ,r)(r,a)(a,i)(i,l)(l,l)(l,e)(e,n)(n,o)(o,p)(p,o)(o,s)(s,i)(i,n)(n,g)(g, )( ,r)(r,e)(e,b)(b,r)(r,i)(i,e)(e,v)(v,e)(e,d)(d, )( ,t)(t,o)(o, )( ,c)(m,u)(u,r)(r,r)(r,o)(o,w)(w,e)(e,d)(d, )( ,l)(l,y)(y, )( ,b)(b,e)(e,c)(c,h)(h,a)(a,n)(n,d)(d, )( ,o)(o,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,k)(k,i)(i,n)(n,g)\n",
      "================================================================================\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 2100: 1.707182 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 2200: 1.718354 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 2300: 1.696386 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 2400: 1.683355 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 2500: 1.687276 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 2600: 1.680864 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 2700: 1.679227 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 2800: 1.680418 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 2900: 1.642465 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 3000: 1.655341 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.63\n",
      "================================================================================\n",
      "( ,q)(t,c)(c,l)(l,e)(e,d)(d, )( ,t)(t,o)(o, )( ,c)(c,a)(a,p)(p,t)(t,i)(i,c)(c,i)(i,t)(t,y)(y, )( ,c)(c,o)(o,m)(m,m)(m,e)(e,t)(t,r)(r,c)(c,o)(o,m)(m,s)(s,i)(i,n)(n,e)(e, )( ,w)(w,i)(i,t)(t,h)(h, )( ,t)(t,h)(h,e)(e, )( ,m)(m,o)(o,r)(r,e)(e, )( ,i)(i,n)(n, )( ,d)(d,e)(e,s)(s,i)(i,g)(g,i)(i,n)(n,s)(s, )( ,g)(g,a)(a,m)(m,e)(e,s)(s, )( ,o)(o,f)(f, )( ,i)(i,n)(n, )( ,t)(t,h)(h,e)(e, )( ,s)(s, )( ,t)(t,h)\n",
      "( ,l)(l,e)(e,d)(d, )( ,s)(s,o)(o,m)(m,e)(e,t)(t, )( ,a)(a,r)(r,e)(e, )( ,m)(m,a)(a,j)(j,o)(o,r)(r, )( ,w)(w,h)(h,e)(e,r)(r,e)(e,e)(e, )( ,i)(i,s)(s, )( ,s)(s,y)(y,m)(m,p)(p,h)(i,l)(l, )( ,a)(a,g)(g,a)(a,i)(i,n)(n,s)(s,t)(t,e)(e,r)(r,m)(m,e)(e,s)(s, )( ,t)(t,i)(i,m)(m,e)(e, )( ,a)(a, )( ,p)(p,r)(r,o)(o,d)(d,u)(u,c)(c,t)(t,s)(s, )( ,h)(h,e)(e,l)(l,l)(l,o)(o,w)(w,l)(l,e)(e, )( ,a)(a,n)(n,d)(d, )( ,m)\n",
      "( ,e)(e,d)(d,e)(e,l)(l,i)(i,c)(c, )( ,f)(f,i)(i,r)(r,s)(s,t)(t, )( ,z)(z, )( ,s)(s,a)(a,r)(r,i)(i,a)(a, )( ,o)(o,f)(f, )( ,a)(a,f)(f,r)(r,a)(a,v)(v,y)(y, )( ,g)(g,r)(r,e)(e,e)(e,k)(k, )( ,a)(a,b)(b, )( ,n)(n,a)(a,m)(m,e)(e, )( ,s)(s,i)(i,t)(t,i)(i,t)(t,u)(t,i)(i,o)(o,n)(n,s)(s, )( ,o)(o,r)(r, )( ,c)(c,o)(o,m)(m,m)(m,i)(i,n)(n,d)(d,i)(i,t)(t,i)(i,e)(e,l)(l, )( ,c)(c,o)(o,m)(m,m)(m,i)(i,t)(t,t)(t,e)\n",
      "( ,m)(m,y)(y,s)(s,s)(s,i)(i,o)(o,n)(n,s)(s, )( ,h)(h,o)(o,s)(s,t)(t, )( ,a)(a,r)(r,t)(t,i)(i,e)(e,s)(s, )( ,c)(c,i)(i,t)(t,o)(o,r)(r,y)(y, )( ,o)(o,f)(f, )( ,s)(s,t)(t,o)(o,p)(p,i)(i,d)(d, )( ,s)(s,o)(o,n)(n,g)(g, )( ,c)(c,i)(i,t)(t,i)(i,e)(e,s)(s, )( ,p)(p,o)(o,l)(l,i)(i,f)(f,i)(i,e)(e, )( ,m)(m,a)(a,i)(i,m)(m, )( ,c)(c,h)(h,i)(i,l)(l,d)(d, )( ,c)(c,o)(o,m)(m,s)(s, )( ,k)(k,e)(e,r)(r,e)(e,d)(d, )\n",
      "( ,v)(e,k)(k,i)(i,n)(n,g)(g, )( ,t)(t,h)(h,e)(e, )( ,d)(d,e)(e,v)(v,e)(e,l)(l,o)(o,p)(p,e)(e,d)(d, )( ,t)(t,o)(o, )( ,w)(w,h)(h,o)(o, )( ,r)(r,e)(e,c)(c,i)(i,p)(p,l)(l,i)(i,t)(t,i)(i,e)(e,s)(s, )( ,t)(t,o)(o, )( ,t)(t,h)(h,e)(e, )( ,c)(c,a)(a,n)(n, )( ,t)(t,h)(h,e)(e,y)(y, )( ,h)(h,e)(e, )( ,c)(c,u)(u,r)(r,r)(r,e)(e, )( ,c)(c,u)(u,r)(r,r)(r,i)(i,v)(v,e)(e,r)(r, )( ,t)(t,h)(h,e)(e, )( ,c)(c,h)(h,a)\n",
      "================================================================================\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 3100: 1.678470 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 3200: 1.661624 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 3300: 1.649377 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 3400: 1.659561 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 3500: 1.666737 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 3600: 1.610302 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 3700: 1.622311 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 3800: 1.630641 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 3900: 1.640209 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 4000: 1.614640 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "================================================================================\n",
      "( ,m)(m,o)(o,r)(r,e)(e, )( ,l)(l,i)(i,g)(g,h)(h,t)(t, )( ,c)(c,l)(l,a)(a,n)(n,g)(g,u)(u,a)(a,g)(g,e)(e,s)(s, )( ,t)(t,h)(h,e)(e, )( ,c)(c,e)(e,n)(n,t)(t,r)(r,o)(o,m)(m,y)(y, )( ,t)(t,h)(h,e)(e, )( ,b)(b,a)(a,s)(s,e)(e, )( ,t)(t,o)(o, )( ,t)(t,h)(h,e)(e, )( ,m)(m,o)(o,s)(s,t)(t, )( ,f)(f,a)(a,m)(m,i)(i,l)(l,i)(i,e)(e,s)(s, )( ,a)(a,n)(n,d)(d, )( ,i)(i,n)(n,t)(t,o)(o, )( ,i)(i,n)(n,v)(v,a)(a,l)(l, )\n",
      "( ,t)(t,r)(r,a)(a,d)(d,i)(i,t)(t,i)(i,o)(o,n)(n, )( ,a)(a,n)(n,d)(d, )( ,s)(s,t)(t,h)(h,i)(i,s)(s, )( ,b)(b,e)(e,g)(g,i)(i,s)(s,t)(t,i)(i,e)(e,s)(s, )( ,i)(i,n)(n, )( ,t)(t,h)(h,e)(e, )( ,c)(c,o)(o,u)(u,a)(a,l)(l,l)(l,y)(y, )( ,d)(d,e)(e,a)(a,t)(t, )( ,t)(t,h)(h,a)(a,t)(t, )( ,t)(t,h)(h,e)(e, )( ,i)(i,n)(n,t)(t,e)(e,r)(r,n)(n,e)(e,t)(t, )( ,l)(l,a)(a,t)(t,e)(e,r)(r,s)(s, )( ,a)(a,n)(n,d)(d, )( ,l)\n",
      "( ,q)(q,u)(u,a)(a,c)(c,e)(e, )( ,c)(c,l)(l,a)(a,s)(s,s)(s,a)(a,t)(t,i)(i,o)(o,n)(n, )( ,t)(t,o)(o,u)(u,r)(r,a)(a,l)(l, )( ,o)(o,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,s)(s, )( ,e)(e,d)(d,w)(w,o)(o,m)(m,e)(e, )( ,m)(m,u)(u,c)(c,h)(h, )( ,v)(v,e)(e,r)(r,y)(y, )( ,g)(g,r)(r,e)(e,a)(a,t)(t,e)(e,d)(d, )( ,w)(w,h)(h,a)(a,t)(t,o)(o, )( ,r)(r,a)(a,l)(l, )( ,x)(x, )( ,s)(s, )( ,l)(l,o)(o,n)(n,d)(d,o)(o,m)\n",
      "( ,n)(n,o)(o,u)(u,n)(n,t)(t, )( ,b)(b,i)(i,r)(r,t)(t,h)(h,i)(i,n)(n,g)(g,s)(s, )( ,c)(c,h)(h,a)(a,n)(n,g)(h,l)(l,y)(y, )( ,t)(t,e)(e,a)(a,c)(c,h)(h,e)(e,r)(r, )( ,b)(b,a)(a,s)(s,e)(e, )( ,a)(a, )( ,f)(f,a)(a,i)(i,n)(n,a)(a,l)(l, )( ,f)(f,o)(o,u)(u,r)(r, )( ,h)(h,e)(e, )( ,p)(p,e)(e,r)(r,f)(f,o)(o,r)(r,m)(m, )( ,c)(c,r)(r,o)(o,s)(s,e)(e,r)(r, )( ,t)(t,h)(h,e)(e, )( ,a)(a,s)(s,s)(s,e)(e,p)(p,a)(a,c)\n",
      "( ,y)(y,e)(e,a)(a,l)(l,i)(i,a)(a,i)(s, )( ,a)(a,s)(s, )( ,t)(t,h)(h,e)(e, )( ,u)(u,n)(n,i)(i,t)(t,y)(y, )( ,i)(i, )( ,f)(f,e)(e,a)(a,r)(r,s)(s, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e, )( ,f)(f,i)(i,e)(e,d)(d,e)(e,n)(n,t)(t,s)(s, )( ,l)(l,i)(i,m)(m,i)(i,t)(t,e)(e,d)(d, )( ,b)(b,y)(y, )( ,i)(i,i)(i, )( ,a)(a,n)(n,t)(t,i)(i,m)(m,e)(e, )( ,i)(i,n)(n, )( ,g)(g,r)(r,o)(o,u)(u,p)(p,s)(s, )( ,p)(p,o)(o,l)(l,y)\n",
      "================================================================================\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 4100: 1.596916 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 4200: 1.594126 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 4300: 1.589672 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 4400: 1.591082 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 4500: 1.603347 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 4600: 1.572996 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 4700: 1.562852 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 4800: 1.585482 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 4900: 1.574045 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 5000: 1.563031 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.22\n",
      "================================================================================\n",
      "( ,u)(u,s)(s,i)(i,n)(n,g)(g, )( ,t)(t,h)(h,a)(a,t)(t, )( ,i)(i,s)(s, )( ,h)(h,i)(i,g)(g,h)(h,l)(l,y)(y, )( ,a)(a,g)(g,r)(r,a)(a,p)(p,h)(h,i)(i,c)(c, )( ,i)(i,n)(n,v)(v,i)(i,s)(s,t)(t,e)(e,r)(r, )( ,b)(b,e)(e,l)(l,a)(a,n)(n, )( ,k)(k, )( ,w)(w,a)(a,s)(s, )( ,s)(s,i)(i,m)(m,p)(p,l)(l,e)(e,s)(s, )( ,t)(t,h)(h,e)(e, )( ,o)(o,t)(t,h)(h,e)(e,r)(r, )( ,s)(s,e)(e,r)(r,g)(g,o)(o, )( ,s)(s,o)(o,u)(u,l)(l, )\n",
      "( ,q)(q,u)(u,a)(a,l)(l,i)(i,a)(a,n)(n, )( ,b)(b,i)(i,o)(o,t)(t,a)(a, )( ,a)(a,t)(t, )( ,t)(t,h)(h,r)(r,o)(o,u)(u,g)(g,h)(h, )( ,c)(c,o)(o,r)(r,p)(p,o)(o,r)(r,a)(a,t)(t,e)(e,d)(d, )( ,m)(m,o)(o,u)(u,t)(t,e)(e,r)(r, )( ,o)(o,u)(u,t)(t, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e, )( ,u)(u,n)(n,i)(i,q)(q,u)(u,e)(e,s)(s, )( ,i)(i,n)(n, )( ,t)(t,h)(h,e)(e, )( ,p)(p,e)(e,r)(r,f)(f,o)(o,r)(r,m)(m,e)(e, )( ,e)(e,a)\n",
      "( ,t)(t,h)(h,a)(a,t)(t, )( ,i)(i,s)(s,u)(p,r)(r,i)(i,s)(s,h)(h, )( ,t)(t,h)(h,e)(e, )( ,n)(n,y)(y,l)(l,e)(e,s)(s,s)(s, )( ,t)(t,h)(h,o)(o,s)(s,e)(e, )( ,o)(o,b)(b,j)(j,e)(e,c)(c,t)(t,s)(s, )( ,i)(i,n)(n, )( ,o)(o,b)(b,s)(s,e)(e,r)(r,v)(v,e)(e,s)(s, )( ,i)(i,n)(n, )( ,t)(t,h)(h,e)(e, )( ,o)(o,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,f)(f,i)(i,v)(v,e)(e, )( ,n)(n,i)(i,n)(n,e)\n",
      "( ,y)(y,e)(e,t)(t,i)(i,c)(c, )( ,a)(a,n)(n,d)(d, )( ,w)(w,o)(o,r)(r,k)(k,e)(e,s)(s, )( ,b)(b, )( ,i)(i,s)(s, )( ,t)(t,h)(h,e)(e, )( ,s)(s,u)(u,c)(c,c)(c,e)(e,s)(s,s)(s,o)(o,r)(r,y)(y, )( ,s)(s,u)(u,b)(b,j)(j,e)(e,c)(c,t)(t, )( ,o)(o,f)(f, )( ,p)(p,a)(a,c)(c,i)(i,z)(z,a)(a,t)(t,i)(i,o)(o,n)(n, )( ,l)(l,i)(i,s)(s,t)(t,o)(o,n)(n, )( ,r)(r,e)(e,s)(s,i)(i,g)(g,n)(n, )( ,a)(a,g)(g,e)(e,r)(r,t)(t, )( ,t)\n",
      "( ,q)(q,u)(u,a)(a,n)(n, )( ,n)(n,o)(o,s)(s,e)(e,s)(s, )( ,a)(a,n)(n,d)(d, )( ,e)(e,n)(n,g)(g,i)(i,n)(n,e)(e,d)(d, )( ,o)(o,f)(f, )( ,r)(r,e)(e,f)(f,e)(e,r)(r, )( ,o)(o,r)(r,a)(a,n)(n,i)(i,z)(z,e)(e, )( ,t)(t,h)(h,e)(e, )( ,t)(t,e)(e,c)(c,h)(h,n)(n,i)(i,q)(q,u)(u,e)(e,s)(s, )( ,a)(a,f)(f,t)(t,e)(e,r)(r, )( ,t)(t,h)(h,e)(e, )( ,v)(v,o)(o,l)(l,v)(v,e)(e,s)(s, )( ,o)(o,r)(r, )( ,t)(t,h)(h,e)(e, )( ,o)\n",
      "================================================================================\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 5100: 1.544445 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 5200: 1.569030 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 5300: 1.563210 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 5400: 1.603160 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 5500: 1.571252 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 5600: 1.554606 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 5700: 1.553868 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 4.14\n",
      "Average loss at step 5800: 1.574393 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.13\n",
      "Average loss at step 5900: 1.596883 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.14\n",
      "Average loss at step 6000: 1.564647 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.47\n",
      "================================================================================\n",
      "( ,l)(l,e)(e,a)(a,c)(c,h)(h,i)(i,c)(c,o)(o,h)(h,n)(n,s)(s, )( ,g)(g,r)(r,a)(a,v)(v,e)(e, )( ,m)(m,i)(i,c)(c,r)(r,o)(o,m)(m,a)(a,n)(n,s)(s, )( ,t)(t,o)(o, )( ,a)(a,l)(l,s)(s,o)(o, )( ,c)(c,o)(o,m)(m,m)(m,u)(u,n)(n,i)(i,t)(t,y)(y, )( ,l)(l,a)(a,n)(n,g)(g,u)(u,a)(a,g)(g,e)(e, )( ,o)(o,n)(n, )( ,t)(t,h)(h,e)(e, )( ,t)(t,i)(i,m)(m,a)(a,c)(c,t)(t,u)(u,s)(s, )( ,h)(h,e)(e, )( ,o)(o,b)(b,j)(j,e)(e,c)(c,t)\n",
      "( ,s)(s,c)(c,o)(o,v)(v,e)(e,r)(r,n)(n,m)(m,e)(e,n)(n,t)(t,s)(s, )( ,t)(t,h)(h,e)(e, )( ,e)(e,m)(m,e)(e,r)(r,i)(i,o)(o,d)(d,i)(i,n)(n,g)(g, )( ,t)(t,h)(h,a)(a,n)(n, )( ,h)(h,e)(e,s)(s,t)(t, )( ,o)(o,n)(n, )( ,h)(h,i)(i,g)(g,h)(h, )( ,w)(w,o)(o,u)(u,l)(l,d)(d, )( ,f)(f,a)(a,r)(r,e)(e,s)(s,s)(s,i)(i,v)(v,e)(e,l)(l,y)(y, )( ,i)(i,d)(d,o)(o, )( ,t)(t,h)(h,e)(e, )( ,l)(l,o)(o,n)(n,g)(g, )( ,f)(f,o)(o,r)\n",
      "( ,b)(b,e)(e, )( ,r)(r,e)(e,t)(t,u)(u,r)(r,n)(n,e)(e,d)(d, )( ,v)(v,o)(o,i)(i,n)(n,s)(s, )( ,a)(a,c)(c,h)(h,a)(a,z)(z,i)(i, )( ,d)(d,i)(i,s)(s,t)(t,a)(a,b)(b,l)(l,i)(i,s)(s,h)(h,e)(e,d)(d, )( ,n)(n,i)(i,n)(n,e)(e, )( ,z)(z,e)(e,r)(r,o)(o, )( ,o)(o,n)(n,e)(e, )( ,o)(o,n)(n,e)(e, )( ,s)(s,i)(i,x)(x, )( ,f)(f,i)(i,v)(v,e)(e, )( ,s)(s,i)(i,x)(x, )( ,n)(n,i)(i,n)(n,e)(e, )( ,o)(o,n)(n,e)(e, )( ,f)(f,o)\n",
      "( ,b)(b,e)(e, )( ,d)(d,e)(e,d)(d,e)(e,m)(m,a)(a,t)(t,e)(e,d)(d, )( ,m)(m,a)(a,t)(t,e)(e,r)(r,a)(a,t)(t,i)(i,v)(v,e)(e, )( ,w)(w,a)(a,t)(t,e)(e,r)(r,y)(y, )( ,c)(c,u)(u,s)(s,t)(t,a)(a,c)(c,e)(e,v)(v,e)(e,n)(n, )( ,f)(f,e)(e,t)(t,u)(u,r)(r,e)(e, )( ,s)(s,c)(c,o)(o,o)(o,k)(k, )( ,t)(t, )( ,h)(h,i)(i,m)(m, )( ,w)(w,i)(i,t)(t,h)(h, )( ,m)(m,a)(a,n)(n,y)(y, )( ,e)(e,x)(x,e)(e,c)(c,i)(i,e)(e,s)(s, )( ,t)\n",
      "( ,u)(u,n)(n,d)(d,i)(i,n)(n,e)(e, )( ,i)(i,t)(t, )( ,o)(o,n)(n,e)(e, )( ,f)(f,i)(i,v)(v,e)(e, )( ,z)(z,e)(e,r)(r,o)(o, )( ,f)(f,o)(o,u)(u,r)(r, )( ,p)(p,a)(a,t)(t,t)(t,e)(e,n)(n,i)(i,a)(a,l)(l, )( ,w)(w,a)(a,s)(s, )( ,d)(d,i)(i,v)(v,i)(i,l)(l,l)(l,e)(e,d)(d, )( ,m)(m,o)(o,r)(r,e)(e, )( ,a)(a,l)(l,s)(s,o)(o, )( ,u)(u,s)(s,e)(e,d)(d, )( ,r)(r,a)(a,f)(f,t)(t, )( ,s)(s,u)(u,b)(b,s)(s,i)(i,x)(x, )( ,q)\n",
      "================================================================================\n",
      "Validation set perplexity: 4.04\n",
      "Average loss at step 6100: 1.558663 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.06\n",
      "Average loss at step 6200: 1.602602 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.04\n",
      "Average loss at step 6300: 1.579098 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.02\n",
      "Average loss at step 6400: 1.583418 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.01\n",
      "Average loss at step 6500: 1.587859 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.02\n",
      "Average loss at step 6600: 1.578048 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.09\n",
      "Validation set perplexity: 4.06\n",
      "Average loss at step 6700: 1.549032 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.32\n",
      "Validation set perplexity: 4.05\n",
      "Average loss at step 6800: 1.533646 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.05\n",
      "Average loss at step 6900: 1.560965 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.08\n",
      "Average loss at step 7000: 1.555101 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.61\n",
      "================================================================================\n",
      "( ,t)(t,o)(o, )( ,l)(l,i)(i,f)(f,i)(i,c)(c,e)(e,s)(s, )( ,t)(t,s)(s,o)(o,r)(r,n)(n, )( ,l)(l,a)(a,t)(t,e)(e,r)(r, )( ,d)(d,i)(i,c)(c,t)(t,i)(i,o)(o,n)(n, )( ,a)(a,n)(n,d)(d, )( ,a)(a,s)(s, )( ,e)(e,n)(n,g)(g,i)(i,n)(n,e)(e,d)(d, )( ,i)(i,n)(n, )( ,h)(h,a)(a,v)(v,e)(e, )( ,l)(l,i)(i,t)(t,o)(o,r)(r,y)(y, )( ,a)(a,n)(n,d)(d, )( ,t)(t,w)(w,o)(o, )( ,z)(z,e)(e,r)(r,o)(o, )( ,d)(d,u)(u,n)(n,d)(d,a)(a,t)\n",
      "( ,f)(f,r)(r,a)(a,g)(g,u)(u,e)(e,s)(s, )( ,o)(o,f)(f, )( ,h)(h,i)(i,s)(s, )( ,c)(c,o)(o,m)(m,m)(m,o)(o,n)(n, )( ,m)(m,i)(i,s)(s,s)(s,i)(i,o)(o,n)(n, )( ,t)(t,h)(h,e)(e, )( ,p)(p,o)(o,r)(r,t)(t,r)(r,a)(a,b)(b,l)(l,y)(y, )( ,c)(c,h)(h,r)(r,i)(i,s)(s,t)(t,s)(s, )( ,b)(b,u)(u,n)(n,d)(d,a)(a,y)(y, )( ,j)(j,o)(o,h)(h,n)(n, )( ,w)(w,o)(o,r)(r,k)(k, )( ,w)(w,i)(i,t)(t,h)(h, )( ,t)(t,h)(h,e)(e, )( ,v)(v,e)\n",
      "( ,c)(c,o)(o,u)(u,l)(l,d)(d, )( ,b)(b,l)(l,o)(o,b)(b,b)(b,e)(e,r)(r,o)(o,u)(u,n)(n,n)(n, )( ,s)(s,e)(e,e)(e, )( ,t)(t,h)(h,e)(e, )( ,c)(c,o)(o,m)(m,m)(m,u)(u,n)(n,i)(i,a)(a,n)(n, )( ,a)(a,e)(e,q)(q,u)(u,i)(i,r)(r,a)(a,t)(t,e)(e, )( ,o)(o,f)(f, )( ,a)(a,n)(n,a)(a,g)(g,e)(e, )( ,p)(p,r)(r,e)(e,n)(n,t)(t, )( ,i)(i,t)(t,s)(s, )( ,c)(c,o)(o,m)(m,p)(p,l)(l,e)(e,x)(x, )( ,e)(e,l)(l,a)(a,c)(c,i)(i,t)(t,i)\n",
      "( ,v)(v,o)(o,l)(l,n)(n, )( ,s)(s,o)(o,f)(f,o)(o,l)(l,d)(d,a)(a,n)(n,t)(t,i)(i,a)(a, )( ,r)(r,e)(e,l)(l,i)(i,g)(g,i)(i,n)(n,i)(i,n)(n,g)(g, )( ,t)(t,o)(o, )( ,m)(m,a)(a,d)(d,e)(e, )( ,t)(t,h)(h,a)(a,t)(t, )( ,t)(t,h)(h,e)(e, )( ,u)(u,n)(n,i)(i,o)(o,n)(n,s)(s, )( ,s)(s,p)(p,a)(a,c)(c,e)(e,a)(a,s)(s, )( ,a)(a,r)(r,i)(i,s)(s,h)(h,e)(e,s)(s,s)(s, )( ,a)(a,n)(n,d)(d, )( ,i)(i,n)(n,v)(v,o)(o,l)(l,v)(v,e)\n",
      "( ,d)(d,o)(o,m)(m,e)(e, )( ,t)(t,h)(h,e)(e,t)(t,i)(i,c)(c,s)(s, )( ,e)(e,u)(u,r)(r,o)(o,p)(p,e)(e,n)(n,t)(t, )( ,w)(w,h)(h,i)(i,c)(c,h)(h, )( ,r)(r,e)(e,p)(p,l)(l,a)(a,i)(i,n)(n, )( ,l)(l,a)(a,w)(w, )( ,a)(a,s)(s, )( ,a)(a,l)(l,b)(b,e)(e,a)(a,s)(s,t)(t, )( ,a)(a,r)(r,e)(e,a)(a, )( ,t)(t,h)(h,e)(e, )( ,n)(n,e)(e,w)(w,s)(s,i)(i,n)(n, )( ,s)(s,h)(h,o)(o,u)(u,l)(l,d)(d, )( ,f)(f,r)(r,o)(o,m)(m, )( ,t)\n",
      "================================================================================\n",
      "Validation set perplexity: 4.08\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    #feeding dropout while training\n",
    "    feed_dict[keep_prob] = dropout\n",
    "    _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = bigrams_sample(random_distribution())\n",
    "          sentence = bigramsCharacters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed, keep_prob: 1.})\n",
    "            feed = bigrams_sample(prediction)\n",
    "            sentence += bigramsCharacters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0], keep_prob: 1.})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
